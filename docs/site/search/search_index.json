{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NDF Studio Backend Documentation","text":"<p>Welcome to the comprehensive documentation for the NDF Studio backend API and implementation.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>NDF Studio is a Node-neighborhood Description Framework that provides a powerful backend API for managing graph-based knowledge representations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Graph Management: Create, update, and manage complex graph structures</li> <li>Node Operations: Handle polymorphic nodes with multiple morphs</li> <li>Relation Management: Define and manage relationships between nodes</li> <li>Authentication: Secure user management with JWT tokens</li> <li>CNL Parsing: Natural language processing for graph descriptions</li> </ul>"},{"location":"#documentation-sections","title":"\ud83d\udcda Documentation Sections","text":""},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Core Modules: Core functionality including models, utilities, and validation</li> <li>Routes: API endpoints for graph operations, user management, and more</li> </ul>"},{"location":"#development","title":"Development","text":"<ul> <li>Installation: Setup instructions for developers</li> </ul>"},{"location":"#getting-started","title":"\ud83d\udd27 Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>FastAPI</li> <li>SQLModel</li> <li>spaCy</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/gnowledge/nodeBook.git\ncd nodeBook\n\n# Install dependencies\npip install -r backend/requirements.txt\n\n# Run the development server\nbash scripts/start_backend.sh\n</code></pre>"},{"location":"#api-access","title":"API Access","text":"<p>Once running, you can access: - API Documentation: http://localhost:8000/docs - Alternative Docs: http://localhost:8000/redoc - Health Check: http://localhost:8000/api/health</p>"},{"location":"#architecture-overview","title":"\ud83c\udfd7\ufe0f Architecture Overview","text":"<p>NDF Studio backend is built with: - FastAPI: Modern, fast web framework - SQLModel: SQL databases in Python, designed for compatibility with both SQLAlchemy Core and Pydantic - Pydantic: Data validation using Python type annotations - JWT: Secure authentication with inactivity-based token expiration</p>"},{"location":"#api-examples","title":"\ud83d\udcd6 API Examples","text":""},{"location":"#creating-a-graph","title":"Creating a Graph","text":"<pre><code>import requests\n\n# Create a new graph\nresponse = requests.post(\n    \"http://localhost:8000/api/ndf/users/{user_id}/graphs/{graph_id}\",\n    json={\n        \"title\": \"My Knowledge Graph\",\n        \"description\": \"A graph representing domain knowledge\"\n    }\n)\n</code></pre>"},{"location":"#parsing-cnl","title":"Parsing CNL","text":"<pre><code># Parse CNL (Controlled Natural Language) into graph structure\nwith open(\"graph.cnl\", \"rb\") as f:\n    response = requests.post(\n        \"http://localhost:8000/api/ndf/users/{user_id}/graphs/{graph_id}/parse_pipeline\",\n        files={\"file\": f}\n    )\n</code></pre>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our Installation Guide for setup instructions.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"api/core/atomic_ops/","title":"Atomic Operations","text":"<p>This section documents the atomic operations and transaction management functions used throughout the NDF Studio backend.</p>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops","title":"<code>backend.core.atomic_ops</code>","text":"<p>Atomic Operations Module for NDF Studio</p> <p>This module provides atomic file operations and transaction support to ensure data integrity across all NDF Studio operations. It implements:</p> <ol> <li>Atomic file writes using temporary files and atomic renames</li> <li>Transaction contexts for multi-file operations</li> <li>Rollback mechanisms for failed operations</li> <li>Validation and consistency checks</li> <li>Backup and restore functionality</li> </ol> <p>All data modifications in NDF Studio should go through this module to ensure atomicity and data integrity.</p>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops-classes","title":"Classes","text":""},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.AtomicityError","title":"<code>AtomicityError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when atomic operations fail.</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>class AtomicityError(Exception):\n    \"\"\"Raised when atomic operations fail.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.ConsistencyError","title":"<code>ConsistencyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when data consistency validation fails.</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>class ConsistencyError(Exception):\n    \"\"\"Raised when data consistency validation fails.\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops-functions","title":"Functions","text":""},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.atomic_write","title":"<code>atomic_write(file_path: Path, mode='w', encoding='utf-8')</code>","text":"<p>Atomic file write using temporary file and rename.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the target file</p> required <code>mode</code> <p>File open mode (default: 'w')</p> <code>'w'</code> <code>encoding</code> <p>File encoding (default: 'utf-8')</p> <code>'utf-8'</code> <p>Yields:</p> Type Description <p>File object for writing</p> <p>Raises:</p> Type Description <code>AtomicityError</code> <p>If the atomic write fails</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>@contextmanager\ndef atomic_write(file_path: Path, mode='w', encoding='utf-8'):\n    \"\"\"\n    Atomic file write using temporary file and rename.\n\n    Args:\n        file_path: Path to the target file\n        mode: File open mode (default: 'w')\n        encoding: File encoding (default: 'utf-8')\n\n    Yields:\n        File object for writing\n\n    Raises:\n        AtomicityError: If the atomic write fails\n    \"\"\"\n    temp_path = file_path.with_suffix(file_path.suffix + '.tmp')\n    file_handle = None\n\n    try:\n        # Ensure parent directory exists\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Open temporary file\n        file_handle = open(temp_path, mode, encoding=encoding)\n        yield file_handle\n\n        # Close file handle before atomic rename\n        file_handle.close()\n        file_handle = None\n\n        # Atomic rename (POSIX systems)\n        temp_path.replace(file_path)\n        logger.debug(f\"Atomic write successful: {file_path}\")\n\n    except Exception as e:\n        logger.error(f\"Atomic write failed for {file_path}: {e}\")\n        if file_handle:\n            file_handle.close()\n        # Clean up temp file on failure\n        if temp_path.exists():\n            temp_path.unlink()\n        raise AtomicityError(f\"Atomic write failed for {file_path}: {e}\")\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.save_json_file_atomic","title":"<code>save_json_file_atomic(file_path: Path, data: Dict[str, Any], backup_dir: Optional[Path] = None) -&gt; None</code>","text":"<p>Atomically save JSON data to a file with backup and rollback capability.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the target file</p> required <code>data</code> <code>Dict[str, Any]</code> <p>JSON data to save</p> required <code>backup_dir</code> <code>Optional[Path]</code> <p>Directory to store backups (optional)</p> <code>None</code> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def save_json_file_atomic(file_path: Path, data: Dict[str, Any], backup_dir: Optional[Path] = None) -&gt; None:\n    \"\"\"\n    Atomically save JSON data to a file with backup and rollback capability.\n\n    Args:\n        file_path: Path to the target file\n        data: JSON data to save\n        backup_dir: Directory to store backups (optional)\n    \"\"\"\n    logger = get_logger()\n    transaction_id = str(uuid.uuid4())\n\n    try:\n        logger.atomic(f\"Starting atomic JSON save\", \n                     transaction_id=transaction_id,\n                     operation=\"save_json_file_atomic\",\n                     file_path=str(file_path))\n\n        # Create backup if backup_dir is provided\n        backup_path = None\n        if backup_dir and file_path.exists():\n            backup_path = backup_dir / f\"{file_path.name}.backup\"\n            shutil.copy2(file_path, backup_path)\n            logger.atomic(f\"Created backup\", \n                         transaction_id=transaction_id,\n                         backup_path=str(backup_path))\n\n        # Write to temporary file first\n        temp_file = file_path.with_suffix(file_path.suffix + '.tmp')\n        with open(temp_file, 'w', encoding='utf-8') as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n\n        # Atomically move temp file to target\n        temp_file.replace(file_path)\n\n        logger.atomic(f\"Successfully saved JSON file atomically\", \n                     transaction_id=transaction_id,\n                     file_path=str(file_path),\n                     file_size=len(json.dumps(data)))\n\n    except Exception as e:\n        logger.error(f\"Atomic JSON save failed\", \n                    error=e,\n                    transaction_id=transaction_id,\n                    file_path=str(file_path))\n\n        # Attempt rollback if backup exists\n        if backup_path and backup_path.exists():\n            try:\n                shutil.copy2(backup_path, file_path)\n                logger.atomic(f\"Rollback successful\", \n                             transaction_id=transaction_id,\n                             backup_path=str(backup_path))\n            except Exception as rollback_error:\n                logger.error(f\"Rollback failed\", \n                           error=rollback_error,\n                           transaction_id=transaction_id)\n\n        raise AtomicityError(f\"Failed to save {file_path}: {str(e)}\")\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.load_json_file","title":"<code>load_json_file(file_path: Path) -&gt; Dict[str, Any]</code>","text":"<p>Load JSON data from a file with error handling.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>Path to the JSON file</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing the JSON data</p> <p>Raises:</p> Type Description <code>AtomicityError</code> <p>If file cannot be loaded</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def load_json_file(file_path: Path) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load JSON data from a file with error handling.\n\n    Args:\n        file_path: Path to the JSON file\n\n    Returns:\n        Dictionary containing the JSON data\n\n    Raises:\n        AtomicityError: If file cannot be loaded\n    \"\"\"\n    logger = get_logger()\n\n    try:\n        if not file_path.exists():\n            logger.debug(f\"JSON file does not exist, returning empty dict\", \n                        file_path=str(file_path))\n            return {}\n\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n\n        logger.debug(f\"Successfully loaded JSON file\", \n                    file_path=str(file_path),\n                    data_size=len(json.dumps(data)))\n\n        return data\n\n    except Exception as e:\n        logger.error(f\"Failed to load JSON file\", \n                    error=e,\n                    file_path=str(file_path))\n        raise AtomicityError(f\"Failed to load {file_path}: {str(e)}\")\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.atomic_registry_save","title":"<code>atomic_registry_save(user_id: str, registry_type: str, registry_data: Dict[str, Any], backup_dir: Optional[Path] = None) -&gt; None</code>","text":"<p>Atomically save registry data with proper error handling.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>User identifier</p> required <code>registry_type</code> <code>str</code> <p>Type of registry (node, relation, attribute)</p> required <code>registry_data</code> <code>Dict[str, Any]</code> <p>Registry data to save</p> required <code>backup_dir</code> <code>Optional[Path]</code> <p>Directory for backups</p> <code>None</code> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def atomic_registry_save(user_id: str, registry_type: str, registry_data: Dict[str, Any], \n                        backup_dir: Optional[Path] = None) -&gt; None:\n    \"\"\"\n    Atomically save registry data with proper error handling.\n\n    Args:\n        user_id: User identifier\n        registry_type: Type of registry (node, relation, attribute)\n        registry_data: Registry data to save\n        backup_dir: Directory for backups\n    \"\"\"\n    logger = get_logger()\n    transaction_id = str(uuid.uuid4())\n\n    try:\n        logger.atomic(f\"Starting atomic registry save\", \n                     transaction_id=transaction_id,\n                     operation=\"atomic_registry_save\",\n                     user_id=user_id,\n                     registry_type=registry_type)\n\n        registry_path = Path(f\"graph_data/users/{user_id}/{registry_type}_registry.json\")\n        registry_path.parent.mkdir(parents=True, exist_ok=True)\n\n        save_json_file_atomic(registry_path, registry_data, backup_dir)\n\n        logger.atomic(f\"Successfully saved registry\", \n                     transaction_id=transaction_id,\n                     registry_type=registry_type,\n                     user_id=user_id,\n                     registry_size=len(registry_data))\n\n    except Exception as e:\n        logger.error(f\"Atomic registry save failed\", \n                    error=e,\n                    transaction_id=transaction_id,\n                    user_id=user_id,\n                    registry_type=registry_type)\n        raise\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.graph_transaction","title":"<code>graph_transaction(user_id: str, graph_id: str, operation_name: str) -&gt; Generator[Path, None, None]</code>","text":"<p>Context manager for atomic graph operations with backup and rollback.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>User identifier</p> required <code>graph_id</code> <code>str</code> <p>Graph identifier</p> required <code>operation_name</code> <code>str</code> <p>Name of the operation for logging</p> required <p>Yields:</p> Type Description <code>Path</code> <p>Path to backup directory</p> <p>Raises:</p> Type Description <code>AtomicityError</code> <p>If transaction fails</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>@contextmanager\ndef graph_transaction(user_id: str, graph_id: str, operation_name: str) -&gt; Generator[Path, None, None]:\n    \"\"\"\n    Context manager for atomic graph operations with backup and rollback.\n\n    Args:\n        user_id: User identifier\n        graph_id: Graph identifier\n        operation_name: Name of the operation for logging\n\n    Yields:\n        Path to backup directory\n\n    Raises:\n        AtomicityError: If transaction fails\n    \"\"\"\n    logger = get_logger()\n    transaction_id = str(uuid.uuid4())\n    backup_dir = None\n\n    try:\n        logger.atomic(f\"Starting graph transaction\", \n                     transaction_id=transaction_id,\n                     operation=operation_name,\n                     user_id=user_id,\n                     graph_id=graph_id)\n\n        # Create backup directory\n        backup_dir = Path(f\"graph_data/users/{user_id}/backups/{graph_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{transaction_id[:8]}\")\n        backup_dir.mkdir(parents=True, exist_ok=True)\n\n        # Create backups of critical files\n        user_data_dir = Path(f\"graph_data/users/{user_id}\")\n        critical_files = [\n            \"node_registry.json\",\n            \"relation_registry.json\", \n            \"attribute_registry.json\"\n        ]\n\n        for filename in critical_files:\n            file_path = user_data_dir / filename\n            if file_path.exists():\n                backup_path = backup_dir / filename\n                shutil.copy2(file_path, backup_path)\n                logger.atomic(f\"Created backup of critical file\", \n                             transaction_id=transaction_id,\n                             file=filename,\n                             backup_path=str(backup_path))\n\n        logger.atomic(f\"Transaction backup created\", \n                     transaction_id=transaction_id,\n                     backup_dir=str(backup_dir))\n\n        yield backup_dir\n\n        logger.atomic(f\"Transaction completed successfully\", \n                     transaction_id=transaction_id,\n                     operation=operation_name)\n\n    except Exception as e:\n        logger.error(f\"Transaction failed\", \n                    error=e,\n                    transaction_id=transaction_id,\n                    operation=operation_name,\n                    user_id=user_id,\n                    graph_id=graph_id)\n\n        # Attempt rollback if backup exists\n        if backup_dir and backup_dir.exists():\n            try:\n                logger.atomic(f\"Attempting transaction rollback\", \n                             transaction_id=transaction_id,\n                             backup_dir=str(backup_dir))\n\n                user_data_dir = Path(f\"graph_data/users/{user_id}\")\n                for backup_file in backup_dir.glob(\"*.json\"):\n                    target_file = user_data_dir / backup_file.name\n                    shutil.copy2(backup_file, target_file)\n\n                logger.atomic(f\"Transaction rollback completed\", \n                             transaction_id=transaction_id)\n\n            except Exception as rollback_error:\n                logger.error(f\"Transaction rollback failed\", \n                           error=rollback_error,\n                           transaction_id=transaction_id)\n\n        raise AtomicityError(f\"Transaction failed: {str(e)}\")\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.validate_data_consistency","title":"<code>validate_data_consistency(user_id: str) -&gt; Dict[str, Any]</code>","text":"<p>Validate data consistency across registries and files.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>User identifier</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with validation results</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def validate_data_consistency(user_id: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Validate data consistency across registries and files.\n\n    Args:\n        user_id: User identifier\n\n    Returns:\n        Dictionary with validation results\n    \"\"\"\n    logger = get_logger()\n    validation_results = {\n        \"user_id\": user_id,\n        \"timestamp\": datetime.now().isoformat(),\n        \"status\": \"valid\",\n        \"issues\": [],\n        \"statistics\": {}\n    }\n\n    try:\n        logger.operation(f\"Starting data consistency validation\", \n                        operation=\"validate_data_consistency\",\n                        user_id=user_id)\n\n        user_data_dir = Path(f\"graph_data/users/{user_id}\")\n\n        # Check if user directory exists\n        if not user_data_dir.exists():\n            validation_results[\"issues\"].append(\"User data directory does not exist\")\n            validation_results[\"status\"] = \"invalid\"\n            return validation_results\n\n        # Validate registry files\n        registries = [\"node_registry.json\", \"relation_registry.json\", \"attribute_registry.json\"]\n        for registry_file in registries:\n            registry_path = user_data_dir / registry_file\n            if registry_path.exists():\n                try:\n                    registry_data = load_json_file(registry_path)\n                    validation_results[\"statistics\"][registry_file] = len(registry_data)\n\n                    # Check for orphaned files\n                    registry_type = registry_file.replace(\"_registry.json\", \"\")\n                    data_dir = user_data_dir / f\"{registry_type}Nodes\" if registry_type != \"node\" else user_data_dir / \"nodes\"\n\n                    if data_dir.exists():\n                        file_count = len(list(data_dir.glob(\"*.json\")))\n                        registry_count = len(registry_data)\n\n                        if file_count != registry_count:\n                            validation_results[\"issues\"].append(\n                                f\"Mismatch in {registry_type}: {file_count} files vs {registry_count} registry entries\"\n                            )\n                            validation_results[\"status\"] = \"warning\"\n\n                except Exception as e:\n                    validation_results[\"issues\"].append(f\"Failed to load {registry_file}: {str(e)}\")\n                    validation_results[\"status\"] = \"invalid\"\n            else:\n                validation_results[\"issues\"].append(f\"Registry file {registry_file} does not exist\")\n                validation_results[\"status\"] = \"warning\"\n\n        logger.operation(f\"Data consistency validation completed\", \n                        operation=\"validate_data_consistency\",\n                        user_id=user_id,\n                        status=validation_results[\"status\"],\n                        issue_count=len(validation_results[\"issues\"]))\n\n        return validation_results\n\n    except Exception as e:\n        logger.error(f\"Data consistency validation failed\", \n                    error=e,\n                    user_id=user_id)\n        validation_results[\"status\"] = \"error\"\n        validation_results[\"issues\"].append(f\"Validation failed: {str(e)}\")\n        return validation_results\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.cleanup_old_backups","title":"<code>cleanup_old_backups(user_id: str, max_age_hours: int = 24) -&gt; int</code>","text":"<p>Clean up old backup directories.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>User identifier</p> required <code>max_age_hours</code> <code>int</code> <p>Maximum age of backups to keep</p> <code>24</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of backups cleaned up</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def cleanup_old_backups(user_id: str, max_age_hours: int = 24) -&gt; int:\n    \"\"\"\n    Clean up old backup directories.\n\n    Args:\n        user_id: User identifier\n        max_age_hours: Maximum age of backups to keep\n\n    Returns:\n        Number of backups cleaned up\n    \"\"\"\n    logger = get_logger()\n\n    try:\n        logger.operation(f\"Starting backup cleanup\", \n                        operation=\"cleanup_old_backups\",\n                        user_id=user_id,\n                        max_age_hours=max_age_hours)\n\n        backup_dir = Path(f\"graph_data/users/{user_id}/backups\")\n        if not backup_dir.exists():\n            return 0\n\n        cutoff_time = datetime.now().timestamp() - (max_age_hours * 3600)\n        cleaned_count = 0\n\n        for backup_path in backup_dir.iterdir():\n            if backup_path.is_dir():\n                try:\n                    # Extract timestamp from directory name\n                    dir_name = backup_path.name\n                    if '_' in dir_name:\n                        timestamp_str = dir_name.split('_')[1] + '_' + dir_name.split('_')[2]\n                        backup_time = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S').timestamp()\n\n                        if backup_time &lt; cutoff_time:\n                            shutil.rmtree(backup_path)\n                            cleaned_count += 1\n                            logger.operation(f\"Cleaned up old backup\", \n                                           operation=\"cleanup_old_backups\",\n                                           backup_path=str(backup_path))\n                except Exception as e:\n                    logger.error(f\"Failed to clean up backup\", \n                               error=e,\n                               backup_path=str(backup_path))\n\n        logger.operation(f\"Backup cleanup completed\", \n                        operation=\"cleanup_old_backups\",\n                        user_id=user_id,\n                        cleaned_count=cleaned_count)\n\n        return cleaned_count\n\n    except Exception as e:\n        logger.error(f\"Backup cleanup failed\", \n                    error=e,\n                    user_id=user_id)\n        return 0\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.atomic_node_save","title":"<code>atomic_node_save(user_id: str, node_id: str, node_data: Dict[str, Any]) -&gt; None</code>","text":"<p>Atomically save a node file.</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def atomic_node_save(user_id: str, node_id: str, node_data: Dict[str, Any]) -&gt; None:\n    \"\"\"Atomically save a node file.\"\"\"\n    node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n    node_path.parent.mkdir(parents=True, exist_ok=True)\n    save_json_file_atomic(node_path, node_data)\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.atomic_relation_save","title":"<code>atomic_relation_save(user_id: str, relation_id: str, relation_data: Dict[str, Any]) -&gt; None</code>","text":"<p>Atomically save a relation file.</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def atomic_relation_save(user_id: str, relation_id: str, relation_data: Dict[str, Any]) -&gt; None:\n    \"\"\"Atomically save a relation file.\"\"\"\n    relation_path = Path(f\"graph_data/users/{user_id}/relationNodes/{relation_id}.json\")\n    relation_path.parent.mkdir(parents=True, exist_ok=True)\n    save_json_file_atomic(relation_path, relation_data)\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.atomic_attribute_save","title":"<code>atomic_attribute_save(user_id: str, attribute_id: str, attribute_data: Dict[str, Any]) -&gt; None</code>","text":"<p>Atomically save an attribute file.</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def atomic_attribute_save(user_id: str, attribute_id: str, attribute_data: Dict[str, Any]) -&gt; None:\n    \"\"\"Atomically save an attribute file.\"\"\"\n    attribute_path = Path(f\"graph_data/users/{user_id}/attributeNodes/{attribute_id}.json\")\n    attribute_path.parent.mkdir(parents=True, exist_ok=True)\n    save_json_file_atomic(attribute_path, attribute_data)\n</code></pre>"},{"location":"api/core/atomic_ops/#backend.core.atomic_ops.atomic_composed_save","title":"<code>atomic_composed_save(user_id: str, graph_id: str, composed_data: Dict[str, Any], format_type: str = 'json') -&gt; None</code>","text":"<p>Atomically save a composed file.</p> Source code in <code>backend/core/atomic_ops.py</code> <pre><code>def atomic_composed_save(user_id: str, graph_id: str, composed_data: Dict[str, Any], format_type: str = \"json\") -&gt; None:\n    \"\"\"Atomically save a composed file.\"\"\"\n    if format_type == \"json\":\n        composed_path = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}/composed.json\")\n        composed_path.parent.mkdir(parents=True, exist_ok=True)\n        save_json_file_atomic(composed_path, composed_data)\n    elif format_type == \"yaml\":\n        import yaml\n        composed_path = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}/composed.yaml\")\n        composed_path.parent.mkdir(parents=True, exist_ok=True)\n        with atomic_write(composed_path) as f:\n            yaml.safe_dump(composed_data, f, sort_keys=False, allow_unicode=True)\n    elif format_type == \"polymorphic\":\n        composed_path = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}/polymorphic_composed.json\")\n        composed_path.parent.mkdir(parents=True, exist_ok=True)\n        save_json_file_atomic(composed_path, composed_data) \n</code></pre>"},{"location":"api/core/models/","title":"Core Models","text":"<p>This section documents the core data models used throughout the NDF Studio backend.</p>"},{"location":"api/core/models/#backend.core.models","title":"<code>backend.core.models</code>","text":"<p>NDF Studio Core Data Models</p> <p>This module defines the core data models used throughout the NDF Studio backend. These models represent the fundamental entities in the Node-neighborhood Description Framework.</p> <p>Classes:</p> Name Description <code>Relation</code> <p>Represents a fundamental edge/relationship between nodes</p> <code>AttributeNode</code> <p>Scalar value modeled as a node-connected attribute</p> <code>Attribute</code> <p>Legacy attribute model for backward compatibility</p> <code>Node</code> <p>Legacy mono-morphic node model</p> <code>Morph</code> <p>Represents a variation in neighborhood</p> <code>PolyNode</code> <p>Polymorphic node model with multiple morphs</p> <code>Transition</code> <p>Represents state transitions with optional tense</p> <code>Function</code> <p>Represents functional transformations</p> <code>RelationNode</code> <p>Relation modeled as a node-connected entity</p>"},{"location":"api/core/models/#backend.core.models-classes","title":"Classes","text":""},{"location":"api/core/models/#backend.core.models.Relation","title":"<code>Relation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a fundamental edge/relationship between nodes in the graph.</p> <p>This model defines the basic structure for relationships between nodes, including optional adverbs and modalities to qualify the relationship.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the relation</p> <code>name</code> <code>str</code> <p>Name/type of the relation (e.g., \"contains\", \"is_a\")</p> <code>source</code> <code>str</code> <p>ID of the source node</p> <code>target</code> <code>str</code> <p>ID of the target node</p> <code>adverb</code> <code>Optional[str]</code> <p>Optional adverb to qualify the relation</p> <code>modality</code> <code>Optional[str]</code> <p>Optional modality (e.g., \"possibly\", \"necessarily\")</p> Source code in <code>backend/core/models.py</code> <pre><code>class Relation(BaseModel):\n    \"\"\"\n    Represents a fundamental edge/relationship between nodes in the graph.\n\n    This model defines the basic structure for relationships between nodes,\n    including optional adverbs and modalities to qualify the relationship.\n\n    Attributes:\n        id (str): Unique identifier for the relation\n        name (str): Name/type of the relation (e.g., \"contains\", \"is_a\")\n        source (str): ID of the source node\n        target (str): ID of the target node\n        adverb (Optional[str]): Optional adverb to qualify the relation\n        modality (Optional[str]): Optional modality (e.g., \"possibly\", \"necessarily\")\n    \"\"\"\n    id: str\n    name: str\n    source: str\n    target: str\n    adverb: Optional[str] = None\n    modality: Optional[str] = None\n</code></pre>"},{"location":"api/core/models/#backend.core.models.AttributeNode","title":"<code>AttributeNode</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Scalar value modeled as a node-connected attribute.</p> <p>This model represents attributes that are connected to nodes, allowing for more complex attribute structures than simple key-value pairs.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[str]</code> <p>Unique identifier for the attribute node</p> <code>name</code> <code>str</code> <p>Name of the attribute</p> <code>source_id</code> <code>str</code> <p>ID of the node this attribute belongs to</p> <code>value</code> <code>Union[str, float, int, bool]</code> <p>The attribute value</p> <code>unit</code> <code>Optional[str]</code> <p>Optional unit of measurement</p> <code>adverb</code> <code>Optional[str]</code> <p>Optional adverb to qualify the attribute</p> <code>modality</code> <code>Optional[str]</code> <p>Optional modality for the attribute</p> <code>morph_id</code> <code>Optional[List[str]]</code> <p>List of morph IDs this attribute belongs to</p> Source code in <code>backend/core/models.py</code> <pre><code>class AttributeNode(BaseModel):\n    \"\"\"\n    Scalar value modeled as a node-connected attribute.\n\n    This model represents attributes that are connected to nodes,\n    allowing for more complex attribute structures than simple key-value pairs.\n\n    Attributes:\n        id (Optional[str]): Unique identifier for the attribute node\n        name (str): Name of the attribute\n        source_id (str): ID of the node this attribute belongs to\n        value (Union[str, float, int, bool]): The attribute value\n        unit (Optional[str]): Optional unit of measurement\n        adverb (Optional[str]): Optional adverb to qualify the attribute\n        modality (Optional[str]): Optional modality for the attribute\n        morph_id (Optional[List[str]]): List of morph IDs this attribute belongs to\n    \"\"\"\n    id: Optional[str] = None\n    name: str\n    source_id: str\n    value: Union[str, float, int, bool]\n    unit: Optional[str] = None\n    adverb: Optional[str] = None\n    modality: Optional[str] = None\n    morph_id: Optional[List[str]] = None\n</code></pre>"},{"location":"api/core/models/#backend.core.models.Attribute","title":"<code>Attribute</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Legacy attribute model for backward compatibility.</p> <p>This model represents simple key-value attributes attached to nodes. It's maintained for backward compatibility with existing code.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the attribute</p> <code>node_id</code> <code>str</code> <p>ID of the node this attribute belongs to</p> <code>name</code> <code>str</code> <p>Name of the attribute</p> <code>value</code> <code>Union[str, float, int, bool, None]</code> <p>The attribute value</p> <code>unit</code> <code>Optional[str]</code> <p>Optional unit of measurement</p> <code>adverb</code> <code>Optional[str]</code> <p>Optional adverb to qualify the attribute</p> <code>modality</code> <code>Optional[str]</code> <p>Optional modality for the attribute</p> Source code in <code>backend/core/models.py</code> <pre><code>class Attribute(BaseModel):\n    \"\"\"\n    Legacy attribute model for backward compatibility.\n\n    This model represents simple key-value attributes attached to nodes.\n    It's maintained for backward compatibility with existing code.\n\n    Attributes:\n        id (str): Unique identifier for the attribute\n        node_id (str): ID of the node this attribute belongs to\n        name (str): Name of the attribute\n        value (Union[str, float, int, bool, None]): The attribute value\n        unit (Optional[str]): Optional unit of measurement\n        adverb (Optional[str]): Optional adverb to qualify the attribute\n        modality (Optional[str]): Optional modality for the attribute\n    \"\"\"\n    id: str\n    node_id: str\n    name: str\n    value: Union[str, float, int, bool, None] = None\n    unit: Optional[str] = None\n    adverb: Optional[str] = None\n    modality: Optional[str] = None\n</code></pre>"},{"location":"api/core/models/#backend.core.models.Node","title":"<code>Node</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Legacy mono-morphic node model.</p> <p>This model represents a simple node with a single neighborhood. It's maintained for backward compatibility with existing code.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[str]</code> <p>Unique identifier for the node</p> <code>name</code> <code>str</code> <p>Name of the node</p> <code>base_name</code> <code>Optional[str]</code> <p>Base name without qualifiers</p> <code>qualifier</code> <code>Optional[str]</code> <p>Optional qualifier for the node</p> <code>role</code> <code>Optional[Literal['class', 'individual', 'process']]</code> <p>Role of the node</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the node</p> <code>attributes</code> <code>List[Attribute]</code> <p>List of attributes attached to the node</p> <code>relations</code> <code>List[Relation]</code> <p>List of relations connected to the node</p> Source code in <code>backend/core/models.py</code> <pre><code>class Node(BaseModel):\n    \"\"\"\n    Legacy mono-morphic node model.\n\n    This model represents a simple node with a single neighborhood.\n    It's maintained for backward compatibility with existing code.\n\n    Attributes:\n        id (Optional[str]): Unique identifier for the node\n        name (str): Name of the node\n        base_name (Optional[str]): Base name without qualifiers\n        qualifier (Optional[str]): Optional qualifier for the node\n        role (Optional[Literal[\"class\", \"individual\", \"process\"]]): Role of the node\n        description (Optional[str]): Optional description of the node\n        attributes (List[Attribute]): List of attributes attached to the node\n        relations (List[Relation]): List of relations connected to the node\n    \"\"\"\n    id: Optional[str] = None\n    name: str\n    base_name: Optional[str] = None\n    qualifier: Optional[str] = None  # &lt;-- allow any string, not Literal\n    role: Optional[Literal[\"class\", \"individual\", \"process\"]] = None\n    description: Optional[str] = None\n    attributes: List[Attribute] = []\n    relations: List[Relation] = []\n</code></pre>"},{"location":"api/core/models/#backend.core.models.Morph","title":"<code>Morph</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a variation in neighborhood.</p> <p>A morph defines a specific variation or context of a polymorphic node. Each morph can have its own set of relations and attributes.</p> <p>Attributes:</p> Name Type Description <code>morph_id</code> <code>str</code> <p>Unique identifier for the morph</p> <code>node_id</code> <code>str</code> <p>ID of the parent polymorphic node</p> <code>name</code> <code>str</code> <p>Name of the morph</p> <code>relationNode_ids</code> <code>Optional[List[str]]</code> <p>List of relation node IDs in this morph</p> <code>attributeNode_ids</code> <code>Optional[List[str]]</code> <p>List of attribute node IDs in this morph</p> Source code in <code>backend/core/models.py</code> <pre><code>class Morph(BaseModel):\n    \"\"\"\n    Represents a variation in neighborhood.\n\n    A morph defines a specific variation or context of a polymorphic node.\n    Each morph can have its own set of relations and attributes.\n\n    Attributes:\n        morph_id (str): Unique identifier for the morph\n        node_id (str): ID of the parent polymorphic node\n        name (str): Name of the morph\n        relationNode_ids (Optional[List[str]]): List of relation node IDs in this morph\n        attributeNode_ids (Optional[List[str]]): List of attribute node IDs in this morph\n    \"\"\"\n    morph_id: str\n    node_id: str\n    name: str\n    relationNode_ids: Optional[List[str]] = []\n    attributeNode_ids: Optional[List[str]] = []\n</code></pre>"},{"location":"api/core/models/#backend.core.models.PolyNode","title":"<code>PolyNode</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Polymorphic node model with multiple morphs.</p> <p>This model represents a node that can exist in multiple variations (morphs), each with its own neighborhood of relations and attributes.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[str]</code> <p>Unique identifier for the polymorphic node</p> <code>name</code> <code>Optional[str]</code> <p>Computed name from base_name + adjective</p> <code>base_name</code> <code>str</code> <p>Required base name for ID generation</p> <code>adjective</code> <code>Optional[str]</code> <p>Optional adjective to qualify the node</p> <code>quantifier</code> <code>Optional[str]</code> <p>Optional quantifier (e.g., \"all\", \"some\")</p> <code>role</code> <code>Optional[str]</code> <p>Role of the node, defaults to \"individual\"</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the node</p> <code>morphs</code> <code>Optional[List[Morph]]</code> <p>List of morphs for this node</p> <code>nbh</code> <code>Optional[str]</code> <p>Currently active morph name</p> Source code in <code>backend/core/models.py</code> <pre><code>class PolyNode(BaseModel):\n    \"\"\"\n    Polymorphic node model with multiple morphs.\n\n    This model represents a node that can exist in multiple variations (morphs),\n    each with its own neighborhood of relations and attributes.\n\n    Attributes:\n        id (Optional[str]): Unique identifier for the polymorphic node\n        name (Optional[str]): Computed name from base_name + adjective\n        base_name (str): Required base name for ID generation\n        adjective (Optional[str]): Optional adjective to qualify the node\n        quantifier (Optional[str]): Optional quantifier (e.g., \"all\", \"some\")\n        role (Optional[str]): Role of the node, defaults to \"individual\"\n        description (Optional[str]): Optional description of the node\n        morphs (Optional[List[Morph]]): List of morphs for this node\n        nbh (Optional[str]): Currently active morph name\n    \"\"\"\n    id: Optional[str] = None  # Optional since we compute it during creation\n    name: Optional[str] = None  # Optional since we can compute it from base_name + adjective\n    base_name: str  # Required for ID generation\n    adjective: Optional[str] = None\n    quantifier: Optional[str] = None\n    role: Optional[str] = \"individual\"  # Default to individual like Node model\n    description: Optional[str] = None\n    morphs: Optional[List[Morph]] = []\n    nbh: Optional[str] = None  # currently active morph\n</code></pre>"},{"location":"api/core/models/#backend.core.models.Transition","title":"<code>Transition</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents state transitions with optional tense.</p> <p>This model defines transitions between different states or morphs of nodes, including temporal information about when the transition occurs.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the transition</p> <code>name</code> <code>Optional[str]</code> <p>Name of the transition</p> <code>adjective</code> <code>Optional[str]</code> <p>Optional adjective to qualify the transition</p> <code>tense</code> <code>Optional[str]</code> <p>Tense of the transition (e.g., \"past\", \"present\", \"future\")</p> <code>inputs</code> <code>List[dict]</code> <p>List of input nodes with their morphs</p> <code>outputs</code> <code>List[dict]</code> <p>List of output nodes with their morphs</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the transition</p> Source code in <code>backend/core/models.py</code> <pre><code>class Transition(BaseModel):\n    \"\"\"\n    Represents state transitions with optional tense.\n\n    This model defines transitions between different states or morphs of nodes,\n    including temporal information about when the transition occurs.\n\n    Attributes:\n        id (str): Unique identifier for the transition\n        name (Optional[str]): Name of the transition\n        adjective (Optional[str]): Optional adjective to qualify the transition\n        tense (Optional[str]): Tense of the transition (e.g., \"past\", \"present\", \"future\")\n        inputs (List[dict]): List of input nodes with their morphs\n        outputs (List[dict]): List of output nodes with their morphs\n        description (Optional[str]): Optional description of the transition\n    \"\"\"\n    id: str\n    name: Optional[str]\n    adjective: Optional[str] = None\n    tense: Optional[str] = None  # e.g., \"past\", \"present\", \"future\"\n    inputs: List[dict]  # {id: node_id, nbh: morph_name}\n    outputs: List[dict]\n    description: Optional[str] = None\n</code></pre>"},{"location":"api/core/models/#backend.core.models.Function","title":"<code>Function</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents functional transformations.</p> <p>This model defines functions that transform inputs into outputs, representing computational or logical operations in the graph.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the function</p> <code>name</code> <code>Optional[str]</code> <p>Name of the function</p> <code>inputs</code> <code>List[str]</code> <p>List of input node IDs</p> <code>outputs</code> <code>List[str]</code> <p>List of output node IDs</p> <code>description</code> <code>Optional[str]</code> <p>Optional description of the function</p> Source code in <code>backend/core/models.py</code> <pre><code>class Function(BaseModel):\n    \"\"\"\n    Represents functional transformations.\n\n    This model defines functions that transform inputs into outputs,\n    representing computational or logical operations in the graph.\n\n    Attributes:\n        id (str): Unique identifier for the function\n        name (Optional[str]): Name of the function\n        inputs (List[str]): List of input node IDs\n        outputs (List[str]): List of output node IDs\n        description (Optional[str]): Optional description of the function\n    \"\"\"\n    id: str\n    name: Optional[str]\n    inputs: List[str]\n    outputs: List[str]\n    description: Optional[str] = None\n</code></pre>"},{"location":"api/core/models/#backend.core.models.RelationNode","title":"<code>RelationNode</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Relation modeled as a node-connected entity.</p> <p>This model represents relations as first-class nodes in the graph, allowing for more complex relationship structures.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>Optional[str]</code> <p>Unique identifier for the relation node</p> <code>name</code> <code>str</code> <p>Name of the relation</p> <code>source_id</code> <code>str</code> <p>ID of the source node</p> <code>target_id</code> <code>str</code> <p>ID of the target node</p> <code>adverb</code> <code>Optional[str]</code> <p>Optional adverb to qualify the relation</p> <code>modality</code> <code>Optional[str]</code> <p>Optional modality for the relation</p> <code>morph_id</code> <code>Optional[List[str]]</code> <p>List of morph IDs this relation belongs to</p> Source code in <code>backend/core/models.py</code> <pre><code>class RelationNode(BaseModel):\n    \"\"\"\n    Relation modeled as a node-connected entity.\n\n    This model represents relations as first-class nodes in the graph,\n    allowing for more complex relationship structures.\n\n    Attributes:\n        id (Optional[str]): Unique identifier for the relation node\n        name (str): Name of the relation\n        source_id (str): ID of the source node\n        target_id (str): ID of the target node\n        adverb (Optional[str]): Optional adverb to qualify the relation\n        modality (Optional[str]): Optional modality for the relation\n        morph_id (Optional[List[str]]): List of morph IDs this relation belongs to\n    \"\"\"\n    id: Optional[str] = None\n    name: str\n    source_id: str\n    target_id: str\n    adverb: Optional[str] = None\n    modality: Optional[str] = None\n    morph_id: Optional[List[str]] = None\n</code></pre>"},{"location":"api/core/node_utils/","title":"Node Utils","text":"<p>This section documents the node utility functions used throughout the NDF Studio backend.</p>"},{"location":"api/core/node_utils/#backend.core.node_utils","title":"<code>backend.core.node_utils</code>","text":"<p>NDF Studio Node Utilities</p> <p>This module provides utility functions for parsing and manipulating node names in the NDF Studio backend. These utilities handle markdown formatting, text extraction, and node ID composition.</p> <p>Functions:</p> Name Description <code>extract_node_name_as_is</code> <p>Extract node name from markdown heading</p> <code>extract_base_name</code> <p>Extract base name by removing markdown markup</p> <code>extract_qualifier</code> <p>Extract qualifier (bold text) from node name</p> <code>extract_quantifier</code> <p>Extract quantifier (italics text) from node name</p> <code>compose_node_id</code> <p>Compose node ID from components</p>"},{"location":"api/core/node_utils/#backend.core.node_utils-functions","title":"Functions","text":""},{"location":"api/core/node_utils/#backend.core.node_utils.extract_node_name_as_is","title":"<code>extract_node_name_as_is(heading: str) -&gt; str</code>","text":"<p>Extract the node name as-is from a markdown heading.</p> <p>This function removes the leading '#' characters and any surrounding whitespace from a markdown heading to extract the clean node name.</p> <p>Parameters:</p> Name Type Description Default <code>heading</code> <code>str</code> <p>Markdown heading string (e.g., \"# Node Name\" or \"### Node Name\")</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Clean node name without markdown heading markers</p> Example <p>extract_node_name_as_is(\"# My Node\") 'My Node' extract_node_name_as_is(\"### Bold Node\") 'Bold Node' extract_node_name_as_is(\"  #   Node with spaces   \") 'Node with spaces'</p> Source code in <code>backend/core/node_utils.py</code> <pre><code>def extract_node_name_as_is(heading: str) -&gt; str:\n    \"\"\"\n    Extract the node name as-is from a markdown heading.\n\n    This function removes the leading '#' characters and any surrounding whitespace\n    from a markdown heading to extract the clean node name.\n\n    Args:\n        heading (str): Markdown heading string (e.g., \"# Node Name\" or \"### Node Name\")\n\n    Returns:\n        str: Clean node name without markdown heading markers\n\n    Example:\n        &gt;&gt;&gt; extract_node_name_as_is(\"# My Node\")\n        'My Node'\n        &gt;&gt;&gt; extract_node_name_as_is(\"### **Bold** Node\")\n        '**Bold** Node'\n        &gt;&gt;&gt; extract_node_name_as_is(\"  #   Node with spaces   \")\n        'Node with spaces'\n    \"\"\"\n    return heading.lstrip('#').strip()\n</code></pre>"},{"location":"api/core/node_utils/#backend.core.node_utils.extract_base_name","title":"<code>extract_base_name(node_name: str) -&gt; str</code>","text":"<p>Extract the base name from a node name by removing all markdown markup.</p> <p>This function removes various markdown formatting elements including: - Bold text (text) - Italic text (text) - Underlined text (++text++) - Modality brackets ([modality])</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>Node name that may contain markdown formatting</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Base name with all markdown markup removed</p> Example <p>extract_base_name(\"Bold italic base_name\") 'base_name' extract_base_name(\"++Underlined++ [possibly] important node\") 'node' extract_base_name(\"simple_node_name\") 'simple_node_name'</p> Source code in <code>backend/core/node_utils.py</code> <pre><code>def extract_base_name(node_name: str) -&gt; str:\n    \"\"\"\n    Extract the base name from a node name by removing all markdown markup.\n\n    This function removes various markdown formatting elements including:\n    - Bold text (**text**)\n    - Italic text (*text*)\n    - Underlined text (++text++)\n    - Modality brackets ([modality])\n\n    Args:\n        node_name (str): Node name that may contain markdown formatting\n\n    Returns:\n        str: Base name with all markdown markup removed\n\n    Example:\n        &gt;&gt;&gt; extract_base_name(\"**Bold** *italic* base_name\")\n        'base_name'\n        &gt;&gt;&gt; extract_base_name(\"++Underlined++ [possibly] **important** node\")\n        'node'\n        &gt;&gt;&gt; extract_base_name(\"simple_node_name\")\n        'simple_node_name'\n    \"\"\"\n    s = re.sub(r'\\*\\*[^*]+\\*\\*', '', node_name)  # remove bold\n    s = re.sub(r'\\*[^*]+\\*', '', s)               # remove italics\n    s = re.sub(r'\\+\\+[^+]+\\+\\+', '', s)         # remove underline\n    s = re.sub(r'\\[[^\\]]+\\]', '', s)             # remove [modality]\n    return s.strip()\n</code></pre>"},{"location":"api/core/node_utils/#backend.core.node_utils.extract_qualifier","title":"<code>extract_qualifier(node_name: str) -&gt; str</code>","text":"<p>Extract the qualifier (bold text) from a node name, if present.</p> <p>This function looks for text wrapped in double asterisks (text) and extracts it as the qualifier.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>Node name that may contain bold qualifier text</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Qualifier text if found, empty string otherwise</p> Example <p>extract_qualifier(\"Important node\") 'Important' extract_node_name_as_is(\"Bold italic node\") 'Bold' extract_qualifier(\"node without qualifier\") ''</p> Source code in <code>backend/core/node_utils.py</code> <pre><code>def extract_qualifier(node_name: str) -&gt; str:\n    \"\"\"\n    Extract the qualifier (bold text) from a node name, if present.\n\n    This function looks for text wrapped in double asterisks (**text**)\n    and extracts it as the qualifier.\n\n    Args:\n        node_name (str): Node name that may contain bold qualifier text\n\n    Returns:\n        str: Qualifier text if found, empty string otherwise\n\n    Example:\n        &gt;&gt;&gt; extract_qualifier(\"**Important** node\")\n        'Important'\n        &gt;&gt;&gt; extract_node_name_as_is(\"**Bold** *italic* node\")\n        'Bold'\n        &gt;&gt;&gt; extract_qualifier(\"node without qualifier\")\n        ''\n    \"\"\"\n    match = re.search(r'\\*\\*([^*]+)\\*\\*', node_name)\n    return match.group(1).strip() if match else ''\n</code></pre>"},{"location":"api/core/node_utils/#backend.core.node_utils.extract_quantifier","title":"<code>extract_quantifier(node_name: str) -&gt; str</code>","text":"<p>Extract the quantifier (italics text, not bold) from a node name, if present.</p> <p>This function looks for text wrapped in single asterisks (text) that is not part of bold formatting (text). It returns the first such match.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>Node name that may contain italic quantifier text</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Quantifier text if found, empty string otherwise</p> Example <p>extract_quantifier(\"some node\") 'some' extract_quantifier(\"Bold italic node\") 'italic' extract_quantifier(\"node without quantifier\") ''</p> Source code in <code>backend/core/node_utils.py</code> <pre><code>def extract_quantifier(node_name: str) -&gt; str:\n    \"\"\"\n    Extract the quantifier (italics text, not bold) from a node name, if present.\n\n    This function looks for text wrapped in single asterisks (*text*) that is not\n    part of bold formatting (**text**). It returns the first such match.\n\n    Args:\n        node_name (str): Node name that may contain italic quantifier text\n\n    Returns:\n        str: Quantifier text if found, empty string otherwise\n\n    Example:\n        &gt;&gt;&gt; extract_quantifier(\"*some* node\")\n        'some'\n        &gt;&gt;&gt; extract_quantifier(\"**Bold** *italic* node\")\n        'italic'\n        &gt;&gt;&gt; extract_quantifier(\"node without quantifier\")\n        ''\n    \"\"\"\n    matches = re.findall(r'(?&lt;!\\*)\\*([^*]+)\\*(?!\\*)', node_name)\n    return matches[0].strip() if matches else ''\n</code></pre>"},{"location":"api/core/node_utils/#backend.core.node_utils.compose_node_id","title":"<code>compose_node_id(quantifier: str, qualifier: str, base_name: str, report: Optional[List[Dict[str, Any]]] = None) -&gt; Optional[str]</code>","text":"<p>Compose a node ID from quantifier, qualifier, and base name components.</p> <p>This function combines the three components into a single node ID by: 1. Filtering out empty components 2. Joining the remaining components with underscores 3. Validating that base_name is provided (mandatory)</p> <p>Parameters:</p> Name Type Description Default <code>quantifier</code> <code>str</code> <p>Quantifier component (e.g., \"some\", \"all\")</p> required <code>qualifier</code> <code>str</code> <p>Qualifier component (e.g., \"important\", \"special\")</p> required <code>base_name</code> <code>str</code> <p>Base name component (mandatory)</p> required <code>report</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Optional list to collect error reports</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: Composed node ID, or None if base_name is missing</p> Example <p>compose_node_id(\"some\", \"important\", \"node\") 'some_important_node' compose_node_id(\"\", \"special\", \"entity\") 'special_entity' compose_node_id(\"\", \"\", \"base\") 'base' compose_node_id(\"\", \"\", \"\") None</p> Note <p>If base_name is empty and a report list is provided, an error message will be appended to the report and None will be returned.</p> Source code in <code>backend/core/node_utils.py</code> <pre><code>def compose_node_id(quantifier: str, qualifier: str, base_name: str, report: Optional[List[Dict[str, Any]]] = None) -&gt; Optional[str]:\n    \"\"\"\n    Compose a node ID from quantifier, qualifier, and base name components.\n\n    This function combines the three components into a single node ID by:\n    1. Filtering out empty components\n    2. Joining the remaining components with underscores\n    3. Validating that base_name is provided (mandatory)\n\n    Args:\n        quantifier (str): Quantifier component (e.g., \"some\", \"all\")\n        qualifier (str): Qualifier component (e.g., \"important\", \"special\")\n        base_name (str): Base name component (mandatory)\n        report (Optional[List[Dict[str, Any]]]): Optional list to collect error reports\n\n    Returns:\n        Optional[str]: Composed node ID, or None if base_name is missing\n\n    Example:\n        &gt;&gt;&gt; compose_node_id(\"some\", \"important\", \"node\")\n        'some_important_node'\n        &gt;&gt;&gt; compose_node_id(\"\", \"special\", \"entity\")\n        'special_entity'\n        &gt;&gt;&gt; compose_node_id(\"\", \"\", \"base\")\n        'base'\n        &gt;&gt;&gt; compose_node_id(\"\", \"\", \"\")\n        None\n\n    Note:\n        If base_name is empty and a report list is provided, an error message\n        will be appended to the report and None will be returned.\n    \"\"\"\n    if not base_name:\n        if report is not None:\n            report.append({\n                'type': 'error',\n                'stage': 'compose_node_id',\n                'message': 'base_name (noun) is mandatory for node_id composition.',\n                'quantifier': quantifier,\n                'qualifier': qualifier\n            })\n        return None\n    parts = [quantifier, qualifier, base_name]\n    node_id = '_'.join([p for p in parts if p])\n    return node_id\n</code></pre>"},{"location":"api/core/registry/","title":"Registry Management","text":"<p>This section documents the registry management functions used throughout the NDF Studio backend.</p>"},{"location":"api/core/registry/#backend.core.registry","title":"<code>backend.core.registry</code>","text":"<p>NDF Studio Registry Management</p> <p>This module provides functions for managing node, relation, and attribute registries in the NDF Studio backend. These registries track metadata about nodes, relations, and attributes across different graphs for a user.</p> <p>Functions:</p> Name Description <code>get_registry_path</code> <p>Get the path to a user's node registry file</p> <code>load_node_registry</code> <p>Load a user's node registry from file</p> <code>save_node_registry</code> <p>Save a user's node registry to file</p> <code>update_node_registry</code> <p>Update registry with new node information</p> <code>create_node_if_missing</code> <p>Create a node file if it doesn't exist</p> <code>relation_registry_path</code> <p>Get the path to a user's relation registry file</p> <code>attribute_registry_path</code> <p>Get the path to a user's attribute registry file</p> <code>load_registry</code> <p>Load any registry from a given path</p> <code>save_registry</code> <p>Save any registry to a given path</p> <code>make_relation_id</code> <p>Create a unique relation identifier</p> <code>make_attribute_id</code> <p>Create a unique attribute identifier</p> <code>make_polynode_id</code> <p>Create a unique polynode identifier</p> <code>make_morph_id</code> <p>Create a unique morph identifier</p>"},{"location":"api/core/registry/#backend.core.registry-functions","title":"Functions","text":""},{"location":"api/core/registry/#backend.core.registry.get_registry_path","title":"<code>get_registry_path(user_id: str) -&gt; Path</code>","text":"<p>Get the path to a user's node registry file.</p> <p>This function constructs the file path for a user's node registry, which stores metadata about all nodes created by that user.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Unique identifier for the user</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path object pointing to the node registry file</p> Example <p>get_registry_path(\"user123\") PosixPath('/path/to/data/users/user123/node_registry.json')</p> Source code in <code>backend/core/registry.py</code> <pre><code>def get_registry_path(user_id: str) -&gt; Path:\n    \"\"\"\n    Get the path to a user's node registry file.\n\n    This function constructs the file path for a user's node registry,\n    which stores metadata about all nodes created by that user.\n\n    Args:\n        user_id (str): Unique identifier for the user\n\n    Returns:\n        Path: Path object pointing to the node registry file\n\n    Example:\n        &gt;&gt;&gt; get_registry_path(\"user123\")\n        PosixPath('/path/to/data/users/user123/node_registry.json')\n    \"\"\"\n    return get_data_root() / \"users\" / user_id / \"node_registry.json\"\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.load_node_registry","title":"<code>load_node_registry(user_id: str) -&gt; dict</code>","text":"<p>Load a user's node registry from file.</p> <p>This function loads the node registry for a specific user. If the registry file doesn't exist, it returns an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Unique identifier for the user</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Node registry data, or empty dict if file doesn't exist</p> Example <p>registry = load_node_registry(\"user123\") print(registry) {'node1': {'name': 'Node1', 'graphs': ['graph1'], 'created_at': '2024-01-01T00:00:00'}}</p> Source code in <code>backend/core/registry.py</code> <pre><code>def load_node_registry(user_id: str) -&gt; dict:\n    \"\"\"\n    Load a user's node registry from file.\n\n    This function loads the node registry for a specific user. If the registry\n    file doesn't exist, it returns an empty dictionary.\n\n    Args:\n        user_id (str): Unique identifier for the user\n\n    Returns:\n        dict: Node registry data, or empty dict if file doesn't exist\n\n    Example:\n        &gt;&gt;&gt; registry = load_node_registry(\"user123\")\n        &gt;&gt;&gt; print(registry)\n        {'node1': {'name': 'Node1', 'graphs': ['graph1'], 'created_at': '2024-01-01T00:00:00'}}\n    \"\"\"\n    path = get_registry_path(user_id)\n    if path.exists():\n        return load_json_file(path)\n    else:\n        return {}\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.save_node_registry","title":"<code>save_node_registry(user_id: str, registry: dict)</code>","text":"<p>Save a user's node registry to file.</p> <p>This function saves the node registry data to the user's registry file. The registry contains metadata about all nodes created by the user.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Unique identifier for the user</p> required <code>registry</code> <code>dict</code> <p>Node registry data to save</p> required Example <p>registry = {'node1': {'name': 'Node1', 'graphs': ['graph1']}} save_node_registry(\"user123\", registry)</p> Source code in <code>backend/core/registry.py</code> <pre><code>def save_node_registry(user_id: str, registry: dict):\n    \"\"\"\n    Save a user's node registry to file.\n\n    This function saves the node registry data to the user's registry file.\n    The registry contains metadata about all nodes created by the user.\n\n    Args:\n        user_id (str): Unique identifier for the user\n        registry (dict): Node registry data to save\n\n    Example:\n        &gt;&gt;&gt; registry = {'node1': {'name': 'Node1', 'graphs': ['graph1']}}\n        &gt;&gt;&gt; save_node_registry(\"user123\", registry)\n    \"\"\"\n    path = get_registry_path(user_id)\n    save_json_file(path, registry)\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.update_node_registry","title":"<code>update_node_registry(registry: dict, node_id: str, graph_id: str)</code>","text":"<p>Update registry with new node information.</p> <p>This function updates the node registry with information about a node being used in a specific graph. If the node doesn't exist in the registry, it creates a new entry. If it exists, it adds the graph to the node's graph list and updates the timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>dict</code> <p>The node registry to update</p> required <code>node_id</code> <code>str</code> <p>Unique identifier for the node</p> required <code>graph_id</code> <code>str</code> <p>Unique identifier for the graph</p> required Example <p>registry = {} update_node_registry(registry, \"oxygen\", \"chemistry_graph\") print(registry[\"oxygen\"][\"graphs\"]) ['chemistry_graph']</p> Source code in <code>backend/core/registry.py</code> <pre><code>def update_node_registry(registry: dict, node_id: str, graph_id: str):\n    \"\"\"\n    Update registry with new node information.\n\n    This function updates the node registry with information about a node\n    being used in a specific graph. If the node doesn't exist in the registry,\n    it creates a new entry. If it exists, it adds the graph to the node's\n    graph list and updates the timestamp.\n\n    Args:\n        registry (dict): The node registry to update\n        node_id (str): Unique identifier for the node\n        graph_id (str): Unique identifier for the graph\n\n    Example:\n        &gt;&gt;&gt; registry = {}\n        &gt;&gt;&gt; update_node_registry(registry, \"oxygen\", \"chemistry_graph\")\n        &gt;&gt;&gt; print(registry[\"oxygen\"][\"graphs\"])\n        ['chemistry_graph']\n    \"\"\"\n    now = datetime.utcnow().isoformat()\n    if node_id not in registry:\n        registry[node_id] = {\n            \"name\": node_id.capitalize(),\n            \"graphs\": [graph_id],\n            \"created_at\": now,\n            \"updated_at\": now\n        }\n    else:\n        if graph_id not in registry[node_id][\"graphs\"]:\n            registry[node_id][\"graphs\"].append(graph_id)\n        registry[node_id][\"updated_at\"] = now\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.create_node_if_missing","title":"<code>create_node_if_missing(user_id: str, node_id: str, name: str = None)</code>","text":"<p>Create a node file if it doesn't exist.</p> <p>This function creates a node JSON file if it doesn't already exist. It uses the parse_node_title function to extract clean fields from the name and creates a structured node data object.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Unique identifier for the user</p> required <code>node_id</code> <code>str</code> <p>Unique identifier for the node</p> required <code>name</code> <code>str</code> <p>Human-readable name for the node. Defaults to None.</p> <code>None</code> Example <p>create_node_if_missing(\"user123\", \"oxygen\", \"Oxygen atom\")</p> Source code in <code>backend/core/registry.py</code> <pre><code>def create_node_if_missing(user_id: str, node_id: str, name: str = None):\n    \"\"\"\n    Create a node file if it doesn't exist.\n\n    This function creates a node JSON file if it doesn't already exist.\n    It uses the parse_node_title function to extract clean fields from the name\n    and creates a structured node data object.\n\n    Args:\n        user_id (str): Unique identifier for the user\n        node_id (str): Unique identifier for the node\n        name (str, optional): Human-readable name for the node. Defaults to None.\n\n    Example:\n        &gt;&gt;&gt; create_node_if_missing(\"user123\", \"oxygen\", \"Oxygen atom\")\n        # Creates file: /path/to/data/users/user123/nodes/oxygen.json\n    \"\"\"\n    node_path = get_data_root() / \"users\" / user_id / \"nodes\" / f\"{node_id}.json\"\n\n    if node_path.exists():\n        return\n\n    # Ensure parent directory exists\n    node_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Use parse_node_title to extract clean fields\n    title_info = parse_node_title(name or node_id)\n    node_data = {\n        \"id\": title_info[\"id\"],\n        \"name\": title_info[\"name\"],\n        \"base\": title_info[\"base\"],\n        \"quantifier\": title_info.get(\"quantifier\"),\n        \"qualifier\": title_info.get(\"qualifier\"),\n        \"description\": \"\",\n        \"attributes\": [],\n        \"relations\": []\n    }\n    save_json_file(node_path, node_data)\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.create_node_if_missing--creates-file-pathtodatausersuser123nodesoxygenjson","title":"Creates file: /path/to/data/users/user123/nodes/oxygen.json","text":""},{"location":"api/core/registry/#backend.core.registry.relation_registry_path","title":"<code>relation_registry_path(user_id: str) -&gt; Path</code>","text":"<p>Get the path to a user's relation registry file.</p> <p>This function constructs the file path for a user's relation registry, which stores metadata about all relations created by that user.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Unique identifier for the user</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path object pointing to the relation registry file</p> Example <p>relation_registry_path(\"user123\") PosixPath('/path/to/data/users/user123/relation_registry.json')</p> Source code in <code>backend/core/registry.py</code> <pre><code>def relation_registry_path(user_id: str) -&gt; Path:\n    \"\"\"\n    Get the path to a user's relation registry file.\n\n    This function constructs the file path for a user's relation registry,\n    which stores metadata about all relations created by that user.\n\n    Args:\n        user_id (str): Unique identifier for the user\n\n    Returns:\n        Path: Path object pointing to the relation registry file\n\n    Example:\n        &gt;&gt;&gt; relation_registry_path(\"user123\")\n        PosixPath('/path/to/data/users/user123/relation_registry.json')\n    \"\"\"\n    return get_data_root() / \"users\" / user_id / \"relation_registry.json\"\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.attribute_registry_path","title":"<code>attribute_registry_path(user_id: str) -&gt; Path</code>","text":"<p>Get the path to a user's attribute registry file.</p> <p>This function constructs the file path for a user's attribute registry, which stores metadata about all attributes created by that user.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Unique identifier for the user</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path object pointing to the attribute registry file</p> Example <p>attribute_registry_path(\"user123\") PosixPath('/path/to/data/users/user123/attribute_registry.json')</p> Source code in <code>backend/core/registry.py</code> <pre><code>def attribute_registry_path(user_id: str) -&gt; Path:\n    \"\"\"\n    Get the path to a user's attribute registry file.\n\n    This function constructs the file path for a user's attribute registry,\n    which stores metadata about all attributes created by that user.\n\n    Args:\n        user_id (str): Unique identifier for the user\n\n    Returns:\n        Path: Path object pointing to the attribute registry file\n\n    Example:\n        &gt;&gt;&gt; attribute_registry_path(\"user123\")\n        PosixPath('/path/to/data/users/user123/attribute_registry.json')\n    \"\"\"\n    return get_data_root() / \"users\" / user_id / \"attribute_registry.json\"\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.load_registry","title":"<code>load_registry(path: Path) -&gt; dict</code>","text":"<p>Load any registry from a given path.</p> <p>This function loads registry data from a specified file path. If the file doesn't exist, it returns an empty dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the registry file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Registry data, or empty dict if file doesn't exist</p> Example <p>path = Path(\"/path/to/registry.json\") registry = load_registry(path)</p> Source code in <code>backend/core/registry.py</code> <pre><code>def load_registry(path: Path) -&gt; dict:\n    \"\"\"\n    Load any registry from a given path.\n\n    This function loads registry data from a specified file path.\n    If the file doesn't exist, it returns an empty dictionary.\n\n    Args:\n        path (Path): Path to the registry file\n\n    Returns:\n        dict: Registry data, or empty dict if file doesn't exist\n\n    Example:\n        &gt;&gt;&gt; path = Path(\"/path/to/registry.json\")\n        &gt;&gt;&gt; registry = load_registry(path)\n    \"\"\"\n    if path.exists():\n        return load_json_file(path)\n    else:\n        return {}\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.save_registry","title":"<code>save_registry(path: Path, registry: dict)</code>","text":"<p>Save any registry to a given path.</p> <p>This function saves registry data to a specified file path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the registry file</p> required <code>registry</code> <code>dict</code> <p>Registry data to save</p> required Example <p>path = Path(\"/path/to/registry.json\") registry = {\"item1\": {\"name\": \"Item1\"}} save_registry(path, registry)</p> Source code in <code>backend/core/registry.py</code> <pre><code>def save_registry(path: Path, registry: dict):\n    \"\"\"\n    Save any registry to a given path.\n\n    This function saves registry data to a specified file path.\n\n    Args:\n        path (Path): Path to the registry file\n        registry (dict): Registry data to save\n\n    Example:\n        &gt;&gt;&gt; path = Path(\"/path/to/registry.json\")\n        &gt;&gt;&gt; registry = {\"item1\": {\"name\": \"Item1\"}}\n        &gt;&gt;&gt; save_registry(path, registry)\n    \"\"\"\n    save_json_file(path, registry)\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.make_relation_id","title":"<code>make_relation_id(source: str, type_: str, target: str, adverb: str = '', modality: str = '') -&gt; str</code>","text":"<p>Create a unique relation identifier.</p> <p>This function creates a unique identifier for a relation by combining the source node, relation type, target node, and optional adverb and modality. Empty components are filtered out and the remaining parts are joined with '::'.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source node identifier</p> required <code>type_</code> <code>str</code> <p>Relation type name</p> required <code>target</code> <code>str</code> <p>Target node identifier</p> required <code>adverb</code> <code>str</code> <p>Adverb modifier. Defaults to \"\".</p> <code>''</code> <code>modality</code> <code>str</code> <p>Modality modifier. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique relation identifier</p> Example <p>make_relation_id(\"oxygen\", \"bonds_with\", \"hydrogen\", \"strongly\", \"[possibly]\") 'oxygen::strongly::bonds_with::hydrogen::[possibly]' make_relation_id(\"atom\", \"contains\", \"nucleus\") 'atom::contains::nucleus'</p> Source code in <code>backend/core/registry.py</code> <pre><code>def make_relation_id(source: str, type_: str, target: str, adverb: str = \"\", modality: str = \"\") -&gt; str:\n    \"\"\"\n    Create a unique relation identifier.\n\n    This function creates a unique identifier for a relation by combining\n    the source node, relation type, target node, and optional adverb and modality.\n    Empty components are filtered out and the remaining parts are joined with '::'.\n\n    Args:\n        source (str): Source node identifier\n        type_ (str): Relation type name\n        target (str): Target node identifier\n        adverb (str, optional): Adverb modifier. Defaults to \"\".\n        modality (str, optional): Modality modifier. Defaults to \"\".\n\n    Returns:\n        str: Unique relation identifier\n\n    Example:\n        &gt;&gt;&gt; make_relation_id(\"oxygen\", \"bonds_with\", \"hydrogen\", \"strongly\", \"[possibly]\")\n        'oxygen::strongly::bonds_with::hydrogen::[possibly]'\n        &gt;&gt;&gt; make_relation_id(\"atom\", \"contains\", \"nucleus\")\n        'atom::contains::nucleus'\n    \"\"\"\n    parts = [source, adverb, type_, target, modality]\n    return '::'.join([p for p in parts if p])\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.make_attribute_id","title":"<code>make_attribute_id(node_id: str, name: str, value: str = '', unit: str = '', adverb: str = '', modality: str = '') -&gt; str</code>","text":"<p>Create a unique attribute identifier.</p> <p>This function creates a unique identifier for an attribute by combining all relevant fields and generating a hash to ensure uniqueness. This allows multiple attributes with the same name but different values.</p> <p>Parameters:</p> Name Type Description Default <code>node_id</code> <code>str</code> <p>Node identifier</p> required <code>name</code> <code>str</code> <p>Attribute name</p> required <code>value</code> <code>str</code> <p>Attribute value. Defaults to \"\".</p> <code>''</code> <code>unit</code> <code>str</code> <p>Unit of measurement. Defaults to \"\".</p> <code>''</code> <code>adverb</code> <code>str</code> <p>Adverb modifier. Defaults to \"\".</p> <code>''</code> <code>modality</code> <code>str</code> <p>Modality modifier. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique attribute identifier with hash suffix</p> Example <p>make_attribute_id(\"oxygen\", \"mass\", \"16\", \"amu\") 'oxygen::mass::a1b2c3d4e5f6' make_attribute_id(\"atom\", \"color\", \"blue\", \"\", \"very\", \"[possibly]\") 'atom::color::g7h8i9j0k1l2'</p> Source code in <code>backend/core/registry.py</code> <pre><code>def make_attribute_id(node_id: str, name: str, value: str = \"\", unit: str = \"\", adverb: str = \"\", modality: str = \"\") -&gt; str:\n    \"\"\"\n    Create a unique attribute identifier.\n\n    This function creates a unique identifier for an attribute by combining\n    all relevant fields and generating a hash to ensure uniqueness. This allows\n    multiple attributes with the same name but different values.\n\n    Args:\n        node_id (str): Node identifier\n        name (str): Attribute name\n        value (str, optional): Attribute value. Defaults to \"\".\n        unit (str, optional): Unit of measurement. Defaults to \"\".\n        adverb (str, optional): Adverb modifier. Defaults to \"\".\n        modality (str, optional): Modality modifier. Defaults to \"\".\n\n    Returns:\n        str: Unique attribute identifier with hash suffix\n\n    Example:\n        &gt;&gt;&gt; make_attribute_id(\"oxygen\", \"mass\", \"16\", \"amu\")\n        'oxygen::mass::a1b2c3d4e5f6'\n        &gt;&gt;&gt; make_attribute_id(\"atom\", \"color\", \"blue\", \"\", \"very\", \"[possibly]\")\n        'atom::color::g7h8i9j0k1l2'\n    \"\"\"\n    # Create a unique identifier by combining all relevant fields\n    # This allows multiple attributes with the same name but different values\n    fields = [node_id, name, value, unit, adverb, modality]\n    # Create a hash of the fields to ensure uniqueness\n    hash_input = \"::\".join([str(f) for f in fields if f])\n    hash_value = hashlib.md5(hash_input.encode()).hexdigest()[:12]  # Use first 12 chars of hash\n    # Return a human-readable prefix with the hash for uniqueness\n    return f\"{node_id}::{name}::{hash_value}\"\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.make_polynode_id","title":"<code>make_polynode_id(quantifier: str = '', adverb: str = '', morph_name: str = '', base_name: str = '') -&gt; str</code>","text":"<p>Create a unique polynode identifier.</p> <p>This function creates a unique identifier for a polynode by combining quantifier, adverb, morph name, and base name. If the morph name is \"basic\", it's excluded from the ID. Empty components are filtered out.</p> <p>Parameters:</p> Name Type Description Default <code>quantifier</code> <code>str</code> <p>Quantifier (e.g., \"some\", \"all\"). Defaults to \"\".</p> <code>''</code> <code>adverb</code> <code>str</code> <p>Adverb modifier. Defaults to \"\".</p> <code>''</code> <code>morph_name</code> <code>str</code> <p>Morph name. Defaults to \"\".</p> <code>''</code> <code>base_name</code> <code>str</code> <p>Base name. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique polynode identifier</p> Example <p>make_polynode_id(\"some\", \"very\", \"ionized\", \"oxygen\") 'some_very_ionized_oxygen' make_polynode_id(\"\", \"\", \"basic\", \"atom\") 'atom'</p> Source code in <code>backend/core/registry.py</code> <pre><code>def make_polynode_id(quantifier: str = \"\", adverb: str = \"\", morph_name: str = \"\", base_name: str = \"\") -&gt; str:\n    \"\"\"\n    Create a unique polynode identifier.\n\n    This function creates a unique identifier for a polynode by combining\n    quantifier, adverb, morph name, and base name. If the morph name is \"basic\",\n    it's excluded from the ID. Empty components are filtered out.\n\n    Args:\n        quantifier (str, optional): Quantifier (e.g., \"some\", \"all\"). Defaults to \"\".\n        adverb (str, optional): Adverb modifier. Defaults to \"\".\n        morph_name (str, optional): Morph name. Defaults to \"\".\n        base_name (str, optional): Base name. Defaults to \"\".\n\n    Returns:\n        str: Unique polynode identifier\n\n    Example:\n        &gt;&gt;&gt; make_polynode_id(\"some\", \"very\", \"ionized\", \"oxygen\")\n        'some_very_ionized_oxygen'\n        &gt;&gt;&gt; make_polynode_id(\"\", \"\", \"basic\", \"atom\")\n        'atom'\n    \"\"\"\n    # If morph_name is \"basic\", don't include it in the ID calculation\n    if morph_name == \"basic\":\n        morph_name = \"\"\n    parts = [quantifier, adverb, morph_name, base_name]\n    return '_'.join([p for p in parts if p])\n</code></pre>"},{"location":"api/core/registry/#backend.core.registry.make_morph_id","title":"<code>make_morph_id(name: str, node_id: str) -&gt; str</code>","text":"<p>Create a unique morph identifier.</p> <p>This function creates a unique identifier for a morph by combining the morph name with the node identifier.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Morph name</p> required <code>node_id</code> <code>str</code> <p>Node identifier</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unique morph identifier</p> Example <p>make_morph_id(\"ionized\", \"oxygen\") 'ionized_oxygen' make_morph_id(\"excited\", \"atom\") 'excited_atom'</p> Source code in <code>backend/core/registry.py</code> <pre><code>def make_morph_id(name: str, node_id: str) -&gt; str:\n    \"\"\"\n    Create a unique morph identifier.\n\n    This function creates a unique identifier for a morph by combining\n    the morph name with the node identifier.\n\n    Args:\n        name (str): Morph name\n        node_id (str): Node identifier\n\n    Returns:\n        str: Unique morph identifier\n\n    Example:\n        &gt;&gt;&gt; make_morph_id(\"ionized\", \"oxygen\")\n        'ionized_oxygen'\n        &gt;&gt;&gt; make_morph_id(\"excited\", \"atom\")\n        'excited_atom'\n    \"\"\"\n    return f\"{name}_{node_id}\"\n</code></pre>"},{"location":"api/core/schema_ops/","title":"Schema Operations","text":"<p>This section documents the schema operations functions used throughout the NDF Studio backend.</p>"},{"location":"api/core/schema_ops/#backend.core.schema_ops","title":"<code>backend.core.schema_ops</code>","text":"<p>NDF Studio Schema Operations</p> <p>This module provides functions for managing schema definitions in the NDF Studio backend. It handles attribute types, relation types, and node types, including loading, saving, validation, and CNL (Controlled Natural Language) parsing for schema definitions.</p> <p>Functions:</p> Name Description <code>ordered_schema_dict</code> <p>Create an ordered dictionary with specified key order</p> <code>ensure_schema_file</code> <p>Ensure a schema file exists with default data</p> <code>load_schema</code> <p>Load schema data from file with default fallback</p> <code>validate_schema_entry</code> <p>Validate that a schema entry has required keys</p> <code>save_schema</code> <p>Save schema data to file with proper formatting</p> <code>load_schema_json</code> <p>Load JSON schema with default data creation</p> <code>create_attribute_type_from_dict</code> <p>Create attribute type from dictionary data</p> <code>create_relation_type_from_dict</code> <p>Create relation type from dictionary data</p> <code>parse_cnl_block</code> <p>Parse CNL block for schema definitions</p> <code>filter_used_schema</code> <p>Filter schema to only include used types</p>"},{"location":"api/core/schema_ops/#backend.core.schema_ops-functions","title":"Functions","text":""},{"location":"api/core/schema_ops/#backend.core.schema_ops.ordered_schema_dict","title":"<code>ordered_schema_dict(entry: dict, key_order: list[str]) -&gt; OrderedDict</code>","text":"<p>Create an ordered dictionary with specified key order.</p> <p>This function creates an OrderedDict from a regular dictionary, ensuring that keys appear in the specified order. Keys not in the order list are appended at the end.</p> <p>Parameters:</p> Name Type Description Default <code>entry</code> <code>dict</code> <p>Input dictionary to reorder</p> required <code>key_order</code> <code>list[str]</code> <p>List of keys in desired order</p> required <p>Returns:</p> Name Type Description <code>OrderedDict</code> <code>OrderedDict</code> <p>Dictionary with keys in specified order</p> Example <p>entry = {\"description\": \"A type\", \"name\": \"MyType\", \"extra\": \"value\"} ordered = ordered_schema_dict(entry, [\"name\", \"description\"]) list(ordered.keys()) ['name', 'description', 'extra']</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def ordered_schema_dict(entry: dict, key_order: list[str]) -&gt; OrderedDict:\n    \"\"\"\n    Create an ordered dictionary with specified key order.\n\n    This function creates an OrderedDict from a regular dictionary, ensuring\n    that keys appear in the specified order. Keys not in the order list\n    are appended at the end.\n\n    Args:\n        entry (dict): Input dictionary to reorder\n        key_order (list[str]): List of keys in desired order\n\n    Returns:\n        OrderedDict: Dictionary with keys in specified order\n\n    Example:\n        &gt;&gt;&gt; entry = {\"description\": \"A type\", \"name\": \"MyType\", \"extra\": \"value\"}\n        &gt;&gt;&gt; ordered = ordered_schema_dict(entry, [\"name\", \"description\"])\n        &gt;&gt;&gt; list(ordered.keys())\n        ['name', 'description', 'extra']\n    \"\"\"\n    ordered = OrderedDict()\n    for key in key_order:\n        if key in entry:\n            ordered[key] = entry[key]\n    for key in entry:\n        if key not in ordered:\n            ordered[key] = entry[key]\n    return ordered\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.ensure_schema_file","title":"<code>ensure_schema_file(file_name, default_data)</code>","text":"<p>Ensure a schema file exists with default data.</p> <p>This function checks if a schema file exists and creates it with default data if it doesn't exist. It also ensures the schema directory exists.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the schema file</p> required <code>default_data</code> <p>Default data to write if file doesn't exist</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Path to the schema file</p> Example <p>ensure_schema_file(\"attribute_types.json\", []) 'graph_data/global/attribute_types.json'</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def ensure_schema_file(file_name, default_data):\n    \"\"\"\n    Ensure a schema file exists with default data.\n\n    This function checks if a schema file exists and creates it with default data\n    if it doesn't exist. It also ensures the schema directory exists.\n\n    Args:\n        file_name (str): Name of the schema file\n        default_data: Default data to write if file doesn't exist\n\n    Returns:\n        str: Path to the schema file\n\n    Example:\n        &gt;&gt;&gt; ensure_schema_file(\"attribute_types.json\", [])\n        'graph_data/global/attribute_types.json'\n    \"\"\"\n    file_path = os.path.join(GLOBAL_SCHEMA_PATH, file_name)\n    if not os.path.exists(file_path):\n        os.makedirs(GLOBAL_SCHEMA_PATH, exist_ok=True)\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(default_data, f, indent=2)\n    return file_path\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.load_schema","title":"<code>load_schema(file_name, default_data)</code>","text":"<p>Load schema data from file with default fallback.</p> <p>This function loads schema data from a file. If the file doesn't exist, it creates it with the default data and returns the default data.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the schema file</p> required <code>default_data</code> <p>Default data to use if file doesn't exist</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>Schema data from file or default data</p> Example <p>schema = load_schema(\"attribute_types.json\", []) isinstance(schema, list) True</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def load_schema(file_name, default_data):\n    \"\"\"\n    Load schema data from file with default fallback.\n\n    This function loads schema data from a file. If the file doesn't exist,\n    it creates it with the default data and returns the default data.\n\n    Args:\n        file_name (str): Name of the schema file\n        default_data: Default data to use if file doesn't exist\n\n    Returns:\n        list: Schema data from file or default data\n\n    Example:\n        &gt;&gt;&gt; schema = load_schema(\"attribute_types.json\", [])\n        &gt;&gt;&gt; isinstance(schema, list)\n        True\n    \"\"\"\n    file_path = ensure_schema_file(file_name, default_data)\n    with open(file_path, encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return data or default_data\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.validate_schema_entry","title":"<code>validate_schema_entry(entry: dict, required_keys: list[str], file_name: str) -&gt; None</code>","text":"<p>Validate that a schema entry has required keys.</p> <p>This function checks if a schema entry contains all required keys. If any required keys are missing, it raises a ValueError with details.</p> <p>Parameters:</p> Name Type Description Default <code>entry</code> <code>dict</code> <p>Schema entry to validate</p> required <code>required_keys</code> <code>list[str]</code> <p>List of required keys</p> required <code>file_name</code> <code>str</code> <p>Name of the schema file for error reporting</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required keys are missing</p> Example <p>entry = {\"name\": \"MyType\"} validate_schema_entry(entry, [\"name\", \"description\"], \"test.json\") Traceback (most recent call last): ValueError: Missing keys in test.json entry: ['description'] \u2192 {'name': 'MyType'}</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def validate_schema_entry(entry: dict, required_keys: list[str], file_name: str) -&gt; None:\n    \"\"\"\n    Validate that a schema entry has required keys.\n\n    This function checks if a schema entry contains all required keys.\n    If any required keys are missing, it raises a ValueError with details.\n\n    Args:\n        entry (dict): Schema entry to validate\n        required_keys (list[str]): List of required keys\n        file_name (str): Name of the schema file for error reporting\n\n    Raises:\n        ValueError: If required keys are missing\n\n    Example:\n        &gt;&gt;&gt; entry = {\"name\": \"MyType\"}\n        &gt;&gt;&gt; validate_schema_entry(entry, [\"name\", \"description\"], \"test.json\")\n        Traceback (most recent call last):\n        ValueError: Missing keys in test.json entry: ['description'] \u2192 {'name': 'MyType'}\n    \"\"\"\n    missing = [key for key in required_keys if key not in entry]\n    if missing:\n        raise ValueError(f\"Missing keys in {file_name} entry: {missing} \u2192 {entry}\")\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.save_schema","title":"<code>save_schema(file_name, data: list[dict])</code>","text":"<p>Save schema data to file with proper formatting.</p> <p>This function saves schema data to a file with proper key ordering and validation. It determines the appropriate key order based on the file name and validates each entry before saving.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the schema file</p> required <code>data</code> <code>list[dict]</code> <p>List of schema entries to save</p> required Example <p>data = [{\"name\": \"MyType\", \"description\": \"A type\"}] save_schema(\"attribute_types.json\", data)</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def save_schema(file_name, data: list[dict]):\n    \"\"\"\n    Save schema data to file with proper formatting.\n\n    This function saves schema data to a file with proper key ordering\n    and validation. It determines the appropriate key order based on\n    the file name and validates each entry before saving.\n\n    Args:\n        file_name (str): Name of the schema file\n        data (list[dict]): List of schema entries to save\n\n    Example:\n        &gt;&gt;&gt; data = [{\"name\": \"MyType\", \"description\": \"A type\"}]\n        &gt;&gt;&gt; save_schema(\"attribute_types.json\", data)\n    \"\"\"\n    file_path = os.path.join(GLOBAL_SCHEMA_PATH, file_name)\n\n    # Choose key order based on file\n    file_str = str(file_name)\n    if \"attribute\" in file_str:\n        key_order = ATTRIBUTE_TYPE_KEYS\n    elif \"relation\" in file_str:\n        key_order = RELATION_TYPE_KEYS\n    elif \"node\" in file_str:\n        key_order = NODE_TYPE_KEYS\n    else:\n        key_order = []\n\n\n    formatted = []\n    for entry in sorted(data, key=lambda x: x.get(\"name\", \"\")):\n        validate_schema_entry(entry, key_order, file_name)\n        formatted.append(ordered_schema_dict(entry, key_order))\n\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(formatted, f, indent=2)\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.load_schema_json","title":"<code>load_schema_json(file_name: str, default_data: list)</code>","text":"<p>Load JSON schema with default data creation.</p> <p>This function loads JSON schema data from a file. If the file is empty or contains None, it writes the default data and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Name of the schema file</p> required <code>default_data</code> <code>list</code> <p>Default data to use if file is empty</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>Schema data from file or default data</p> Example <p>schema = load_schema_json(\"relation_types.json\", []) isinstance(schema, list) True</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def load_schema_json(file_name: str, default_data: list):\n    \"\"\"\n    Load JSON schema with default data creation.\n\n    This function loads JSON schema data from a file. If the file is empty\n    or contains None, it writes the default data and returns it.\n\n    Args:\n        file_name (str): Name of the schema file\n        default_data (list): Default data to use if file is empty\n\n    Returns:\n        list: Schema data from file or default data\n\n    Example:\n        &gt;&gt;&gt; schema = load_schema_json(\"relation_types.json\", [])\n        &gt;&gt;&gt; isinstance(schema, list)\n        True\n    \"\"\"\n    file_path = ensure_schema_file(file_name, default_data)\n    with open(file_path, encoding=\"utf-8\") as f:\n        data = json.load(f)\n    if data is None:\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(default_data, f, indent=2)\n        return default_data\n    return data\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.create_attribute_type_from_dict","title":"<code>create_attribute_type_from_dict(data: dict)</code>","text":"<p>Create attribute type from dictionary data.</p> <p>This function creates a new attribute type from dictionary data and adds it to the attribute types schema. If an attribute type with the same name already exists, the function returns without making changes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Attribute type data with keys: name, data_type, unit, applicable_classes</p> required Example <p>data = { ...     \"name\": \"mass\", ...     \"data_type\": \"float\", ...     \"unit\": \"kg\", ...     \"applicable_classes\": [\"atom\", \"molecule\"] ... } create_attribute_type_from_dict(data)</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def create_attribute_type_from_dict(data: dict):\n    \"\"\"\n    Create attribute type from dictionary data.\n\n    This function creates a new attribute type from dictionary data and adds it\n    to the attribute types schema. If an attribute type with the same name\n    already exists, the function returns without making changes.\n\n    Args:\n        data (dict): Attribute type data with keys: name, data_type, unit, applicable_classes\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"name\": \"mass\",\n        ...     \"data_type\": \"float\",\n        ...     \"unit\": \"kg\",\n        ...     \"applicable_classes\": [\"atom\", \"molecule\"]\n        ... }\n        &gt;&gt;&gt; create_attribute_type_from_dict(data)\n    \"\"\"\n    attr_types = load_schema(\"attribute_types.json\", default_data=[])\n    existing_names = {a[\"name\"] for a in attr_types}\n    if data[\"name\"] in existing_names:\n        return  # or raise or skip silently\n\n    attr_types.append(OrderedDict([\n        (\"name\", data[\"name\"]),\n        (\"data_type\", data[\"data_type\"]),\n        (\"unit\", data[\"unit\"]),\n        (\"applicable_classes\", data[\"applicable_classes\"]),\n    ]))\n    save_schema(\"attribute_types.json\", attr_types)\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.create_relation_type_from_dict","title":"<code>create_relation_type_from_dict(data: dict)</code>","text":"<p>Create relation type from dictionary data.</p> <p>This function creates a new relation type from dictionary data and adds it to the relation types schema. If a relation type with the same name already exists, the function returns without making changes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Relation type data with keys: name, inverse, domain, range</p> required Example <p>data = { ...     \"name\": \"bonds_with\", ...     \"inverse\": \"bonded_by\", ...     \"domain\": \"atom\", ...     \"range\": \"atom\" ... } create_relation_type_from_dict(data)</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def create_relation_type_from_dict(data: dict):\n    \"\"\"\n    Create relation type from dictionary data.\n\n    This function creates a new relation type from dictionary data and adds it\n    to the relation types schema. If a relation type with the same name\n    already exists, the function returns without making changes.\n\n    Args:\n        data (dict): Relation type data with keys: name, inverse, domain, range\n\n    Example:\n        &gt;&gt;&gt; data = {\n        ...     \"name\": \"bonds_with\",\n        ...     \"inverse\": \"bonded_by\",\n        ...     \"domain\": \"atom\",\n        ...     \"range\": \"atom\"\n        ... }\n        &gt;&gt;&gt; create_relation_type_from_dict(data)\n    \"\"\"\n    rel_types = load_schema(\"relation_types.json\", default_data=[])\n    existing_names = {r[\"name\"] for r in rel_types}\n    if data[\"name\"] in existing_names:\n        return\n\n    rel_types.append(OrderedDict([\n        (\"name\", data[\"name\"]),\n        (\"inverse\", data[\"inverse\"]),\n        (\"domain\", data[\"domain\"]),\n        (\"range\", data[\"range\"]),\n    ]))\n    save_schema(\"relation_types.json\", rel_types)\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.parse_cnl_block","title":"<code>parse_cnl_block(block: str) -&gt; list[dict]</code>","text":"<p>Parse CNL block for schema definitions.</p> <p>This function parses a Controlled Natural Language (CNL) block to extract schema definitions for attributes and relations. It supports the following CNL patterns: - \"define attribute 'name' as a type with unit 'unit' applicable to classes.\" - \"define relation 'name' with inverse 'inverse' between 'domain' and 'range'.\"</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>str</code> <p>CNL block containing schema definitions</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of parsed schema statements</p> Example <p>cnl = ''' ... define attribute 'mass' as a float with unit 'kg' applicable to atom, molecule. ... define relation 'bonds_with' with inverse 'bonded_by' between 'atom' and 'atom'. ... ''' statements = parse_cnl_block(cnl) len(statements) 2</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def parse_cnl_block(block: str) -&gt; list[dict]:\n    \"\"\"\n    Parse CNL block for schema definitions.\n\n    This function parses a Controlled Natural Language (CNL) block to extract\n    schema definitions for attributes and relations. It supports the following\n    CNL patterns:\n    - \"define attribute 'name' as a type with unit 'unit' applicable to classes.\"\n    - \"define relation 'name' with inverse 'inverse' between 'domain' and 'range'.\"\n\n    Args:\n        block (str): CNL block containing schema definitions\n\n    Returns:\n        list[dict]: List of parsed schema statements\n\n    Example:\n        &gt;&gt;&gt; cnl = '''\n        ... define attribute 'mass' as a float with unit 'kg' applicable to atom, molecule.\n        ... define relation 'bonds_with' with inverse 'bonded_by' between 'atom' and 'atom'.\n        ... '''\n        &gt;&gt;&gt; statements = parse_cnl_block(cnl)\n        &gt;&gt;&gt; len(statements)\n        2\n    \"\"\"\n    lines = block.strip().splitlines()\n    statements = []\n    for line in lines:\n        line = line.strip()\n\n        # --- Define attribute ---\n        if line.lower().startswith(\"define attribute\"):\n            m = re.match(\n                r\"define attribute '(.+?)' as a (\\w+)(?: with unit '(.+?)')?(?: applicable to (.+?))?\\.\", line)\n            if m:\n                name, data_type, unit, classes = m.groups()\n                statements.append({\n                    \"type\": \"define_attribute\",\n                    \"name\": name,\n                    \"data_type\": data_type,\n                    \"unit\": unit or \"\",\n                    \"applicable_classes\": [c.strip(\" '\") for c in classes.split(\",\")] if classes else []\n                })\n\n        # --- Define relation ---\n        elif line.lower().startswith(\"define relation\"):\n            m = re.match(\n                r\"define relation '(.+?)' with inverse '(.+?)'(?: between '(.+?)' and '(.+?)')?\\.\", line)\n            if m:\n                name, inverse, domain, range_ = m.groups()\n                statements.append({\n                    \"type\": \"define_relation\",\n                    \"name\": name,\n                    \"inverse\": inverse,\n                    \"domain\": domain,\n                    \"range\": range_,\n                })\n\n        # [existing parsing continues...]\n    return statements\n</code></pre>"},{"location":"api/core/schema_ops/#backend.core.schema_ops.filter_used_schema","title":"<code>filter_used_schema(parsed_json_path, relation_schema_path, attribute_schema_path, output_path)</code>","text":"<p>Filters only the used relation and attribute types from the global schema and writes them into used_schema.json.</p> <p>This function analyzes a parsed graph to identify which relation and attribute types are actually used, then creates a filtered schema containing only those types. This is useful for creating lightweight schemas for specific graphs.</p> <p>Parameters:</p> Name Type Description Default <code>parsed_json_path</code> <code>str</code> <p>Path to the parsed graph JSON file</p> required <code>relation_schema_path</code> <code>str</code> <p>Path to the global relation schema file</p> required <code>attribute_schema_path</code> <code>str</code> <p>Path to the global attribute schema file</p> required <code>output_path</code> <code>str</code> <p>Path where the filtered schema will be written</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The filtered schema containing only used types</p> Example <p>filter_used_schema( ...     \"parsed_graph.json\", ...     \"relation_types.json\",  ...     \"attribute_types.json\", ...     \"used_schema.json\" ... ) {'relation_types': [...], 'attribute_types': [...]}</p> Source code in <code>backend/core/schema_ops.py</code> <pre><code>def filter_used_schema(parsed_json_path, relation_schema_path, attribute_schema_path, output_path):\n    \"\"\"\n    Filters only the used relation and attribute types from the global schema\n    and writes them into used_schema.json.\n\n    This function analyzes a parsed graph to identify which relation and attribute\n    types are actually used, then creates a filtered schema containing only those\n    types. This is useful for creating lightweight schemas for specific graphs.\n\n    Args:\n        parsed_json_path (str): Path to the parsed graph JSON file\n        relation_schema_path (str): Path to the global relation schema file\n        attribute_schema_path (str): Path to the global attribute schema file\n        output_path (str): Path where the filtered schema will be written\n\n    Returns:\n        dict: The filtered schema containing only used types\n\n    Example:\n        &gt;&gt;&gt; filter_used_schema(\n        ...     \"parsed_graph.json\",\n        ...     \"relation_types.json\", \n        ...     \"attribute_types.json\",\n        ...     \"used_schema.json\"\n        ... )\n        {'relation_types': [...], 'attribute_types': [...]}\n    \"\"\"\n    # Load parsed graph\n    with open(parsed_json_path, 'r') as f:\n        parsed_data = json.load(f)\n\n    # Collect used relation and attribute names\n    used_relation_names = set()\n    used_attribute_names = set()\n\n    for node in parsed_data.get(\"nodes\", []):\n        for rel in node.get(\"relations\", []):\n            used_relation_names.add(rel[\"name\"])\n        for attr in node.get(\"attributes\", []):\n            used_attribute_names.add(attr[\"name\"])\n\n    # Load global schemas\n    with open(relation_schema_path, 'r') as f:\n        global_relations = json.load(f)\n    with open(attribute_schema_path, 'r') as f:\n        global_attributes = json.load(f)\n\n    # Filter schemas\n    used_relations = [r for r in global_relations if r[\"name\"] in used_relation_names]\n    used_attributes = [a for a in global_attributes if a[\"name\"] in used_attribute_names]\n\n    # Compose output\n    used_schema = {\n        \"relation_types\": used_relations,\n        \"attribute_types\": used_attributes\n    }\n\n    # Write to file\n    with open(output_path, \"w\") as f:\n        json.dump(used_schema, f, indent=2, sort_keys=False)\n\n    return used_schema\n</code></pre>"},{"location":"api/core/schema_utils/","title":"Schema Utilities","text":"<p>This section documents the schema utility functions used throughout the NDF Studio backend.</p>"},{"location":"api/core/schema_utils/#backend.core.schema_utils","title":"<code>backend.core.schema_utils</code>","text":"<p>NDF Studio Schema Utilities</p> <p>This module provides utility functions for managing schema definitions in YAML format in the NDF Studio backend. It handles filtering of used schema types from global schemas based on actual usage in parsed graphs.</p> <p>Functions:</p> Name Description <code>filter_used_schema</code> <p>Filter schema to only include used types from YAML files</p>"},{"location":"api/core/schema_utils/#backend.core.schema_utils-functions","title":"Functions","text":""},{"location":"api/core/schema_utils/#backend.core.schema_utils.filter_used_schema","title":"<code>filter_used_schema(parsed_yaml_path, relation_schema_path, attribute_schema_path, output_path)</code>","text":"<p>Filters only the used relation and attribute types from the global schema and writes them into used_schema.yaml.</p> <p>This function analyzes a parsed graph in YAML format to identify which relation and attribute types are actually used, then creates a filtered schema containing only those types. This is useful for creating lightweight schemas for specific graphs.</p> <p>Parameters:</p> Name Type Description Default <code>parsed_yaml_path</code> <code>str</code> <p>Path to the parsed graph YAML file</p> required <code>relation_schema_path</code> <code>str</code> <p>Path to the global relation schema YAML file</p> required <code>attribute_schema_path</code> <code>str</code> <p>Path to the global attribute schema YAML file</p> required <code>output_path</code> <code>str</code> <p>Path where the filtered schema will be written</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The filtered schema containing only used types</p> Example <p>filter_used_schema( ...     \"parsed_graph.yaml\", ...     \"relation_types.yaml\",  ...     \"attribute_types.yaml\", ...     \"used_schema.yaml\" ... ) {'relation_types': [...], 'attribute_types': [...]}</p> Note <p>This function is similar to the JSON version in schema_ops.py but works with YAML files instead of JSON files.</p> Source code in <code>backend/core/schema_utils.py</code> <pre><code>def filter_used_schema(parsed_yaml_path, relation_schema_path, attribute_schema_path, output_path):\n    \"\"\"\n    Filters only the used relation and attribute types from the global schema\n    and writes them into used_schema.yaml.\n\n    This function analyzes a parsed graph in YAML format to identify which relation\n    and attribute types are actually used, then creates a filtered schema containing\n    only those types. This is useful for creating lightweight schemas for specific graphs.\n\n    Args:\n        parsed_yaml_path (str): Path to the parsed graph YAML file\n        relation_schema_path (str): Path to the global relation schema YAML file\n        attribute_schema_path (str): Path to the global attribute schema YAML file\n        output_path (str): Path where the filtered schema will be written\n\n    Returns:\n        dict: The filtered schema containing only used types\n\n    Example:\n        &gt;&gt;&gt; filter_used_schema(\n        ...     \"parsed_graph.yaml\",\n        ...     \"relation_types.yaml\", \n        ...     \"attribute_types.yaml\",\n        ...     \"used_schema.yaml\"\n        ... )\n        {'relation_types': [...], 'attribute_types': [...]}\n\n    Note:\n        This function is similar to the JSON version in schema_ops.py but works\n        with YAML files instead of JSON files.\n    \"\"\"\n    # Load parsed graph\n    parsed_data = yaml.safe_load(Path(parsed_yaml_path).read_text())\n\n    # Collect used relation and attribute names\n    used_relation_names = set()\n    used_attribute_names = set()\n\n    for node in parsed_data.get(\"nodes\", []):\n        for rel in node.get(\"relations\", []):\n            used_relation_names.add(rel[\"name\"])\n        for attr in node.get(\"attributes\", []):\n            used_attribute_names.add(attr[\"name\"])\n\n    # Load global schemas\n    global_relations = yaml.safe_load(Path(relation_schema_path).read_text())\n    global_attributes = yaml.safe_load(Path(attribute_schema_path).read_text())\n\n    # Filter schemas\n    used_relations = [r for r in global_relations if r[\"name\"] in used_relation_names]\n    used_attributes = [a for a in global_attributes if a[\"name\"] in used_attribute_names]\n\n    # Compose output\n    used_schema = {\n        \"relation_types\": used_relations,\n        \"attribute_types\": used_attributes\n    }\n\n    # Write to file\n    with open(output_path, \"w\") as f:\n        yaml.dump(used_schema, f, sort_keys=False)\n\n    return used_schema\n</code></pre>"},{"location":"api/core/utils/","title":"Core Utilities","text":"<p>This section documents the utility functions and helpers used throughout the NDF Studio backend.</p>"},{"location":"api/core/utils/#backend.core.utils","title":"<code>backend.core.utils</code>","text":"<p>NDF Studio Core Utilities</p> <p>This module provides utility functions used throughout the NDF Studio backend. These utilities handle common operations like file I/O, text processing, and data normalization.</p> <p>Functions:</p> Name Description <code>render_description_md</code> <p>Converts markdown text to safe HTML</p> <code>normalize_id</code> <p>Normalizes strings to valid identifiers</p> <code>save_json_file</code> <p>Saves data to JSON file</p> <code>load_json_file</code> <p>Loads data from JSON file</p> <code>load_text_file</code> <p>Loads text content from file</p>"},{"location":"api/core/utils/#backend.core.utils-functions","title":"Functions","text":""},{"location":"api/core/utils/#backend.core.utils.render_description_md","title":"<code>render_description_md(text: Optional[str]) -&gt; str</code>","text":"<p>Converts markdown text to safe HTML for display.</p> <p>This function takes markdown text and converts it to HTML, then sanitizes the HTML to prevent XSS attacks by allowing only safe tags and attributes.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Optional[str]</code> <p>Markdown text to convert and sanitize</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Safe HTML string ready for display</p> Example <p>render_description_md(\"Bold text and link\") '<p>Bold text and link</p>'</p> Source code in <code>backend/core/utils.py</code> <pre><code>def render_description_md(text: Optional[str]) -&gt; str:\n    \"\"\"\n    Converts markdown text to safe HTML for display.\n\n    This function takes markdown text and converts it to HTML, then sanitizes\n    the HTML to prevent XSS attacks by allowing only safe tags and attributes.\n\n    Args:\n        text (Optional[str]): Markdown text to convert and sanitize\n\n    Returns:\n        str: Safe HTML string ready for display\n\n    Example:\n        &gt;&gt;&gt; render_description_md(\"**Bold text** and [link](http://example.com)\")\n        '&lt;p&gt;&lt;strong&gt;Bold text&lt;/strong&gt; and &lt;a href=\"http://example.com\"&gt;link&lt;/a&gt;&lt;/p&gt;'\n    \"\"\"\n    raw_html = markdown.markdown(text or \"\")\n    safe_html = bleach.clean(\n        raw_html,\n        tags=[\"p\", \"b\", \"i\", \"strong\", \"em\", \"ul\", \"ol\", \"li\", \"a\", \"code\", \"pre\", \"blockquote\"],\n        attributes={\"a\": [\"href\"]},\n    )\n    return safe_html\n</code></pre>"},{"location":"api/core/utils/#backend.core.utils.normalize_id","title":"<code>normalize_id(name: str) -&gt; str</code>","text":"<p>Normalizes a string to create a valid identifier.</p> <p>This function converts a string to a valid identifier by: - Trimming whitespace - Converting to lowercase - Replacing spaces with underscores</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The string to normalize</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Normalized identifier string</p> Example <p>normalize_id(\"  My Node Name  \") 'my_node_name'</p> Source code in <code>backend/core/utils.py</code> <pre><code>def normalize_id(name: str) -&gt; str:\n    \"\"\"\n    Normalizes a string to create a valid identifier.\n\n    This function converts a string to a valid identifier by:\n    - Trimming whitespace\n    - Converting to lowercase\n    - Replacing spaces with underscores\n\n    Args:\n        name (str): The string to normalize\n\n    Returns:\n        str: Normalized identifier string\n\n    Example:\n        &gt;&gt;&gt; normalize_id(\"  My Node Name  \")\n        'my_node_name'\n    \"\"\"\n    return name.strip().lower().replace(\" \", \"_\")\n</code></pre>"},{"location":"api/core/utils/#backend.core.utils.save_json_file","title":"<code>save_json_file(path: Path, data: Dict[str, Any]) -&gt; None</code>","text":"<p>Saves data to a JSON file with proper formatting.</p> <p>This function saves a dictionary to a JSON file with UTF-8 encoding and pretty formatting (2-space indentation).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the JSON file to create/overwrite</p> required <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary data to save</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If the file cannot be written</p> <code>TypeError</code> <p>If the data cannot be serialized to JSON</p> Example <p>save_json_file(Path(\"config.json\"), {\"key\": \"value\"})</p> Source code in <code>backend/core/utils.py</code> <pre><code>def save_json_file(path: Path, data: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Saves data to a JSON file with proper formatting.\n\n    This function saves a dictionary to a JSON file with UTF-8 encoding\n    and pretty formatting (2-space indentation).\n\n    Args:\n        path (Path): Path to the JSON file to create/overwrite\n        data (Dict[str, Any]): Dictionary data to save\n\n    Raises:\n        IOError: If the file cannot be written\n        TypeError: If the data cannot be serialized to JSON\n\n    Example:\n        &gt;&gt;&gt; save_json_file(Path(\"config.json\"), {\"key\": \"value\"})\n        # Creates config.json with: {\"key\": \"value\"}\n    \"\"\"\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"api/core/utils/#backend.core.utils.save_json_file--creates-configjson-with-key-value","title":"Creates config.json with: {\"key\": \"value\"}","text":""},{"location":"api/core/utils/#backend.core.utils.load_json_file","title":"<code>load_json_file(path: Path) -&gt; Dict[str, Any]</code>","text":"<p>Loads data from a JSON file.</p> <p>This function reads a JSON file and returns the parsed dictionary. The file is expected to be UTF-8 encoded.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the JSON file to read</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Parsed JSON data as a dictionary</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file doesn't exist</p> <code>JSONDecodeError</code> <p>If the file contains invalid JSON</p> <code>IOError</code> <p>If the file cannot be read</p> Example <p>data = load_json_file(Path(\"config.json\")) print(data) {'key': 'value'}</p> Source code in <code>backend/core/utils.py</code> <pre><code>def load_json_file(path: Path) -&gt; Dict[str, Any]:\n    \"\"\"\n    Loads data from a JSON file.\n\n    This function reads a JSON file and returns the parsed dictionary.\n    The file is expected to be UTF-8 encoded.\n\n    Args:\n        path (Path): Path to the JSON file to read\n\n    Returns:\n        Dict[str, Any]: Parsed JSON data as a dictionary\n\n    Raises:\n        FileNotFoundError: If the file doesn't exist\n        json.JSONDecodeError: If the file contains invalid JSON\n        IOError: If the file cannot be read\n\n    Example:\n        &gt;&gt;&gt; data = load_json_file(Path(\"config.json\"))\n        &gt;&gt;&gt; print(data)\n        {'key': 'value'}\n    \"\"\"\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/core/utils/#backend.core.utils.load_text_file","title":"<code>load_text_file(path: Path) -&gt; str</code>","text":"<pre><code>Loads text content from a file.\n\nThis function reads a text file and returns its content as a string.\nThe file is expected to be UTF-8 encoded.\n\nArgs:\n    path (Path): Path to the text file to read\n\nReturns:\n    str: Content of the text file\n\nRaises:\n    FileNotFoundError: If the file doesn't exist\n    IOError: If the file cannot be read\n\nExample:\n    &gt;&gt;&gt; content = load_text_file(Path(\"readme.md\"))\n    &gt;&gt;&gt; print(content[:50])\n    '# NDF Studio Documentation\n</code></pre> <p>This is the main...'</p> Source code in <code>backend/core/utils.py</code> <pre><code>def load_text_file(path: Path) -&gt; str:\n    \"\"\"\n    Loads text content from a file.\n\n    This function reads a text file and returns its content as a string.\n    The file is expected to be UTF-8 encoded.\n\n    Args:\n        path (Path): Path to the text file to read\n\n    Returns:\n        str: Content of the text file\n\n    Raises:\n        FileNotFoundError: If the file doesn't exist\n        IOError: If the file cannot be read\n\n    Example:\n        &gt;&gt;&gt; content = load_text_file(Path(\"readme.md\"))\n        &gt;&gt;&gt; print(content[:50])\n        '# NDF Studio Documentation\\n\\nThis is the main...'\n    \"\"\"\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        return f.read()    \n</code></pre>"},{"location":"api/external/auth/","title":"User Registration/Authentication","text":"<p>This section documents the external authentication and user registration services that the NDF Studio system depends on.</p>"},{"location":"api/external/auth/#overview","title":"Overview","text":"<p>The NDF Studio system integrates with external authentication services for user management, including:</p> <ul> <li>User registration and account creation</li> <li>User authentication and login</li> <li>Password management and recovery</li> <li>Session management</li> <li>Role-based access control</li> </ul>"},{"location":"api/external/auth/#external-dependencies","title":"External Dependencies","text":""},{"location":"api/external/auth/#authentication-providers","title":"Authentication Providers","text":"<ul> <li>FastAPI Users: Primary authentication framework</li> <li>JWT Tokens: For session management</li> <li>Password Hashing: Secure password storage</li> </ul>"},{"location":"api/external/auth/#integration-points","title":"Integration Points","text":"<ul> <li>User registration endpoints</li> <li>Login/logout functionality</li> <li>Password reset workflows</li> <li>User profile management</li> </ul>"},{"location":"api/external/auth/#configuration","title":"Configuration","text":"<p>Authentication services are configured through environment variables and configuration files in the backend system.</p>"},{"location":"api/external/auth/#security-considerations","title":"Security Considerations","text":"<ul> <li>All passwords are hashed using secure algorithms</li> <li>JWT tokens have configurable expiration times</li> <li>Session management includes automatic cleanup</li> <li>Rate limiting is applied to authentication endpoints </li> </ul>"},{"location":"api/external/cytoscape/","title":"Cytoscape Integration","text":"<p>This section documents the Cytoscape integration and graph visualization services that the NDF Studio system depends on.</p>"},{"location":"api/external/cytoscape/#overview","title":"Overview","text":"<p>The NDF Studio system integrates with Cytoscape for:</p> <ul> <li>Interactive graph visualization and exploration</li> <li>Network analysis and visualization</li> <li>Graph layout and styling</li> <li>User interface for graph manipulation</li> <li>Real-time graph updates and interactions</li> </ul>"},{"location":"api/external/cytoscape/#external-dependencies","title":"External Dependencies","text":""},{"location":"api/external/cytoscape/#cytoscape-libraries-and-services","title":"Cytoscape Libraries and Services","text":"<ul> <li>Cytoscape.js: Core graph visualization library</li> <li>Cytoscape.js Extensions: Additional functionality and plugins</li> <li>Cytoscape.js Layouts: Graph layout algorithms</li> <li>Cytoscape.js Styling: Visual styling and theming</li> <li>Cytoscape.js Events: Interactive event handling</li> </ul>"},{"location":"api/external/cytoscape/#core-visualization-features","title":"Core Visualization Features","text":"<ul> <li>Interactive Graphs: Click, drag, and zoom functionality</li> <li>Layout Algorithms: Automatic graph layout and positioning</li> <li>Visual Styling: Node and edge appearance customization</li> <li>Animation: Smooth transitions and updates</li> <li>Performance Optimization: Large graph handling</li> </ul>"},{"location":"api/external/cytoscape/#integration-points","title":"Integration Points","text":""},{"location":"api/external/cytoscape/#frontend-integration","title":"Frontend Integration","text":"<ol> <li>Data Formatting: Converting NDF data to Cytoscape format</li> <li>Event Handling: Managing user interactions</li> <li>Real-time Updates: Synchronizing graph changes</li> <li>Styling: Applying visual themes and styles</li> <li>Performance: Optimizing for large datasets</li> </ol>"},{"location":"api/external/cytoscape/#graph-operations","title":"Graph Operations","text":"<ul> <li>Node Creation/Deletion: Adding and removing nodes</li> <li>Edge Management: Creating and modifying relationships</li> <li>Layout Updates: Recalculating graph layouts</li> <li>Selection: Highlighting and selecting graph elements</li> <li>Filtering: Showing/hiding graph elements</li> </ul>"},{"location":"api/external/cytoscape/#configuration","title":"Configuration","text":"<p>Cytoscape integration is configured through: - Layout algorithm selection - Visual styling and themes - Performance settings and optimizations - Event handling and interaction modes - Plugin and extension management</p>"},{"location":"api/external/cytoscape/#features","title":"Features","text":""},{"location":"api/external/cytoscape/#interactive-capabilities","title":"Interactive Capabilities","text":"<ul> <li>Zoom and Pan: Navigation through large graphs</li> <li>Node Selection: Click to select and highlight nodes</li> <li>Edge Highlighting: Visualizing relationships</li> <li>Context Menus: Right-click actions and options</li> <li>Keyboard Shortcuts: Quick access to common actions</li> </ul>"},{"location":"api/external/cytoscape/#visual-customization","title":"Visual Customization","text":"<ul> <li>Node Styling: Colors, shapes, sizes, and labels</li> <li>Edge Styling: Line styles, colors, and thickness</li> <li>Layout Options: Force-directed, hierarchical, and custom layouts</li> <li>Animation: Smooth transitions and updates</li> <li>Themes: Consistent visual styling across the application </li> </ul>"},{"location":"api/external/database/","title":"Database Operations","text":"<p>This section documents the database operations and data persistence services that the NDF Studio system depends on.</p>"},{"location":"api/external/database/#overview","title":"Overview","text":"<p>The NDF Studio system uses database services for:</p> <ul> <li>Data persistence and storage</li> <li>User data management</li> <li>Graph data storage and retrieval</li> <li>Transaction management</li> <li>Data backup and recovery</li> </ul>"},{"location":"api/external/database/#external-dependencies","title":"External Dependencies","text":""},{"location":"api/external/database/#database-systems-and-services","title":"Database Systems and Services","text":"<ul> <li>File System Storage: Primary data storage using JSON/YAML files</li> <li>SQLite: Lightweight database for metadata and indexing</li> <li>PostgreSQL: Relational database (if used for production)</li> <li>Redis: Caching and session storage</li> <li>Backup Services: Data backup and recovery systems</li> </ul>"},{"location":"api/external/database/#core-database-functions","title":"Core Database Functions","text":"<ul> <li>CRUD Operations: Create, Read, Update, Delete operations</li> <li>Transaction Management: Ensuring data consistency</li> <li>Query Optimization: Efficient data retrieval</li> <li>Data Validation: Ensuring data integrity</li> <li>Backup and Recovery: Data protection and restoration</li> </ul>"},{"location":"api/external/database/#integration-points","title":"Integration Points","text":""},{"location":"api/external/database/#data-storage-architecture","title":"Data Storage Architecture","text":"<ol> <li>File-based Storage: JSON/YAML files for graph data</li> <li>Registry Management: Centralized metadata storage</li> <li>User Data Isolation: Per-user data organization</li> <li>Atomic Operations: Ensuring data consistency</li> <li>Backup Systems: Regular data protection</li> </ol>"},{"location":"api/external/database/#data-management","title":"Data Management","text":"<ul> <li>User Data: User-specific graphs and configurations</li> <li>Global Data: Shared templates and schemas</li> <li>System Data: Configuration and metadata</li> <li>Temporary Data: Cache and session information</li> <li>Backup Data: Historical versions and recovery points</li> </ul>"},{"location":"api/external/database/#configuration","title":"Configuration","text":"<p>Database operations are configured through: - Storage path configuration - Database connection settings - Backup and recovery policies - Performance optimization settings - Security and access controls</p>"},{"location":"api/external/database/#features","title":"Features","text":""},{"location":"api/external/database/#data-persistence","title":"Data Persistence","text":"<ul> <li>Atomic Writes: Ensuring data consistency</li> <li>Version Control: Tracking data changes</li> <li>Rollback Capability: Reverting to previous states</li> <li>Data Validation: Ensuring data quality</li> <li>Compression: Optimizing storage space</li> </ul>"},{"location":"api/external/database/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Caching: Frequently accessed data caching</li> <li>Indexing: Fast data retrieval</li> <li>Query Optimization: Efficient data access patterns</li> <li>Connection Pooling: Managing database connections</li> <li>Load Balancing: Distributing database load </li> </ul>"},{"location":"api/external/docs/","title":"Documentation","text":"<p>This section documents the external documentation services and tools that the NDF Studio system depends on.</p>"},{"location":"api/external/docs/#overview","title":"Overview","text":"<p>The NDF Studio system uses various documentation services for:</p> <ul> <li>API documentation generation and hosting</li> <li>Code documentation and reference</li> <li>User guides and tutorials</li> <li>System architecture documentation</li> <li>Development documentation</li> </ul>"},{"location":"api/external/docs/#external-dependencies","title":"External Dependencies","text":""},{"location":"api/external/docs/#documentation-tools-and-services","title":"Documentation Tools and Services","text":"<ul> <li>MkDocs: Static site generator for documentation</li> <li>Material for MkDocs: Documentation theme and components</li> <li>mkdocstrings: Automatic API documentation generation</li> <li>GitHub Pages: Documentation hosting and deployment</li> <li>Sphinx: Alternative documentation generator (if used)</li> </ul>"},{"location":"api/external/docs/#documentation-features","title":"Documentation Features","text":"<ul> <li>Auto-generated API docs: From code docstrings and type hints</li> <li>Interactive examples: Code samples and demonstrations</li> <li>Search functionality: Full-text search across documentation</li> <li>Version control: Documentation versioning and history</li> <li>Multi-format export: PDF, HTML, and other formats</li> </ul>"},{"location":"api/external/docs/#integration-points","title":"Integration Points","text":""},{"location":"api/external/docs/#documentation-generation-pipeline","title":"Documentation Generation Pipeline","text":"<ol> <li>Code Analysis: Extracting docstrings and type information</li> <li>API Documentation: Generating endpoint documentation</li> <li>Example Generation: Creating usage examples</li> <li>Site Building: Compiling static documentation site</li> <li>Deployment: Publishing to hosting platform</li> </ol>"},{"location":"api/external/docs/#documentation-types","title":"Documentation Types","text":"<ul> <li>API Reference: Complete API endpoint documentation</li> <li>User Guides: Step-by-step usage instructions</li> <li>Developer Guides: Technical implementation details</li> <li>Architecture Docs: System design and structure</li> <li>Tutorials: Hands-on learning materials</li> </ul>"},{"location":"api/external/docs/#configuration","title":"Configuration","text":"<p>Documentation services are configured through: - MkDocs configuration files - Theme and styling settings - Search and navigation options - Deployment and hosting settings - Custom plugins and extensions</p>"},{"location":"api/external/docs/#maintenance","title":"Maintenance","text":"<ul> <li>Documentation is automatically updated with code changes</li> <li>Regular reviews ensure accuracy and completeness</li> <li>User feedback is incorporated into documentation</li> <li>Version control tracks documentation history </li> </ul>"},{"location":"api/external/filesystem/","title":"File System Operations","text":"<p>This section documents the file system operations and storage management services that the NDF Studio system depends on.</p>"},{"location":"api/external/filesystem/#overview","title":"Overview","text":"<p>The NDF Studio system uses file system services for:</p> <ul> <li>Data file storage and management</li> <li>Configuration file handling</li> <li>Backup and recovery operations</li> <li>File format conversions</li> <li>Directory structure management</li> </ul>"},{"location":"api/external/filesystem/#external-dependencies","title":"External Dependencies","text":""},{"location":"api/external/filesystem/#file-system-services-and-libraries","title":"File System Services and Libraries","text":"<ul> <li>Python pathlib: Modern file system path handling</li> <li>JSON/YAML Libraries: Data serialization and deserialization</li> <li>File Watchers: Monitoring file system changes</li> <li>Compression Libraries: Data compression and archiving</li> <li>File Locking: Concurrent access management</li> </ul>"},{"location":"api/external/filesystem/#core-file-system-functions","title":"Core File System Functions","text":"<ul> <li>File I/O Operations: Reading and writing files</li> <li>Directory Management: Creating and organizing directories</li> <li>File Format Handling: JSON, YAML, and other formats</li> <li>Atomic Operations: Ensuring file consistency</li> <li>Backup and Recovery: File protection and restoration</li> </ul>"},{"location":"api/external/filesystem/#integration-points","title":"Integration Points","text":""},{"location":"api/external/filesystem/#file-system-architecture","title":"File System Architecture","text":"<ol> <li>User Data Directories: Organized by user ID</li> <li>Graph Data Storage: JSON files for graph structures</li> <li>Configuration Files: YAML files for settings</li> <li>Backup Directories: Historical data preservation</li> <li>Temporary Files: Cache and processing files</li> </ol>"},{"location":"api/external/filesystem/#data-organization","title":"Data Organization","text":"<ul> <li>User Isolation: Separate directories per user</li> <li>Graph Organization: Hierarchical graph data structure</li> <li>Registry Files: Centralized metadata storage</li> <li>Template Storage: Reusable graph templates</li> <li>Log Files: System and application logs</li> </ul>"},{"location":"api/external/filesystem/#configuration","title":"Configuration","text":"<p>File system operations are configured through: - Base directory paths - File permissions and access controls - Backup and retention policies - File format preferences - Performance optimization settings</p>"},{"location":"api/external/filesystem/#features","title":"Features","text":""},{"location":"api/external/filesystem/#file-operations","title":"File Operations","text":"<ul> <li>Atomic Writes: Ensuring file consistency</li> <li>File Locking: Preventing concurrent access conflicts</li> <li>Error Handling: Graceful failure management</li> <li>File Validation: Ensuring file integrity</li> <li>Compression: Optimizing storage space</li> </ul>"},{"location":"api/external/filesystem/#directory-management","title":"Directory Management","text":"<ul> <li>Automatic Creation: Creating necessary directories</li> <li>Permission Management: Setting appropriate access rights</li> <li>Cleanup Operations: Removing temporary files</li> <li>Structure Validation: Ensuring proper organization</li> <li>Migration Support: Updating directory structures</li> </ul>"},{"location":"api/external/filesystem/#data-formats","title":"Data Formats","text":"<ul> <li>JSON Storage: Primary data format for graphs</li> <li>YAML Configuration: Human-readable configuration files</li> <li>Binary Formats: Efficient storage for large datasets</li> <li>Export Formats: Multiple output format support</li> <li>Import Formats: Flexible input format handling </li> </ul>"},{"location":"api/external/llm/","title":"LLM Endpoints","text":"<p>This section documents the Large Language Model (LLM) endpoints and services that the NDF Studio system depends on.</p>"},{"location":"api/external/llm/#overview","title":"Overview","text":"<p>The NDF Studio system integrates with LLM services for:</p> <ul> <li>Natural language generation and completion</li> <li>Code generation and assistance</li> <li>Content summarization and analysis</li> <li>Intelligent suggestions and recommendations</li> <li>Automated content creation</li> </ul>"},{"location":"api/external/llm/#external-dependencies","title":"External Dependencies","text":""},{"location":"api/external/llm/#llm-services-and-apis","title":"LLM Services and APIs","text":"<ul> <li>OpenAI GPT Models: Primary language model for text generation</li> <li>Mistral AI: Alternative language model for specific tasks</li> <li>Hugging Face Transformers: Local model inference capabilities</li> <li>Custom Fine-tuned Models: Domain-specific language models</li> </ul>"},{"location":"api/external/llm/#core-llm-functions","title":"Core LLM Functions","text":"<ul> <li>Text Generation: Creating natural language content</li> <li>Code Completion: Assisting with code generation</li> <li>Text Summarization: Creating concise summaries</li> <li>Question Answering: Providing intelligent responses</li> <li>Translation: Multi-language support</li> <li>Content Classification: Categorizing and organizing content</li> </ul>"},{"location":"api/external/llm/#integration-points","title":"Integration Points","text":""},{"location":"api/external/llm/#llm-service-integration","title":"LLM Service Integration","text":"<ol> <li>API Configuration: Setting up LLM service connections</li> <li>Prompt Engineering: Designing effective prompts for specific tasks</li> <li>Response Processing: Handling and validating LLM outputs</li> <li>Error Handling: Managing API failures and rate limits</li> <li>Caching: Optimizing performance with response caching</li> </ol>"},{"location":"api/external/llm/#use-cases-in-ndf-studio","title":"Use Cases in NDF Studio","text":"<ul> <li>CNL Generation: Creating controlled natural language from structured data</li> <li>Documentation Generation: Automatically generating documentation</li> <li>Code Assistance: Helping with development tasks</li> <li>Content Analysis: Analyzing and categorizing graph content</li> <li>User Assistance: Providing intelligent help and suggestions</li> </ul>"},{"location":"api/external/llm/#configuration","title":"Configuration","text":"<p>LLM services are configured through: - API key management - Model selection and parameters - Rate limiting and quota management - Response caching strategies - Fallback mechanisms for service failures</p>"},{"location":"api/external/llm/#security-and-privacy","title":"Security and Privacy","text":"<ul> <li>API keys are securely stored and managed</li> <li>User data is processed according to privacy policies</li> <li>Rate limiting prevents abuse and controls costs</li> <li>Response validation ensures quality and safety </li> </ul>"},{"location":"api/external/network/","title":"Network Operations","text":"<p>This section documents the network operations and external service communication that the NDF Studio system depends on.</p>"},{"location":"api/external/network/#overview","title":"Overview","text":"<p>The NDF Studio system uses network services for:</p> <ul> <li>API communication and external service integration</li> <li>Real-time data synchronization</li> <li>WebSocket connections for live updates</li> <li>HTTP/HTTPS client operations</li> <li>Network security and authentication</li> </ul>"},{"location":"api/external/network/#external-dependencies","title":"External Dependencies","text":""},{"location":"api/external/network/#network-libraries-and-services","title":"Network Libraries and Services","text":"<ul> <li>FastAPI: Web framework for API development</li> <li>HTTPX: Modern HTTP client for external requests</li> <li>WebSockets: Real-time bidirectional communication</li> <li>SSL/TLS: Secure communication protocols</li> <li>Proxy Support: Network proxy configuration</li> </ul>"},{"location":"api/external/network/#core-network-functions","title":"Core Network Functions","text":"<ul> <li>HTTP Client: Making external API requests</li> <li>WebSocket Management: Real-time communication</li> <li>Request/Response Handling: Processing network data</li> <li>Error Handling: Managing network failures</li> <li>Security: Authentication and encryption</li> </ul>"},{"location":"api/external/network/#integration-points","title":"Integration Points","text":""},{"location":"api/external/network/#external-service-integration","title":"External Service Integration","text":"<ol> <li>LLM APIs: Communication with language model services</li> <li>Authentication Services: External auth provider integration</li> <li>File Storage Services: Cloud storage integration</li> <li>Monitoring Services: System monitoring and logging</li> <li>Backup Services: Remote backup and recovery</li> </ol>"},{"location":"api/external/network/#network-architecture","title":"Network Architecture","text":"<ul> <li>RESTful APIs: Standard HTTP-based communication</li> <li>GraphQL: Alternative API query language (if used)</li> <li>WebSocket Connections: Real-time data streaming</li> <li>Message Queues: Asynchronous communication</li> <li>Load Balancing: Distributing network load</li> </ul>"},{"location":"api/external/network/#configuration","title":"Configuration","text":"<p>Network operations are configured through: - API endpoint URLs and authentication - Network timeout and retry settings - SSL/TLS certificate management - Proxy and firewall configurations - Rate limiting and throttling</p>"},{"location":"api/external/network/#features","title":"Features","text":""},{"location":"api/external/network/#communication-protocols","title":"Communication Protocols","text":"<ul> <li>HTTP/HTTPS: Standard web protocols</li> <li>WebSockets: Real-time bidirectional communication</li> <li>REST APIs: Representational state transfer</li> <li>GraphQL: Flexible data querying</li> <li>gRPC: High-performance RPC framework</li> </ul>"},{"location":"api/external/network/#security-features","title":"Security Features","text":"<ul> <li>SSL/TLS Encryption: Secure data transmission</li> <li>API Authentication: Token-based and key-based auth</li> <li>Rate Limiting: Preventing abuse and overload</li> <li>Request Validation: Ensuring data integrity</li> <li>Error Handling: Graceful failure management</li> </ul>"},{"location":"api/external/network/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Connection Pooling: Reusing network connections</li> <li>Caching: Reducing redundant requests</li> <li>Compression: Optimizing data transfer</li> <li>Load Balancing: Distributing network load</li> <li>Monitoring: Tracking network performance </li> </ul>"},{"location":"api/external/nlp/","title":"NLP Support","text":"<p>This section documents the Natural Language Processing (NLP) services and capabilities that the NDF Studio system depends on.</p>"},{"location":"api/external/nlp/#overview","title":"Overview","text":"<p>The NDF Studio system uses NLP services for:</p> <ul> <li>Parsing CNL (Controlled Natural Language) input</li> <li>Extracting structured data from natural language</li> <li>Text analysis and processing</li> <li>Language understanding and interpretation</li> </ul>"},{"location":"api/external/nlp/#external-dependencies","title":"External Dependencies","text":""},{"location":"api/external/nlp/#nlp-libraries-and-services","title":"NLP Libraries and Services","text":"<ul> <li>spaCy: Advanced natural language processing</li> <li>NLTK: Natural language toolkit for text processing</li> <li>TextBlob: Simple text processing and sentiment analysis</li> <li>Custom CNL Parser: Domain-specific language parsing</li> </ul>"},{"location":"api/external/nlp/#core-nlp-functions","title":"Core NLP Functions","text":"<ul> <li>Text Tokenization: Breaking text into meaningful units</li> <li>Part-of-Speech Tagging: Identifying grammatical components</li> <li>Named Entity Recognition: Extracting entities from text</li> <li>Dependency Parsing: Understanding sentence structure</li> <li>Semantic Analysis: Understanding meaning and context</li> </ul>"},{"location":"api/external/nlp/#integration-points","title":"Integration Points","text":""},{"location":"api/external/nlp/#cnl-processing-pipeline","title":"CNL Processing Pipeline","text":"<ol> <li>Input Validation: Checking CNL syntax and structure</li> <li>Tokenization: Breaking CNL into parseable units</li> <li>Entity Extraction: Identifying nodes, relations, and attributes</li> <li>Structure Building: Creating graph structures from parsed text</li> <li>Validation: Ensuring extracted structures are valid</li> </ol>"},{"location":"api/external/nlp/#text-analysis-features","title":"Text Analysis Features","text":"<ul> <li>Sentiment Analysis: Understanding emotional context</li> <li>Keyword Extraction: Identifying important terms</li> <li>Text Classification: Categorizing content</li> <li>Summarization: Creating concise representations</li> </ul>"},{"location":"api/external/nlp/#configuration","title":"Configuration","text":"<p>NLP services are configured through: - Language model selection - Processing pipeline configuration - Performance optimization settings - Custom domain-specific rules </p>"},{"location":"api/routes/atomic_routes/","title":"Atomic Routes","text":"<p>This section documents the atomic operations and data validation API endpoints.</p>"},{"location":"api/routes/atomic_routes/#backend.routes.atomic_routes","title":"<code>backend.routes.atomic_routes</code>","text":"<p>Atomic Operations and Data Validation Routes</p> <p>This module provides API endpoints for: 1. Data consistency validation 2. Backup management 3. Atomic operation status 4. Data integrity checks</p> <p>These endpoints ensure the NDF Studio backend maintains data integrity and provides tools for monitoring and maintaining the system.</p>"},{"location":"api/routes/atomic_routes/#backend.routes.atomic_routes-classes","title":"Classes","text":""},{"location":"api/routes/atomic_routes/#backend.routes.atomic_routes-functions","title":"Functions","text":""},{"location":"api/routes/atomic_routes/#backend.routes.atomic_routes.validate_graph_consistency_endpoint","title":"<code>validate_graph_consistency_endpoint(user_id: str, graph_id: str)</code>","text":"<p>Validate the consistency of a graph's data.</p> <p>This endpoint performs comprehensive validation of: - Node registry consistency - Relation registry consistency - Attribute registry consistency - File existence checks - Reference integrity</p> <p>Returns validation results with issues and warnings.</p> Source code in <code>backend/routes/atomic_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/validate\")\ndef validate_graph_consistency_endpoint(user_id: str, graph_id: str):\n    \"\"\"\n    Validate the consistency of a graph's data.\n\n    This endpoint performs comprehensive validation of:\n    - Node registry consistency\n    - Relation registry consistency  \n    - Attribute registry consistency\n    - File existence checks\n    - Reference integrity\n\n    Returns validation results with issues and warnings.\n    \"\"\"\n    try:\n        validation_result = validate_data_consistency(user_id)\n        return validation_result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Validation failed: {str(e)}\")\n</code></pre>"},{"location":"api/routes/atomic_routes/#backend.routes.atomic_routes.cleanup_old_backups_endpoint","title":"<code>cleanup_old_backups_endpoint(user_id: str, max_age_hours: int = 24)</code>","text":"<p>Clean up old backup directories for a user.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>User ID</p> required <code>max_age_hours</code> <code>int</code> <p>Maximum age of backups to keep (default: 24 hours)</p> <code>24</code> <p>Returns:</p> Type Description <p>Number of backups cleaned up</p> Source code in <code>backend/routes/atomic_routes.py</code> <pre><code>@router.post(\"/users/{user_id}/cleanup-backups\")\ndef cleanup_old_backups_endpoint(user_id: str, max_age_hours: int = 24):\n    \"\"\"\n    Clean up old backup directories for a user.\n\n    Args:\n        user_id: User ID\n        max_age_hours: Maximum age of backups to keep (default: 24 hours)\n\n    Returns:\n        Number of backups cleaned up\n    \"\"\"\n    try:\n        cleaned_count = cleanup_old_backups(user_id, max_age_hours)\n        return {\n            \"status\": \"Backup cleanup completed\",\n            \"cleaned_count\": cleaned_count,\n            \"max_age_hours\": max_age_hours\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Backup cleanup failed: {str(e)}\")\n</code></pre>"},{"location":"api/routes/atomic_routes/#backend.routes.atomic_routes.get_backup_status","title":"<code>get_backup_status(user_id: str)</code>","text":"<p>Get the status of backups for a user.</p> <p>Returns:</p> Type Description <p>Information about existing backups</p> Source code in <code>backend/routes/atomic_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/backups/status\")\ndef get_backup_status(user_id: str):\n    \"\"\"\n    Get the status of backups for a user.\n\n    Returns:\n        Information about existing backups\n    \"\"\"\n    try:\n        backup_dir = Path(f\"graph_data/users/{user_id}/backups\")\n        if not backup_dir.exists():\n            return {\n                \"backup_count\": 0,\n                \"backups\": [],\n                \"total_size_mb\": 0\n            }\n\n        backups = []\n        total_size = 0\n\n        for backup in backup_dir.iterdir():\n            if backup.is_dir():\n                # Calculate backup size\n                backup_size = sum(f.stat().st_size for f in backup.rglob('*') if f.is_file())\n                total_size += backup_size\n\n                # Extract timestamp and operation from directory name\n                try:\n                    timestamp_str = backup.name.split('_')[0]\n                    operation = '_'.join(backup.name.split('_')[1:])\n                    backups.append({\n                        \"name\": backup.name,\n                        \"timestamp\": int(timestamp_str),\n                        \"operation\": operation,\n                        \"size_mb\": round(backup_size / (1024 * 1024), 2)\n                    })\n                except (ValueError, IndexError):\n                    backups.append({\n                        \"name\": backup.name,\n                        \"timestamp\": None,\n                        \"operation\": \"unknown\",\n                        \"size_mb\": round(backup_size / (1024 * 1024), 2)\n                    })\n\n        # Sort by timestamp (newest first)\n        backups.sort(key=lambda x: x[\"timestamp\"] or 0, reverse=True)\n\n        return {\n            \"backup_count\": len(backups),\n            \"backups\": backups,\n            \"total_size_mb\": round(total_size / (1024 * 1024), 2)\n        }\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to get backup status: {str(e)}\")\n</code></pre>"},{"location":"api/routes/atomic_routes/#backend.routes.atomic_routes.get_atomic_operation_status","title":"<code>get_atomic_operation_status(user_id: str, graph_id: str)</code>","text":"<p>Get the status of atomic operations for a graph.</p> <p>Returns:</p> Type Description <p>Information about the graph's atomic operation status</p> Source code in <code>backend/routes/atomic_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/atomic-status\")\ndef get_atomic_operation_status(user_id: str, graph_id: str):\n    \"\"\"\n    Get the status of atomic operations for a graph.\n\n    Returns:\n        Information about the graph's atomic operation status\n    \"\"\"\n    try:\n        # Check if graph directory exists\n        graph_dir = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}\")\n        if not graph_dir.exists():\n            return {\n                \"graph_exists\": False,\n                \"composed_files\": [],\n                \"last_modified\": None\n            }\n\n        # Check composed files\n        composed_files = []\n        for composed_file in [\"composed.json\", \"composed.yaml\", \"polymorphic_composed.json\"]:\n            file_path = graph_dir / composed_file\n            if file_path.exists():\n                stat = file_path.stat()\n                composed_files.append({\n                    \"name\": composed_file,\n                    \"exists\": True,\n                    \"size_bytes\": stat.st_size,\n                    \"last_modified\": stat.st_mtime\n                })\n            else:\n                composed_files.append({\n                    \"name\": composed_file,\n                    \"exists\": False,\n                    \"size_bytes\": 0,\n                    \"last_modified\": None\n                })\n\n        # Get graph directory last modified time\n        graph_stat = graph_dir.stat()\n\n        return {\n            \"graph_exists\": True,\n            \"composed_files\": composed_files,\n            \"last_modified\": graph_stat.st_mtime,\n            \"directory_size_mb\": round(sum(f.stat().st_size for f in graph_dir.rglob('*') if f.is_file()) / (1024 * 1024), 2)\n        }\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to get atomic operation status: {str(e)}\")\n</code></pre>"},{"location":"api/routes/atomic_routes/#backend.routes.atomic_routes.force_regenerate_composed_files","title":"<code>force_regenerate_composed_files(user_id: str, graph_id: str)</code>","text":"<p>Force regeneration of all composed files for a graph.</p> <p>This endpoint triggers a complete regeneration of: - composed.json - composed.yaml - polymorphic_composed.json</p> <p>Returns:</p> Type Description <p>Status of the regeneration operation</p> Source code in <code>backend/routes/atomic_routes.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/force-regenerate\")\ndef force_regenerate_composed_files(user_id: str, graph_id: str):\n    \"\"\"\n    Force regeneration of all composed files for a graph.\n\n    This endpoint triggers a complete regeneration of:\n    - composed.json\n    - composed.yaml  \n    - polymorphic_composed.json\n\n    Returns:\n        Status of the regeneration operation\n    \"\"\"\n    try:\n        from backend.core.registry import load_node_registry\n        from backend.core.compose import compose_graph\n        from backend.core.atomic_ops import atomic_composed_save\n\n        # Get all nodes for this graph\n        node_registry = load_node_registry(user_id)\n        node_ids = [nid for nid, entry in node_registry.items() if graph_id in (entry.get('graphs') or [])]\n\n        # Get graph description\n        metadata_path = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}/metadata.yaml\")\n        graph_description = \"\"\n        if metadata_path.exists():\n            import yaml\n            with open(metadata_path, \"r\") as f:\n                metadata = yaml.safe_load(f) or {}\n                graph_description = metadata.get(\"description\", \"\")\n\n        # Regenerate composed files\n        composed_data = compose_graph(user_id, graph_id, node_ids, graph_description)\n        if composed_data:\n            atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"json\")\n            atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"yaml\")\n            atomic_composed_save(user_id, graph_id, composed_data[\"polymorphic\"], \"polymorphic\")\n\n        return {\n            \"status\": \"Composed files regenerated successfully\",\n            \"node_count\": len(node_ids),\n            \"files_generated\": [\"composed.json\", \"composed.yaml\", \"polymorphic_composed.json\"]\n        }\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to regenerate composed files: {str(e)}\") \n</code></pre>"},{"location":"api/routes/functions/","title":"Functions Routes","text":"<p>Functions routes provide endpoints for managing and executing custom functions within the NDF system.</p>"},{"location":"api/routes/functions/#overview","title":"Overview","text":"<p>The functions routes module handles the registration, execution, and management of custom functions that can be used to process nodes, perform calculations, or implement business logic.</p>"},{"location":"api/routes/functions/#key-endpoints","title":"Key Endpoints","text":""},{"location":"api/routes/functions/#function-management","title":"Function Management","text":"<ul> <li><code>GET /functions</code> - List all available functions</li> <li><code>POST /functions</code> - Register new function</li> <li><code>PUT /functions/{function_id}</code> - Update function definition</li> <li><code>DELETE /functions/{function_id}</code> - Remove function</li> </ul>"},{"location":"api/routes/functions/#function-execution","title":"Function Execution","text":"<ul> <li><code>POST /functions/{function_id}/execute</code> - Execute function with parameters</li> <li><code>POST /functions/batch</code> - Execute multiple functions</li> <li><code>GET /functions/{function_id}/status</code> - Get function execution status</li> </ul>"},{"location":"api/routes/functions/#function-metadata","title":"Function Metadata","text":"<ul> <li><code>GET /functions/{function_id}</code> - Get function details</li> <li><code>GET /functions/{function_id}/schema</code> - Get function input/output schema</li> </ul>"},{"location":"api/routes/functions/#usage-examples","title":"Usage Examples","text":""},{"location":"api/routes/functions/#registering-a-custom-function","title":"Registering a Custom Function","text":"<pre><code>import requests\n\nfunction_def = {\n    \"name\": \"calculate_molecular_weight\",\n    \"description\": \"Calculate molecular weight from chemical formula\",\n    \"parameters\": {\n        \"formula\": {\"type\": \"string\", \"required\": True}\n    },\n    \"code\": \"def calculate_molecular_weight(formula): ...\"\n}\n\nresponse = requests.post(\"http://localhost:8000/functions\", json=function_def)\n</code></pre>"},{"location":"api/routes/functions/#executing-a-function","title":"Executing a Function","text":"<pre><code>params = {\"formula\": \"H2O\"}\nresponse = requests.post(\"http://localhost:8000/functions/calculate_molecular_weight/execute\", \n                        json=params)\nresult = response.json()\nprint(f\"Molecular weight: {result['result']}\")\n</code></pre>"},{"location":"api/routes/functions/#listing-available-functions","title":"Listing Available Functions","text":"<pre><code>response = requests.get(\"http://localhost:8000/functions\")\nfunctions = response.json()\nfor func in functions:\n    print(f\"- {func['name']}: {func['description']}\")\n</code></pre>"},{"location":"api/routes/functions/#function-types","title":"Function Types","text":"<p>Functions can be categorized by type: - Node Processing: Functions that transform or analyze nodes - Calculation: Mathematical and computational functions - Validation: Functions that validate data structures - Utility: Helper functions for common operations</p>"},{"location":"api/routes/functions/#error-handling","title":"Error Handling","text":"<ul> <li><code>200</code> - Success</li> <li><code>400</code> - Invalid function definition or parameters</li> <li><code>404</code> - Function not found</li> <li><code>422</code> - Function execution error</li> <li><code>500</code> - Internal server error</li> </ul>"},{"location":"api/routes/functions/#dependencies","title":"Dependencies","text":"<ul> <li>Function execution engine</li> <li>Parameter validation</li> <li>Security sandboxing</li> <li>Result caching </li> </ul>"},{"location":"api/routes/graph/","title":"Graph Routes","text":"<p>Graph routes provide basic graph management and visualization endpoints.</p>"},{"location":"api/routes/graph/#overview","title":"Overview","text":"<p>The graph routes module handles fundamental graph operations including graph creation, basic queries, and graph visualization endpoints.</p>"},{"location":"api/routes/graph/#key-endpoints","title":"Key Endpoints","text":""},{"location":"api/routes/graph/#graph-management","title":"Graph Management","text":"<ul> <li><code>GET /graph</code> - Get basic graph information</li> <li><code>POST /graph</code> - Create new graph</li> <li><code>PUT /graph</code> - Update graph properties</li> <li><code>DELETE /graph</code> - Delete graph</li> </ul>"},{"location":"api/routes/graph/#graph-queries","title":"Graph Queries","text":"<ul> <li><code>GET /graph/nodes</code> - Get all nodes in graph</li> <li><code>GET /graph/edges</code> - Get all edges in graph</li> <li><code>GET /graph/structure</code> - Get graph structure</li> </ul>"},{"location":"api/routes/graph/#graph-visualization","title":"Graph Visualization","text":"<ul> <li><code>GET /graph/visualize</code> - Get graph visualization data</li> <li><code>POST /graph/layout</code> - Apply layout algorithm</li> </ul>"},{"location":"api/routes/graph/#usage-examples","title":"Usage Examples","text":""},{"location":"api/routes/graph/#getting-graph-information","title":"Getting Graph Information","text":"<pre><code>import requests\n\nresponse = requests.get(\"http://localhost:8000/graph\")\ngraph_info = response.json()\nprint(f\"Graph has {graph_info['node_count']} nodes and {graph_info['edge_count']} edges\")\n</code></pre>"},{"location":"api/routes/graph/#creating-a-new-graph","title":"Creating a New Graph","text":"<pre><code>graph_data = {\n    \"name\": \"my_graph\",\n    \"description\": \"A sample graph\",\n    \"type\": \"concept_graph\"\n}\n\nresponse = requests.post(\"http://localhost:8000/graph\", json=graph_data)\n</code></pre>"},{"location":"api/routes/graph/#getting-graph-visualization-data","title":"Getting Graph Visualization Data","text":"<pre><code>response = requests.get(\"http://localhost:8000/graph/visualize\")\nvisualization_data = response.json()\n# Use with Cytoscape or other visualization libraries\n</code></pre>"},{"location":"api/routes/graph/#graph-properties","title":"Graph Properties","text":"<p>Graphs can have various properties: - Name and description - Graph type (concept, knowledge, etc.) - Metadata and tags - Creation and modification timestamps</p>"},{"location":"api/routes/graph/#error-handling","title":"Error Handling","text":"<ul> <li><code>200</code> - Success</li> <li><code>400</code> - Invalid graph data</li> <li><code>404</code> - Graph not found</li> <li><code>500</code> - Internal server error</li> </ul>"},{"location":"api/routes/graph/#dependencies","title":"Dependencies","text":"<ul> <li>Graph state management</li> <li>Basic graph operations</li> <li>Visualization utilities </li> </ul>"},{"location":"api/routes/graph_ops/","title":"Graph Operations","text":"<p>This section documents the graph operation endpoints and their functionality.</p>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops","title":"<code>backend.routes.graph_ops</code>","text":"<p>This module provides API endpoints and helper functions for managing node attributes and relations in the NDF Studio knowledge graph backend.</p> <p>Key Features: - All node data is stored as JSON files under graph_data/users/{user_id}/nodes/{node_id}.json. - Attribute and relation types are validated against global schema files (attribute_types.json, relation_types.json). - Endpoints support full CRUD (create, update, delete) for both attributes and relations. - When creating or updating a relation, if the source or target node does not exist, the canonical create_node function from nodes.py is called to ensure proper node creation and registry updates. - All endpoints are designed for robust integration with the frontend, supporting both selection and creation of new nodes/relations/attributes.</p> <p>Endpoints: - POST   /users/{user_id}/graphs/{graph_id}/attribute/create - PUT    /users/{user_id}/graphs/{graph_id}/attribute/update/{node_id}/{attr_name} - DELETE /users/{user_id}/graphs/{graph_id}/attribute/delete/{node_id}/{attr_name} - POST   /users/{user_id}/graphs/{graph_id}/relation/create - PUT    /users/{user_id}/graphs/{graph_id}/relation/update/{source}/{name}/{target} - DELETE /users/{user_id}/graphs/{graph_id}/relation/delete/{source}/{name}/{target}</p> <p>Helpers: - node_path, load_node, save_node: JSON-based node storage helpers. - load_schema: Loads global schema files for validation.</p> <p>All logic is designed to be robust, extensible, and consistent with the rest of the backend.</p>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops-classes","title":"Classes","text":""},{"location":"api/routes/graph_ops/#backend.routes.graph_ops-functions","title":"Functions","text":""},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.get_attribute_node","title":"<code>get_attribute_node(user_id: str, graph_id: str, attribute_id: str)</code>","text":"<p>Get a specific attribute node by its ID</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/attributeNodes/{attribute_id}\")\ndef get_attribute_node(user_id: str, graph_id: str, attribute_id: str):\n    \"\"\"Get a specific attribute node by its ID\"\"\"\n    attr_path = f\"graph_data/users/{user_id}/attributeNodes/{attribute_id}.json\"\n    if not os.path.exists(attr_path):\n        raise HTTPException(status_code=404, detail=\"AttributeNode not found\")\n    with open(attr_path, \"r\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.unlist_attribute_from_morph","title":"<code>unlist_attribute_from_morph(user_id: str, graph_id: str, node_id: str, attr_name: str, request: MorphOperationRequest)</code>","text":"<p>Remove an attribute from a specific morph without deleting the attribute itself. The attribute continues to exist in other morphs.</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/attribute/unlist_from_morph/{node_id}/{attr_name}\")\ndef unlist_attribute_from_morph(user_id: str, graph_id: str, node_id: str, attr_name: str, request: MorphOperationRequest):\n    \"\"\"\n    Remove an attribute from a specific morph without deleting the attribute itself.\n    The attribute continues to exist in other morphs.\n    \"\"\"\n    try:\n        with graph_transaction(user_id, graph_id, \"unlist_attribute_from_morph\") as backup_dir:\n            morph_id = request.morph_id\n            # Find the attributeNode id by node_id and attr_name\n            reg_path = Path(f\"graph_data/users/{user_id}/attribute_registry.json\")\n            registry = load_registry(reg_path)\n            attr_id = None\n            for k, v in registry.items():\n                if v.get(\"source_id\") == node_id and v.get(\"name\") == attr_name:\n                    attr_id = k\n                    break\n            if not attr_id:\n                raise HTTPException(status_code=404, detail=\"AttributeNode not found\")\n\n            # Update source node to remove attribute from specific morph\n            source_node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n            if not source_node_path.exists():\n                raise HTTPException(status_code=404, detail=\"Source node not found\")\n\n            source_node = load_json_file(source_node_path)\n\n            # Find the specific morph and remove the attribute\n            morph_found = False\n            for morph in source_node.get(\"morphs\", []):\n                if morph.get(\"morph_id\") == morph_id:\n                    morph_found = True\n                    if \"attributeNode_ids\" in morph and attr_id in morph[\"attributeNode_ids\"]:\n                        morph[\"attributeNode_ids\"].remove(attr_id)\n                        break\n\n            if not morph_found:\n                raise HTTPException(status_code=404, detail=f\"Morph {morph_id} not found\")\n\n            # Atomically save updated source node\n            atomic_node_save(user_id, node_id, source_node)\n\n            # Regenerate composed files atomically\n            try:\n                node_ids = get_graph_node_ids(user_id, graph_id)\n                metadata_path = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}/metadata.yaml\")\n                graph_description = \"\"\n                if metadata_path.exists():\n                    import yaml\n                    with open(metadata_path, \"r\") as f:\n                        metadata = yaml.safe_load(f) or {}\n                        graph_description = metadata.get(\"description\", \"\")\n\n                composed_data = compose_graph(user_id, graph_id, node_ids, graph_description)\n                if composed_data:\n                    atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"json\")\n                    atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"yaml\")\n                    atomic_composed_save(user_id, graph_id, composed_data[\"polymorphic\"], \"polymorphic\")\n            except Exception as e:\n                print(f\"Warning: Failed to regenerate composed files: {e}\")\n\n            return {\"status\": \"Attribute unlisted from morph\", \"attribute_id\": attr_id, \"morph_id\": morph_id}\n\n    except AtomicityError as e:\n        raise HTTPException(status_code=500, detail=f\"Atomic operation failed: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to unlist attribute from morph: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.add_attribute_to_morph","title":"<code>add_attribute_to_morph(user_id: str, graph_id: str, node_id: str, attr_name: str, request: MorphOperationRequest)</code>","text":"<p>Add an existing attribute to a specific morph.</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/attribute/add_to_morph/{node_id}/{attr_name}\")\ndef add_attribute_to_morph(user_id: str, graph_id: str, node_id: str, attr_name: str, request: MorphOperationRequest):\n    \"\"\"\n    Add an existing attribute to a specific morph.\n    \"\"\"\n    try:\n        with graph_transaction(user_id, graph_id, \"add_attribute_to_morph\") as backup_dir:\n            morph_id = request.morph_id\n            # Find the attributeNode id by node_id and attr_name\n            reg_path = Path(f\"graph_data/users/{user_id}/attribute_registry.json\")\n            registry = load_registry(reg_path)\n            attr_id = None\n            for k, v in registry.items():\n                if v.get(\"source_id\") == node_id and v.get(\"name\") == attr_name:\n                    attr_id = k\n                    break\n            if not attr_id:\n                raise HTTPException(status_code=404, detail=\"AttributeNode not found\")\n\n            # Update source node to add attribute to specific morph\n            source_node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n            if not source_node_path.exists():\n                raise HTTPException(status_code=404, detail=\"Source node not found\")\n\n            source_node = load_json_file(source_node_path)\n\n            # Find the specific morph and add the attribute\n            morph_found = False\n            for morph in source_node.get(\"morphs\", []):\n                if morph.get(\"morph_id\") == morph_id:\n                    morph_found = True\n                    if \"attributeNode_ids\" not in morph:\n                        morph[\"attributeNode_ids\"] = []\n                    if attr_id not in morph[\"attributeNode_ids\"]:\n                        morph[\"attributeNode_ids\"].append(attr_id)\n                    break\n\n            if not morph_found:\n                raise HTTPException(status_code=404, detail=f\"Morph {morph_id} not found\")\n\n            # Atomically save updated source node\n            atomic_node_save(user_id, node_id, source_node)\n\n            # Regenerate composed files atomically\n            try:\n                node_ids = get_graph_node_ids(user_id, graph_id)\n                metadata_path = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}/metadata.yaml\")\n                graph_description = \"\"\n                if metadata_path.exists():\n                    import yaml\n                    with open(metadata_path, \"r\") as f:\n                        metadata = yaml.safe_load(f) or {}\n                        graph_description = metadata.get(\"description\", \"\")\n\n                composed_data = compose_graph(user_id, graph_id, node_ids, graph_description)\n                if composed_data:\n                    atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"json\")\n                    atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"yaml\")\n                    atomic_composed_save(user_id, graph_id, composed_data[\"polymorphic\"], \"polymorphic\")\n            except Exception as e:\n                print(f\"Warning: Failed to regenerate composed files: {e}\")\n\n            return {\"status\": \"Attribute added to morph\", \"attribute_id\": attr_id, \"morph_id\": morph_id}\n\n    except AtomicityError as e:\n        raise HTTPException(status_code=500, detail=f\"Atomic operation failed: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to add attribute to morph: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.move_attribute_to_morph","title":"<code>move_attribute_to_morph(user_id: str, graph_id: str, node_id: str, attr_name: str, request: MoveMorphRequest)</code>","text":"<p>Move an attribute from one morph to another.</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/attribute/move_to_morph/{node_id}/{attr_name}\")\ndef move_attribute_to_morph(user_id: str, graph_id: str, node_id: str, attr_name: str, request: MoveMorphRequest):\n    \"\"\"\n    Move an attribute from one morph to another.\n    \"\"\"\n    try:\n        print(f\"[DEBUG] move_attribute_to_morph called: user_id={user_id}, graph_id={graph_id}, node_id={node_id}, attr_name={attr_name}, from_morph_id={request.from_morph_id}, to_morph_id={request.to_morph_id}\")\n        with graph_transaction(user_id, graph_id, \"move_attribute_to_morph\") as backup_dir:\n            from_morph_id = request.from_morph_id\n            to_morph_id = request.to_morph_id\n            # Find the attributeNode id by node_id and attr_name\n            reg_path = Path(f\"graph_data/users/{user_id}/attribute_registry.json\")\n            registry = load_registry(reg_path)\n            attr_id = None\n            for k, v in registry.items():\n                if v.get(\"source_id\") == node_id and v.get(\"name\") == attr_name:\n                    attr_id = k\n                    break\n            print(f\"[DEBUG] Found attr_id: {attr_id}\")\n            if not attr_id:\n                raise HTTPException(status_code=404, detail=\"AttributeNode not found\")\n\n            # Update source node to move attribute between morphs\n            source_node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n            if not source_node_path.exists():\n                raise HTTPException(status_code=404, detail=\"Source node not found\")\n\n            source_node = load_json_file(source_node_path)\n\n            # Find both morphs\n            from_morph = None\n            to_morph = None\n            for morph in source_node.get(\"morphs\", []):\n                if morph.get(\"morph_id\") == from_morph_id:\n                    from_morph = morph\n                if morph.get(\"morph_id\") == to_morph_id:\n                    to_morph = morph\n            print(f\"[DEBUG] from_morph: {from_morph}\")\n            print(f\"[DEBUG] to_morph: {to_morph}\")\n            if not from_morph:\n                raise HTTPException(status_code=404, detail=f\"Source morph {from_morph_id} not found\")\n            if not to_morph:\n                raise HTTPException(status_code=404, detail=f\"Target morph {to_morph_id} not found\")\n            print(f\"[DEBUG] from_morph attributeNode_ids before: {from_morph.get('attributeNode_ids', [])}\")\n            print(f\"[DEBUG] to_morph attributeNode_ids before: {to_morph.get('attributeNode_ids', [])}\")\n            # Remove from source morph\n            if \"attributeNode_ids\" in from_morph and attr_id in from_morph[\"attributeNode_ids\"]:\n                from_morph[\"attributeNode_ids\"].remove(attr_id)\n            # Add to target morph\n            if \"attributeNode_ids\" not in to_morph:\n                to_morph[\"attributeNode_ids\"] = []\n            if attr_id not in to_morph[\"attributeNode_ids\"]:\n                to_morph[\"attributeNode_ids\"].append(attr_id)\n            print(f\"[DEBUG] from_morph attributeNode_ids after: {from_morph.get('attributeNode_ids', [])}\")\n            print(f\"[DEBUG] to_morph attributeNode_ids after: {to_morph.get('attributeNode_ids', [])}\")\n            # Atomically save updated source node\n            atomic_node_save(user_id, node_id, source_node)\n            # Regenerate composed files atomically\n            try:\n                node_ids = get_graph_node_ids(user_id, graph_id)\n                metadata_path = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}/metadata.yaml\")\n                graph_description = \"\"\n                if metadata_path.exists():\n                    import yaml\n                    with open(metadata_path, \"r\") as f:\n                        metadata = yaml.safe_load(f) or {}\n                        graph_description = metadata.get(\"description\", \"\")\n                composed_data = compose_graph(user_id, graph_id, node_ids, graph_description)\n                if composed_data:\n                    atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"json\")\n                    atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"yaml\")\n                    atomic_composed_save(user_id, graph_id, composed_data[\"polymorphic\"], \"polymorphic\")\n            except Exception as e:\n                print(f\"Warning: Failed to regenerate composed files: {e}\")\n            return {\"status\": \"Attribute moved between morphs\", \"attribute_id\": attr_id, \"from_morph_id\": from_morph_id, \"to_morph_id\": to_morph_id}\n    except AtomicityError as e:\n        raise HTTPException(status_code=500, detail=f\"Atomic operation failed: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to move attribute between morphs: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.copy_attribute_to_morph","title":"<code>copy_attribute_to_morph(user_id: str, graph_id: str, node_id: str, attr_name: str, request: MorphOperationRequest)</code>","text":"<p>Copy an existing attribute to a specific morph (keeps it in all other morphs as well).</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/attribute/copy_to_morph/{node_id}/{attr_name}\")\ndef copy_attribute_to_morph(user_id: str, graph_id: str, node_id: str, attr_name: str, request: MorphOperationRequest):\n    \"\"\"\n    Copy an existing attribute to a specific morph (keeps it in all other morphs as well).\n    \"\"\"\n    try:\n        with graph_transaction(user_id, graph_id, \"copy_attribute_to_morph\") as backup_dir:\n            morph_id = request.morph_id\n            # Find the attributeNode id by node_id and attr_name\n            reg_path = Path(f\"graph_data/users/{user_id}/attribute_registry.json\")\n            registry = load_registry(reg_path)\n            attr_id = None\n            for k, v in registry.items():\n                if v.get(\"source_id\") == node_id and v.get(\"name\") == attr_name:\n                    attr_id = k\n                    break\n\n            if not attr_id:\n                raise HTTPException(status_code=404, detail=\"Attribute not found\")\n\n            # Load the source node\n            source_node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n            if not source_node_path.exists():\n                raise HTTPException(status_code=404, detail=\"Source node not found\")\n\n            source_node = load_json_file(source_node_path)\n\n            # Ensure morphs array exists\n            if \"morphs\" not in source_node:\n                source_node[\"morphs\"] = []\n\n            # Find the target morph\n            target_morph = None\n            for morph in source_node[\"morphs\"]:\n                if morph.get(\"morph_id\") == morph_id:\n                    target_morph = morph\n                    break\n\n            if not target_morph:\n                raise HTTPException(status_code=404, detail=\"Target morph not found\")\n\n            # Ensure attributeNode_ids array exists\n            if \"attributeNode_ids\" not in target_morph:\n                target_morph[\"attributeNode_ids\"] = []\n\n            # Add attribute to the morph if not already present\n            if attr_id not in target_morph[\"attributeNode_ids\"]:\n                target_morph[\"attributeNode_ids\"].append(attr_id)\n\n                # Atomically save the updated node\n                atomic_node_save(user_id, node_id, source_node)\n\n                # Regenerate composed files\n                try:\n                    node_ids = get_graph_node_ids(user_id, graph_id)\n                    metadata_path = Path(f\"graph_data/users/{user_id}/graphs/{graph_id}/metadata.yaml\")\n                    graph_description = \"\"\n                    if metadata_path.exists():\n                        import yaml\n                        with open(metadata_path, \"r\") as f:\n                            metadata = yaml.safe_load(f) or {}\n                            graph_description = metadata.get(\"description\", \"\")\n\n                    composed_data = compose_graph(user_id, graph_id, node_ids, graph_description)\n                    if composed_data:\n                        atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"json\")\n                        atomic_composed_save(user_id, graph_id, composed_data[\"cytoscape\"], \"yaml\")\n                        atomic_composed_save(user_id, graph_id, composed_data[\"polymorphic\"], \"polymorphic\")\n                except Exception as e:\n                    print(f\"Warning: Failed to regenerate composed files: {e}\")\n\n                return {\"status\": \"Attribute copied to morph\", \"attribute_id\": attr_id, \"morph_id\": morph_id}\n            else:\n                return {\"status\": \"Attribute already exists in target morph\", \"attribute_id\": attr_id, \"morph_id\": morph_id}\n\n    except AtomicityError as e:\n        raise HTTPException(status_code=500, detail=f\"Atomic operation failed: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to copy attribute to morph: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.list_attributes_by_morph","title":"<code>list_attributes_by_morph(user_id: str, graph_id: str, node_id: str)</code>","text":"<p>List all attributes organized by morph for a given node.</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/attribute/list_by_morph/{node_id}\")\ndef list_attributes_by_morph(user_id: str, graph_id: str, node_id: str):\n    \"\"\"\n    List all attributes organized by morph for a given node.\n    \"\"\"\n    try:\n        # Load source node\n        source_node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n        if not source_node_path.exists():\n            raise HTTPException(status_code=404, detail=\"Source node not found\")\n\n        source_node = load_json_file(source_node_path)\n\n        # Load attribute registry\n        reg_path = Path(f\"graph_data/users/{user_id}/attribute_registry.json\")\n        registry = load_registry(reg_path)\n\n        # Organize attributes by morph\n        morph_attributes = {}\n        for morph in source_node.get(\"morphs\", []):\n            morph_id = morph.get(\"morph_id\")\n            morph_name = morph.get(\"name\", \"Unknown\")\n            morph_attributes[morph_id] = {\n                \"morph_id\": morph_id,\n                \"morph_name\": morph_name,\n                \"attributes\": []\n            }\n\n            for attr_id in morph.get(\"attributeNode_ids\", []):\n                if attr_id in registry:\n                    attr_info = registry[attr_id]\n\n                    # Load full attribute data from the attribute file\n                    attr_file_path = Path(f\"graph_data/users/{user_id}/attributeNodes/{attr_id}.json\")\n                    full_attr_data = {}\n                    if attr_file_path.exists():\n                        try:\n                            full_attr_data = load_json_file(attr_file_path)\n                        except Exception as e:\n                            print(f\"Warning: Failed to load attribute file {attr_id}: {e}\")\n\n                    # Combine registry info with full attribute data\n                    attribute_data = {\n                        \"attribute_id\": attr_id,\n                        \"name\": attr_info.get(\"name\"),\n                        \"source_id\": attr_info.get(\"source_id\"),\n                        \"value\": full_attr_data.get(\"value\"),\n                        \"unit\": full_attr_data.get(\"unit\"),\n                        \"adverb\": full_attr_data.get(\"adverb\"),\n                        \"modality\": full_attr_data.get(\"modality\")\n                    }\n\n                    morph_attributes[morph_id][\"attributes\"].append(attribute_data)\n\n        return {\"node_id\": node_id, \"morphs\": morph_attributes}\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to list attributes by morph: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.list_relations_by_morph","title":"<code>list_relations_by_morph(user_id: str, graph_id: str, node_id: str)</code>","text":"<p>List all relations organized by morph for a given node.</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/relation/list_by_morph/{node_id}\")\ndef list_relations_by_morph(user_id: str, graph_id: str, node_id: str):\n    \"\"\"\n    List all relations organized by morph for a given node.\n    \"\"\"\n    try:\n        # Load source node\n        source_node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n        if not source_node_path.exists():\n            raise HTTPException(status_code=404, detail=\"Source node not found\")\n\n        source_node = load_json_file(source_node_path)\n\n        # Load relation registry\n        reg_path = Path(f\"graph_data/users/{user_id}/relation_registry.json\")\n        registry = load_registry(reg_path)\n\n        # Organize relations by morph\n        morph_relations = {}\n        for morph in source_node.get(\"morphs\", []):\n            morph_id = morph.get(\"morph_id\")\n            morph_name = morph.get(\"name\", \"Unknown\")\n            morph_relations[morph_id] = {\n                \"morph_id\": morph_id,\n                \"morph_name\": morph_name,\n                \"relations\": []\n            }\n\n            for rel_id in morph.get(\"relationNode_ids\", []):\n                if rel_id in registry:\n                    rel_info = registry[rel_id]\n\n                    # Load full relation data from the relation file\n                    rel_file_path = Path(f\"graph_data/users/{user_id}/relationNodes/{rel_id}.json\")\n                    full_rel_data = {}\n                    if rel_file_path.exists():\n                        try:\n                            full_rel_data = load_json_file(rel_file_path)\n                        except Exception as e:\n                            print(f\"Warning: Failed to load relation file {rel_id}: {e}\")\n\n                    # Combine registry info with full relation data\n                    relation_data = {\n                        \"relation_id\": rel_id,\n                        \"name\": rel_info.get(\"name\"),\n                        \"source_id\": rel_info.get(\"source_id\"),\n                        \"target_id\": rel_info.get(\"target_id\"),\n                        \"adverb\": full_rel_data.get(\"adverb\"),\n                        \"modality\": full_rel_data.get(\"modality\")\n                    }\n\n                    morph_relations[morph_id][\"relations\"].append(relation_data)\n\n        return {\"node_id\": node_id, \"morphs\": morph_relations}\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to list relations by morph: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.create_morph","title":"<code>create_morph(user_id: str, graph_id: str, request: CreateMorphRequest)</code>","text":"<p>Create a new morph for a node.</p> <p>Scenarios: 1. Empty morph: copy_from_morph is None - creates morph with empty properties 2. Copy from existing morph: copy_from_morph specifies any existing morph to copy from 3. Node context is always required - morphs must belong to a node</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/morph/create\")\ndef create_morph(user_id: str, graph_id: str, request: CreateMorphRequest):\n    \"\"\"\n    Create a new morph for a node.\n\n    Scenarios:\n    1. Empty morph: copy_from_morph is None - creates morph with empty properties\n    2. Copy from existing morph: copy_from_morph specifies any existing morph to copy from\n    3. Node context is always required - morphs must belong to a node\n    \"\"\"\n    try:\n        print(f\"DEBUG: Morph creation called with:\")\n        print(f\"  user_id: {user_id}\")\n        print(f\"  graph_id: {graph_id}\")\n        print(f\"  node_id: {request.node_id}\")\n        print(f\"  name: {request.name}\")\n        print(f\"  copy_from_morph: {request.copy_from_morph}\")\n        print(f\"  auto-generated morph_id: {request.name}_{request.node_id}\")\n\n        with graph_transaction(user_id, graph_id, \"create_morph\") as backup_dir:\n            # Auto-generate morph_id from name and node_id\n            morph_id = f\"{request.name}_{request.node_id}\"\n            node_id = request.node_id\n            morph_name = request.name\n            copy_from_morph = request.copy_from_morph\n\n            # Load the source node (required context)\n            source_node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n            if not source_node_path.exists():\n                raise HTTPException(status_code=404, detail=f\"Node {node_id} not found\")\n\n            with open(source_node_path, 'r') as f:\n                source_node = json.load(f)\n\n            # Check if morph already exists\n            existing_morph = None\n            for morph in source_node.get(\"morphs\", []):\n                if morph.get(\"morph_id\") == morph_id:\n                    existing_morph = morph\n                    break\n\n            if existing_morph:\n                # Morph already exists, but we need to ensure registries are updated\n                print(f\"DEBUG: Morph {morph_id} already exists, checking registry updates\")\n\n                # If copying from another morph, ensure registries are updated\n                if copy_from_morph:\n                    print(f\"DEBUG: Updating registries for existing morph {morph_id}\")\n\n                    # Update registries to include the morph_id for all relations and attributes\n                    # Update relation registry\n                    rel_reg_path = Path(f\"graph_data/users/{user_id}/relation_registry.json\")\n                    if rel_reg_path.exists():\n                        rel_registry = load_json_file(rel_reg_path)\n                        for rel_id in existing_morph.get(\"relationNode_ids\", []):\n                            if rel_id in rel_registry:\n                                current_morph_ids = rel_registry[rel_id].get(\"morph_id\", [])\n                                if isinstance(current_morph_ids, str):\n                                    current_morph_ids = [current_morph_ids]\n                                if morph_id not in current_morph_ids:\n                                    current_morph_ids.append(morph_id)\n                                    rel_registry[rel_id][\"morph_id\"] = current_morph_ids\n                        atomic_registry_save(user_id, \"relation\", rel_registry)\n\n                    # Update attribute registry\n                    attr_reg_path = Path(f\"graph_data/users/{user_id}/attribute_registry.json\")\n                    if attr_reg_path.exists():\n                        attr_registry = load_json_file(attr_reg_path)\n                        for attr_id in existing_morph.get(\"attributeNode_ids\", []):\n                            if attr_id in attr_registry:\n                                current_morph_ids = attr_registry[attr_id].get(\"morph_id\", [])\n                                if isinstance(current_morph_ids, str):\n                                    current_morph_ids = [current_morph_ids]\n                                if morph_id not in current_morph_ids:\n                                    current_morph_ids.append(morph_id)\n                                    attr_registry[attr_id][\"morph_id\"] = current_morph_ids\n                        atomic_registry_save(user_id, \"attribute\", attr_registry)\n\n                # Return success if morph already exists (idempotent behavior)\n                return {\n                    \"status\": \"Morph already exists\",\n                    \"morph_id\": morph_id,\n                    \"node_id\": node_id,\n                    \"name\": morph_name,\n                    \"copied_from\": copy_from_morph,\n                    \"relation_count\": len(existing_morph.get(\"relationNode_ids\", [])),\n                    \"attribute_count\": len(existing_morph.get(\"attributeNode_ids\", []))\n                }\n\n            # Create new morph\n            new_morph = {\n                \"morph_id\": morph_id,\n                \"node_id\": node_id,\n                \"name\": morph_name,\n                \"relationNode_ids\": [],\n                \"attributeNode_ids\": []\n            }\n\n            # If copying from another morph, copy all properties\n            print(f\"DEBUG: About to check copy_from_morph condition: {copy_from_morph}\")\n            print(f\"DEBUG: copy_from_morph type: {type(copy_from_morph)}\")\n            print(f\"DEBUG: copy_from_morph truthiness: {bool(copy_from_morph)}\")\n            if copy_from_morph:\n                print(f\"DEBUG: Copying from morph: {copy_from_morph}\")\n                # Reload the node file to ensure latest state\n                with open(source_node_path, 'r') as f:\n                    source_node = json.load(f)\n                print(f\"DEBUG: Loaded source node with {len(source_node.get('morphs', []))} morphs\")\n                source_morph = None\n                for morph in source_node.get(\"morphs\", []):\n                    print(f\"DEBUG: Checking morph: {morph.get('morph_id')}\")\n                    if morph.get(\"morph_id\") == copy_from_morph:\n                        source_morph = morph\n                        print(f\"DEBUG: Found source morph: {copy_from_morph}\")\n                        break\n\n                if not source_morph:\n                    print(f\"DEBUG: Source morph {copy_from_morph} not found!\")\n                    raise HTTPException(status_code=404, detail=f\"Source morph {copy_from_morph} not found in node {node_id}\")\n\n                print(f\"DEBUG: Copying from source morph {copy_from_morph}\")\n                print(f\"DEBUG: Source morph relationNode_ids: {source_morph.get('relationNode_ids', [])}\")\n                print(f\"DEBUG: Source morph attributeNode_ids: {source_morph.get('attributeNode_ids', [])}\")\n\n                # Copy relations and attributes\n                new_morph[\"relationNode_ids\"] = source_morph.get(\"relationNode_ids\", []).copy()\n                new_morph[\"attributeNode_ids\"] = source_morph.get(\"attributeNode_ids\", []).copy()\n\n                print(f\"DEBUG: New morph relationNode_ids: {new_morph['relationNode_ids']}\")\n                print(f\"DEBUG: New morph attributeNode_ids: {new_morph['attributeNode_ids']}\")\n\n                # Update registries to include the new morph_id for all copied relations and attributes\n                # Update relation registry\n                rel_reg_path = Path(f\"graph_data/users/{user_id}/relation_registry.json\")\n                print(f\"DEBUG: Checking relation registry at {rel_reg_path}\")\n                print(f\"DEBUG: Relation registry exists: {rel_reg_path.exists()}\")\n                if rel_reg_path.exists():\n                    rel_registry = load_json_file(rel_reg_path)\n                    print(f\"DEBUG: Current relation registry keys: {list(rel_registry.keys())}\")\n                    for rel_id in new_morph[\"relationNode_ids\"]:\n                        print(f\"DEBUG: Processing relation {rel_id}\")\n                        if rel_id in rel_registry:\n                            current_morph_ids = rel_registry[rel_id].get(\"morph_id\", [])\n                            print(f\"DEBUG: Current morph_ids for {rel_id}: {current_morph_ids}\")\n                            if isinstance(current_morph_ids, str):\n                                current_morph_ids = [current_morph_ids]\n                            if morph_id not in current_morph_ids:\n                                current_morph_ids.append(morph_id)\n                            print(f\"DEBUG: Updated morph_ids for {rel_id}: {current_morph_ids}\")\n                            rel_registry[rel_id][\"morph_id\"] = current_morph_ids\n                        else:\n                            print(f\"DEBUG: Relation {rel_id} not found in registry\")\n                    print(f\"DEBUG: Saving updated relation registry\")\n                    atomic_registry_save(user_id, \"relation\", rel_registry)\n\n                # Update attribute registry\n                attr_reg_path = Path(f\"graph_data/users/{user_id}/attribute_registry.json\")\n                print(f\"DEBUG: Checking attribute registry at {attr_reg_path}\")\n                print(f\"DEBUG: Attribute registry exists: {attr_reg_path.exists()}\")\n                if attr_reg_path.exists():\n                    attr_registry = load_json_file(attr_reg_path)\n                    print(f\"DEBUG: Current attribute registry keys: {list(attr_registry.keys())}\")\n                    for attr_id in new_morph[\"attributeNode_ids\"]:\n                        print(f\"DEBUG: Processing attribute {attr_id}\")\n                        if attr_id in attr_registry:\n                            current_morph_ids = attr_registry[attr_id].get(\"morph_id\", [])\n                            print(f\"DEBUG: Current morph_ids for {attr_id}: {current_morph_ids}\")\n                            if isinstance(current_morph_ids, str):\n                                current_morph_ids = [current_morph_ids]\n                            if morph_id not in current_morph_ids:\n                                current_morph_ids.append(morph_id)\n                            print(f\"DEBUG: Updated morph_ids for {attr_id}: {current_morph_ids}\")\n                            attr_registry[attr_id][\"morph_id\"] = current_morph_ids\n                        else:\n                            print(f\"DEBUG: Attribute {attr_id} not found in registry\")\n                    print(f\"DEBUG: Saving updated attribute registry\")\n                    atomic_registry_save(user_id, \"attribute\", attr_registry)\n                else:\n                    print(f\"DEBUG: Attribute registry does not exist\")\n\n            # Add new morph to node\n            if \"morphs\" not in source_node:\n                source_node[\"morphs\"] = []\n            source_node[\"morphs\"].append(new_morph)\n\n            # Save updated node\n            with open(source_node_path, 'w') as f:\n                json.dump(source_node, f, indent=2)\n\n            return {\n                \"status\": \"Morph created successfully\",\n                \"morph_id\": morph_id,\n                \"node_id\": node_id,\n                \"name\": morph_name,\n                \"copied_from\": copy_from_morph,\n                \"relation_count\": len(new_morph[\"relationNode_ids\"]),\n                \"attribute_count\": len(new_morph[\"attributeNode_ids\"]),\n                \"is_empty\": copy_from_morph is None\n            }\n\n    except AtomicityError as e:\n        raise HTTPException(status_code=500, detail=f\"Atomic operation failed: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to create morph: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.get_graph_node_ids","title":"<code>get_graph_node_ids(user_id: str, graph_id: str) -&gt; list[str]</code>","text":"<p>Get list of node IDs that belong to a specific graph</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>def get_graph_node_ids(user_id: str, graph_id: str) -&gt; list[str]:\n    \"\"\"Get list of node IDs that belong to a specific graph\"\"\"\n    from backend.core.registry import load_node_registry\n    registry = load_node_registry(user_id)\n    graph_nodes = []\n\n    for node_id, entry in registry.items():\n        if \"graphs\" in entry and graph_id in entry[\"graphs\"]:\n            graph_nodes.append(node_id)\n\n    return graph_nodes\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.get_relation_node","title":"<code>get_relation_node(user_id: str, graph_id: str, relation_id: str)</code>","text":"<p>Get a specific relation node by its ID</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/relationNodes/{relation_id}\")\ndef get_relation_node(user_id: str, graph_id: str, relation_id: str):\n    \"\"\"Get a specific relation node by its ID\"\"\"\n    rel_path = f\"graph_data/users/{user_id}/relationNodes/{relation_id}.json\"\n    if not os.path.exists(rel_path):\n        raise HTTPException(status_code=404, detail=\"RelationNode not found\")\n    with open(rel_path, \"r\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.validate_graph_consistency","title":"<code>validate_graph_consistency(user_id: str, graph_id: str)</code>","text":"<p>Validate the consistency of a graph's data.</p> <p>This endpoint performs comprehensive validation of: - Node registry consistency - Relation registry consistency - Attribute registry consistency - File existence checks - Reference integrity</p> <p>Returns validation results with issues and warnings.</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/validate\")\ndef validate_graph_consistency(user_id: str, graph_id: str):\n    \"\"\"\n    Validate the consistency of a graph's data.\n\n    This endpoint performs comprehensive validation of:\n    - Node registry consistency\n    - Relation registry consistency  \n    - Attribute registry consistency\n    - File existence checks\n    - Reference integrity\n\n    Returns validation results with issues and warnings.\n    \"\"\"\n    try:\n        validation_result = validate_consistency(user_id)\n        return validation_result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Validation failed: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graph_ops/#backend.routes.graph_ops.cleanup_old_backups","title":"<code>cleanup_old_backups(user_id: str, graph_id: str, max_age_hours: int = 24)</code>","text":"<p>Clean up old backup directories for a user.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>User ID</p> required <code>graph_id</code> <code>str</code> <p>Graph ID (not used but kept for consistency)</p> required <code>max_age_hours</code> <code>int</code> <p>Maximum age of backups to keep (default: 24 hours)</p> <code>24</code> <p>Returns:</p> Type Description <p>Number of backups cleaned up</p> Source code in <code>backend/routes/graph_ops.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/cleanup-backups\")\ndef cleanup_old_backups(user_id: str, graph_id: str, max_age_hours: int = 24):\n    \"\"\"\n    Clean up old backup directories for a user.\n\n    Args:\n        user_id: User ID\n        graph_id: Graph ID (not used but kept for consistency)\n        max_age_hours: Maximum age of backups to keep (default: 24 hours)\n\n    Returns:\n        Number of backups cleaned up\n    \"\"\"\n    try:\n        cleaned_count = cleanup_backups(user_id, max_age_hours)\n        return {\n            \"status\": \"Backup cleanup completed\",\n            \"cleaned_count\": cleaned_count,\n            \"max_age_hours\": max_age_hours\n        }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Backup cleanup failed: {str(e)}\")\n</code></pre>"},{"location":"api/routes/graphs/","title":"Graph Management Routes","text":"<p>This section documents the graph management API endpoints.</p>"},{"location":"api/routes/graphs/#backend.routes.graphs","title":"<code>backend.routes.graphs</code>","text":""},{"location":"api/routes/graphs/#backend.routes.graphs-functions","title":"Functions","text":""},{"location":"api/routes/logging/","title":"Logging Routes","text":"<p>Logging routes provide API endpoints for accessing and managing the logging system.</p>"},{"location":"api/routes/logging/#overview","title":"Overview","text":"<p>The logging routes module handles log retrieval, export, performance metrics, and logging system management. Users can view logs, export them, and get performance metrics through these endpoints.</p>"},{"location":"api/routes/logging/#key-endpoints","title":"Key Endpoints","text":""},{"location":"api/routes/logging/#log-retrieval","title":"Log Retrieval","text":"<ul> <li><code>GET /api/logs/recent</code> - Get recent logs with optional filtering</li> <li><code>GET /api/logs/categories</code> - Get all available log categories</li> <li><code>GET /api/logs/stats</code> - Get logging statistics</li> </ul>"},{"location":"api/routes/logging/#log-export","title":"Log Export","text":"<ul> <li><code>POST /api/logs/export</code> - Export logs to JSON file</li> <li><code>DELETE /api/logs/clear</code> - Clear the in-memory log buffer</li> </ul>"},{"location":"api/routes/logging/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li><code>GET /api/logs/performance</code> - Get performance metrics</li> <li><code>GET /api/logs/health</code> - Check logging system health</li> </ul>"},{"location":"api/routes/logging/#usage-examples","title":"Usage Examples","text":""},{"location":"api/routes/logging/#getting-recent-logs","title":"Getting Recent Logs","text":"<pre><code>import requests\n\n# Get recent logs\nresponse = requests.get(\"http://localhost:8000/api/logs/recent\")\nlogs = response.json()\n\n# Get logs filtered by category\nresponse = requests.get(\"http://localhost:8000/api/logs/recent?category=ERROR&amp;limit=50\")\nerror_logs = response.json()\n\n# Get logs for specific user\nresponse = requests.get(\"http://localhost:8000/api/logs/recent?user_id=user123\")\nuser_logs = response.json()\n</code></pre>"},{"location":"api/routes/logging/#exporting-logs","title":"Exporting Logs","text":"<pre><code># Export logs from last 24 hours\nresponse = requests.post(\"http://localhost:8000/api/logs/export?hours=24\")\nwith open(\"logs_export.json\", \"wb\") as f:\n    f.write(response.content)\n\n# Export error logs only\nresponse = requests.post(\"http://localhost:8000/api/logs/export?category=ERROR&amp;hours=48\")\n</code></pre>"},{"location":"api/routes/logging/#getting-performance-metrics","title":"Getting Performance Metrics","text":"<pre><code>response = requests.get(\"http://localhost:8000/api/logs/performance\")\nmetrics = response.json()\nprint(f\"Average response time: {metrics['metrics']['avg_response_time']}\")\n</code></pre>"},{"location":"api/routes/logging/#log-categories","title":"Log Categories","text":"<p>Available log categories: - AUDIT: User actions and system access - OPERATION: General operations and workflows - DEBUG: Debugging information - ERROR: Error messages and exceptions - SECURITY: Security-related events - PERFORMANCE: Performance metrics and timing - ATOMIC: Atomic operation logs - SYSTEM: System-level events</p>"},{"location":"api/routes/logging/#error-handling","title":"Error Handling","text":"<ul> <li><code>200</code> - Success</li> <li><code>400</code> - Invalid parameters (e.g., invalid category)</li> <li><code>500</code> - Internal server error</li> </ul>"},{"location":"api/routes/logging/#dependencies","title":"Dependencies","text":"<ul> <li>Logging system core module</li> <li>File system operations</li> <li>Performance monitoring utilities </li> </ul>"},{"location":"api/routes/ndf_routes/","title":"NDF Routes","text":"<p>NDF (Node Definition Format) routes provide API endpoints for managing node definitions and related operations.</p>"},{"location":"api/routes/ndf_routes/#overview","title":"Overview","text":"<p>The NDF routes module handles operations related to node definitions, including creation, updates, validation, and management of node structures.</p>"},{"location":"api/routes/ndf_routes/#key-endpoints","title":"Key Endpoints","text":""},{"location":"api/routes/ndf_routes/#node-definition-management","title":"Node Definition Management","text":"<ul> <li><code>POST /ndf/nodes</code> - Create new node definitions</li> <li><code>GET /ndf/nodes</code> - Retrieve node definitions</li> <li><code>PUT /ndf/nodes/{node_id}</code> - Update node definitions</li> <li><code>DELETE /ndf/nodes/{node_id}</code> - Delete node definitions</li> </ul>"},{"location":"api/routes/ndf_routes/#node-validation","title":"Node Validation","text":"<ul> <li><code>POST /ndf/validate</code> - Validate node definition structure</li> <li><code>GET /ndf/schema</code> - Get NDF schema information</li> </ul>"},{"location":"api/routes/ndf_routes/#node-operations","title":"Node Operations","text":"<ul> <li><code>POST /ndf/compose</code> - Compose complex node structures</li> <li><code>GET /ndf/export</code> - Export node definitions</li> <li><code>POST /ndf/import</code> - Import node definitions</li> </ul>"},{"location":"api/routes/ndf_routes/#usage-examples","title":"Usage Examples","text":""},{"location":"api/routes/ndf_routes/#creating-a-node-definition","title":"Creating a Node Definition","text":"<pre><code>import requests\n\nnode_data = {\n    \"name\": \"example_node\",\n    \"type\": \"concept\",\n    \"attributes\": {\n        \"description\": \"An example node\",\n        \"category\": \"examples\"\n    }\n}\n\nresponse = requests.post(\"http://localhost:8000/ndf/nodes\", json=node_data)\n</code></pre>"},{"location":"api/routes/ndf_routes/#validating-node-structure","title":"Validating Node Structure","text":"<pre><code>response = requests.post(\"http://localhost:8000/ndf/validate\", json=node_data)\nif response.status_code == 200:\n    print(\"Node definition is valid\")\nelse:\n    print(\"Validation errors:\", response.json())\n</code></pre>"},{"location":"api/routes/ndf_routes/#error-handling","title":"Error Handling","text":"<p>The NDF routes return appropriate HTTP status codes: - <code>200</code> - Success - <code>400</code> - Bad request (invalid data) - <code>404</code> - Node not found - <code>422</code> - Validation error - <code>500</code> - Internal server error</p>"},{"location":"api/routes/ndf_routes/#dependencies","title":"Dependencies","text":"<ul> <li>Core NDF operations module</li> <li>Node validation utilities</li> <li>Schema management functions </li> </ul>"},{"location":"api/routes/nodes/","title":"Node Management Routes","text":"<p>This section documents the node management API endpoints.</p>"},{"location":"api/routes/nodes/#backend.routes.nodes","title":"<code>backend.routes.nodes</code>","text":""},{"location":"api/routes/nodes/#backend.routes.nodes-classes","title":"Classes","text":""},{"location":"api/routes/nodes/#backend.routes.nodes-functions","title":"Functions","text":""},{"location":"api/routes/nodes/#backend.routes.nodes.set_node_nbh","title":"<code>set_node_nbh(user_id: str, graph_id: str, node_id: str, nbh: str)</code>","text":"<p>Set the active morph (neighborhood) for a polynode. This determines which morph state the node is currently in.</p> Source code in <code>backend/routes/nodes.py</code> <pre><code>@router.put(\"/users/{user_id}/graphs/{graph_id}/set_nbh/{node_id}\")\ndef set_node_nbh(user_id: str, graph_id: str, node_id: str, nbh: str):\n    \"\"\"\n    Set the active morph (neighborhood) for a polynode.\n    This determines which morph state the node is currently in.\n    \"\"\"\n    try:\n        with graph_transaction(user_id, graph_id, \"set_node_nbh\") as backup_dir:\n            # Load the node\n            node_path = Path(f\"graph_data/users/{user_id}/nodes/{node_id}.json\")\n            if not node_path.exists():\n                raise HTTPException(status_code=404, detail=\"Node not found\")\n\n            node_data = load_json_file(node_path)\n\n            # Verify the morph exists\n            morph_exists = False\n            for morph in node_data.get(\"morphs\", []):\n                if morph.get(\"morph_id\") == nbh:\n                    morph_exists = True\n                    break\n\n            if not morph_exists:\n                raise HTTPException(status_code=404, detail=f\"Morph {nbh} not found in node {node_id}\")\n\n            # Set the active morph\n            node_data[\"nbh\"] = nbh\n\n            # Save the updated node\n            atomic_node_save(user_id, node_id, node_data)\n\n            return {\n                \"status\": \"Active morph set successfully\",\n                \"node_id\": node_id,\n                \"active_morph\": nbh,\n                \"available_morphs\": [m.get(\"morph_id\") for m in node_data.get(\"morphs\", [])]\n            }\n\n    except AtomicityError as e:\n        raise HTTPException(status_code=500, detail=f\"Atomic operation failed: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to set active morph: {str(e)}\")\n</code></pre>"},{"location":"api/routes/nodes/#backend.routes.nodes.get_node_registry","title":"<code>get_node_registry(user_id: str)</code>","text":"<p>Return the user's canonical node_registry.json for fast node lookup and autocomplete.</p> Source code in <code>backend/routes/nodes.py</code> <pre><code>@router.get(\"/users/{user_id}/node_registry\")\ndef get_node_registry(user_id: str):\n    \"\"\"\n    Return the user's canonical node_registry.json for fast node lookup and autocomplete.\n    \"\"\"\n    registry_path = os.path.join(\"graph_data\", \"users\", user_id, \"node_registry.json\")\n    if not os.path.exists(registry_path):\n        return {}\n    with open(registry_path, \"r\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/routes/parse_pipeline/","title":"Parse Pipeline Routes","text":"<p>Parse pipeline routes handle the processing and parsing of various input formats into structured node definitions.</p>"},{"location":"api/routes/parse_pipeline/#overview","title":"Overview","text":"<p>The parse pipeline module provides endpoints for processing different input formats (CNL, markdown, JSON) and converting them into structured node definitions that can be used by the NDF system.</p>"},{"location":"api/routes/parse_pipeline/#key-endpoints","title":"Key Endpoints","text":""},{"location":"api/routes/parse_pipeline/#pipeline-processing","title":"Pipeline Processing","text":"<ul> <li><code>POST /parse/cnl</code> - Parse CNL (Controlled Natural Language) input</li> <li><code>POST /parse/markdown</code> - Parse markdown documents</li> <li><code>POST /parse/json</code> - Parse JSON structures</li> <li><code>POST /parse/validate</code> - Validate parsed structures</li> </ul>"},{"location":"api/routes/parse_pipeline/#pipeline-management","title":"Pipeline Management","text":"<ul> <li><code>GET /parse/pipelines</code> - List available parsing pipelines</li> <li><code>POST /parse/pipelines</code> - Create custom parsing pipeline</li> <li><code>PUT /parse/pipelines/{pipeline_id}</code> - Update parsing pipeline</li> <li><code>DELETE /parse/pipelines/{pipeline_id}</code> - Delete parsing pipeline</li> </ul>"},{"location":"api/routes/parse_pipeline/#usage-examples","title":"Usage Examples","text":""},{"location":"api/routes/parse_pipeline/#parsing-cnl-input","title":"Parsing CNL Input","text":"<pre><code>import requests\n\ncnl_text = \"\"\"\nThere is a concept called \"oxygen\" that has:\n- atomic number: 8\n- symbol: O\n- category: chemical element\n\"\"\"\n\nresponse = requests.post(\"http://localhost:8000/parse/cnl\", json={\"text\": cnl_text})\nparsed_nodes = response.json()\n</code></pre>"},{"location":"api/routes/parse_pipeline/#parsing-markdown","title":"Parsing Markdown","text":"<pre><code>markdown_content = \"\"\"\n# Oxygen\n\nOxygen is a chemical element with:\n- Atomic number: 8\n- Symbol: O\n- Category: Chemical element\n\"\"\"\n\nresponse = requests.post(\"http://localhost:8000/parse/markdown\", json={\"content\": markdown_content})\n</code></pre>"},{"location":"api/routes/parse_pipeline/#pipeline-configuration","title":"Pipeline Configuration","text":"<p>Parsing pipelines can be configured with various options: - Input format detection - Output format specification - Validation rules - Custom parsing rules</p>"},{"location":"api/routes/parse_pipeline/#error-handling","title":"Error Handling","text":"<ul> <li><code>200</code> - Successful parsing</li> <li><code>400</code> - Invalid input format</li> <li><code>422</code> - Parsing errors</li> <li><code>500</code> - Pipeline processing error</li> </ul>"},{"location":"api/routes/parse_pipeline/#dependencies","title":"Dependencies","text":"<ul> <li>CNL parser module</li> <li>Markdown processing utilities</li> <li>JSON schema validation</li> <li>Node structure validation </li> </ul>"},{"location":"api/routes/preferences/","title":"Preferences Routes","text":"<p>Preferences routes manage user and system preferences for the NDF Studio application.</p>"},{"location":"api/routes/preferences/#overview","title":"Overview","text":"<p>The preferences routes module handles the storage, retrieval, and management of user preferences, system settings, and configuration options that customize the behavior of the NDF Studio application.</p>"},{"location":"api/routes/preferences/#key-endpoints","title":"Key Endpoints","text":""},{"location":"api/routes/preferences/#user-preferences","title":"User Preferences","text":"<ul> <li><code>GET /preferences</code> - Get current user preferences</li> <li><code>POST /preferences</code> - Create or update user preferences</li> <li><code>PUT /preferences/{preference_id}</code> - Update specific preference</li> <li><code>DELETE /preferences/{preference_id}</code> - Delete specific preference</li> </ul>"},{"location":"api/routes/preferences/#system-preferences","title":"System Preferences","text":"<ul> <li><code>GET /preferences/system</code> - Get system-wide preferences</li> <li><code>POST /preferences/system</code> - Update system preferences (admin only)</li> <li><code>GET /preferences/defaults</code> - Get default preference values</li> </ul>"},{"location":"api/routes/preferences/#preference-categories","title":"Preference Categories","text":"<ul> <li><code>GET /preferences/categories</code> - List available preference categories</li> <li><code>GET /preferences/category/{category}</code> - Get preferences by category</li> </ul>"},{"location":"api/routes/preferences/#usage-examples","title":"Usage Examples","text":""},{"location":"api/routes/preferences/#getting-user-preferences","title":"Getting User Preferences","text":"<pre><code>import requests\n\nresponse = requests.get(\"http://localhost:8000/preferences\")\npreferences = response.json()\nprint(f\"Theme: {preferences.get('theme', 'default')}\")\nprint(f\"Language: {preferences.get('language', 'en')}\")\n</code></pre>"},{"location":"api/routes/preferences/#updating-user-preferences","title":"Updating User Preferences","text":"<pre><code>new_preferences = {\n    \"theme\": \"dark\",\n    \"language\": \"en\",\n    \"auto_save\": True,\n    \"notifications\": {\n        \"email\": True,\n        \"browser\": False\n    }\n}\n\nresponse = requests.post(\"http://localhost:8000/preferences\", json=new_preferences)\n</code></pre>"},{"location":"api/routes/preferences/#getting-system-preferences","title":"Getting System Preferences","text":"<pre><code>response = requests.get(\"http://localhost:8000/preferences/system\")\nsystem_prefs = response.json()\nprint(f\"Max file size: {system_prefs['max_file_size']}\")\n</code></pre>"},{"location":"api/routes/preferences/#preference-categories_1","title":"Preference Categories","text":"<p>Preferences are organized into categories: - Interface: UI theme, language, layout - Behavior: Auto-save, notifications, confirmations - Performance: Cache settings, timeout values - Security: Session timeout, authentication settings - Custom: User-defined preferences</p>"},{"location":"api/routes/preferences/#preference-schema","title":"Preference Schema","text":"<p>Preferences follow a structured schema: <pre><code>{\n    \"category\": \"interface\",\n    \"key\": \"theme\",\n    \"value\": \"dark\",\n    \"type\": \"string\",\n    \"description\": \"UI theme preference\",\n    \"default\": \"light\",\n    \"options\": [\"light\", \"dark\", \"auto\"]\n}\n</code></pre></p>"},{"location":"api/routes/preferences/#error-handling","title":"Error Handling","text":"<ul> <li><code>200</code> - Success</li> <li><code>400</code> - Invalid preference data</li> <li><code>401</code> - Unauthorized (for system preferences)</li> <li><code>404</code> - Preference not found</li> <li><code>500</code> - Internal server error</li> </ul>"},{"location":"api/routes/preferences/#dependencies","title":"Dependencies","text":"<ul> <li>User authentication system</li> <li>Preference storage backend</li> <li>Configuration management</li> <li>Validation utilities </li> </ul>"},{"location":"api/routes/schema_routes/","title":"Schema Management Routes","text":"<p>This section documents the schema management API endpoints.</p>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes","title":"<code>backend.routes.schema_routes</code>","text":""},{"location":"api/routes/schema_routes/#backend.routes.schema_routes-functions","title":"Functions","text":""},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.load_global_json","title":"<code>load_global_json(filename, fallback)</code>","text":"<p>Load JSON from global schema directory</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>def load_global_json(filename, fallback):\n    \"\"\"Load JSON from global schema directory\"\"\"\n    filepath = GLOBAL_SCHEMA_DIR / filename\n    if filepath.exists():\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    return fallback\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.load_user_json","title":"<code>load_user_json(user_id: str, filename, fallback)</code>","text":"<p>Load JSON from user schema directory</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>def load_user_json(user_id: str, filename, fallback):\n    \"\"\"Load JSON from user schema directory\"\"\"\n    user_dir = USER_SCHEMA_DIR / user_id\n    filepath = user_dir / filename\n    if filepath.exists():\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    return fallback\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.save_user_json","title":"<code>save_user_json(user_id: str, filename, data)</code>","text":"<p>Save JSON to user schema directory</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>def save_user_json(user_id: str, filename, data):\n    \"\"\"Save JSON to user schema directory\"\"\"\n    user_dir = USER_SCHEMA_DIR / user_id\n    user_dir.mkdir(parents=True, exist_ok=True)\n    filepath = user_dir / filename\n    with open(filepath, 'w') as f:\n        json.dump(data, f, indent=2)\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.combine_schemas","title":"<code>combine_schemas(global_data: List[dict], user_data: List[dict], key_field: str = 'name') -&gt; List[dict]</code>","text":"<p>Combine global and user schemas, with user schemas taking precedence</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>def combine_schemas(global_data: List[dict], user_data: List[dict], key_field: str = \"name\") -&gt; List[dict]:\n    \"\"\"Combine global and user schemas, with user schemas taking precedence\"\"\"\n    combined = global_data.copy()\n    user_names = {item[key_field] for item in user_data}\n\n    # Remove global items that have user overrides\n    combined = [item for item in combined if item[key_field] not in user_names]\n\n    # Add user items\n    combined.extend(user_data)\n    return combined\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.get_relation_names","title":"<code>get_relation_names(user_id: str, graph_id: str)</code>","text":"<p>Get all relation names (global + user)</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/relation-names\")\ndef get_relation_names(user_id: str, graph_id: str):\n    \"\"\"Get all relation names (global + user)\"\"\"\n    global_relations = load_global_json(\"relation_types.json\", [])\n    user_relations = load_user_json(user_id, \"relation_types.json\", [])\n    combined = combine_schemas(global_relations, user_relations)\n    return [r[\"name\"] for r in combined]\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.get_attribute_names","title":"<code>get_attribute_names(user_id: str, graph_id: str)</code>","text":"<p>Get all attribute names (global + user)</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/attribute-names\")\ndef get_attribute_names(user_id: str, graph_id: str):\n    \"\"\"Get all attribute names (global + user)\"\"\"\n    global_attributes = load_global_json(\"attribute_types.json\", [])\n    user_attributes = load_user_json(user_id, \"attribute_types.json\", [])\n    combined = combine_schemas(global_attributes, user_attributes)\n    return [a[\"name\"] for a in combined]\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.get_relation_types","title":"<code>get_relation_types(user_id: str, graph_id: str)</code>","text":"<p>Get all relation types (global + user)</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/relation-types\")\ndef get_relation_types(user_id: str, graph_id: str):\n    \"\"\"Get all relation types (global + user)\"\"\"\n    global_relations = load_global_json(\"relation_types.json\", [])\n    user_relations = load_user_json(user_id, \"relation_types.json\", [])\n    return combine_schemas(global_relations, user_relations)\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.create_relation_type","title":"<code>create_relation_type(user_id: str, graph_id: str, rt: RelationType, user: User = Depends(current_active_user))</code>","text":"<p>Create a new relation type in user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/relation-types/create\")\ndef create_relation_type(\n    user_id: str, \n    graph_id: str, \n    rt: RelationType,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Create a new relation type in user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot create relation type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it already exists in global schemas\n    global_relations = load_global_json(\"relation_types.json\", [])\n    if any(r[\"name\"] == rt.name for r in global_relations):\n        raise HTTPException(status_code=400, detail=\"Relation type already exists in global schemas\")\n\n    # Check if it already exists in user schemas\n    user_relations = load_user_json(user_id, \"relation_types.json\", [])\n    if any(r[\"name\"] == rt.name for r in user_relations):\n        raise HTTPException(status_code=400, detail=\"Relation type already exists in your schemas\")\n\n    # Add to user schemas\n    user_relations.append(rt.dict())\n    save_user_json(user_id, \"relation_types.json\", user_relations)\n    return {\"status\": \"relation type added to user schemas\"}\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.update_relation_type","title":"<code>update_relation_type(user_id: str, graph_id: str, name: str, rt: RelationType, user: User = Depends(current_active_user))</code>","text":"<p>Update a relation type in user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.put(\"/users/{user_id}/graphs/{graph_id}/relation-types/{name}\")\ndef update_relation_type(\n    user_id: str, \n    graph_id: str, \n    name: str, \n    rt: RelationType,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Update a relation type in user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot update relation type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it exists in global schemas (read-only)\n    global_relations = load_global_json(\"relation_types.json\", [])\n    if any(r[\"name\"] == name for r in global_relations):\n        raise HTTPException(status_code=403, detail=\"Cannot modify global schema elements. Create a user-specific version instead.\")\n\n    # Update in user schemas\n    user_relations = load_user_json(user_id, \"relation_types.json\", [])\n    for i, entry in enumerate(user_relations):\n        if entry[\"name\"] == name:\n            user_relations[i] = rt.dict()\n            save_user_json(user_id, \"relation_types.json\", user_relations)\n            return {\"status\": \"updated in user schemas\"}\n\n    raise HTTPException(status_code=404, detail=\"Relation type not found in user schemas\")\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.delete_relation_type","title":"<code>delete_relation_type(user_id: str, graph_id: str, name: str, user: User = Depends(current_active_user))</code>","text":"<p>Delete a relation type from user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.delete(\"/users/{user_id}/graphs/{graph_id}/relation-types/{name}\")\ndef delete_relation_type(\n    user_id: str, \n    graph_id: str, \n    name: str,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Delete a relation type from user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot delete relation type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it exists in global schemas (cannot delete)\n    global_relations = load_global_json(\"relation_types.json\", [])\n    if any(r[\"name\"] == name for r in global_relations):\n        raise HTTPException(status_code=403, detail=\"Cannot delete global schema elements\")\n\n    # Delete from user schemas\n    user_relations = load_user_json(user_id, \"relation_types.json\", [])\n    user_relations = [r for r in user_relations if r[\"name\"] != name]\n    save_user_json(user_id, \"relation_types.json\", user_relations)\n    return {\"status\": \"deleted from user schemas\"}\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.get_attribute_types","title":"<code>get_attribute_types(user_id: str, graph_id: str)</code>","text":"<p>Get all attribute types (global + user)</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/attribute-types\")\ndef get_attribute_types(user_id: str, graph_id: str):\n    \"\"\"Get all attribute types (global + user)\"\"\"\n    global_attributes = load_global_json(\"attribute_types.json\", [])\n    user_attributes = load_user_json(user_id, \"attribute_types.json\", [])\n    return combine_schemas(global_attributes, user_attributes)\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.create_attribute_type","title":"<code>create_attribute_type(user_id: str, graph_id: str, item: AttributeType, user: User = Depends(current_active_user))</code>","text":"<p>Create a new attribute type in user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/attribute-types\")\ndef create_attribute_type(\n    user_id: str, \n    graph_id: str, \n    item: AttributeType,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Create a new attribute type in user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot create attribute type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it already exists in global schemas\n    global_attributes = load_global_json(\"attribute_types.json\", [])\n    if any(a[\"name\"] == item.name for a in global_attributes):\n        raise HTTPException(status_code=400, detail=\"Attribute type already exists in global schemas\")\n\n    # Check if it already exists in user schemas\n    user_attributes = load_user_json(user_id, \"attribute_types.json\", [])\n    if any(a[\"name\"] == item.name for a in user_attributes):\n        raise HTTPException(status_code=400, detail=\"Attribute type already exists in your schemas\")\n\n    # Add to user schemas\n    user_attributes.append(item.dict())\n    save_user_json(user_id, \"attribute_types.json\", user_attributes)\n    return {\"status\": \"attribute type added to user schemas\"}\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.update_attribute_type","title":"<code>update_attribute_type(user_id: str, graph_id: str, name: str, item: AttributeType, user: User = Depends(current_active_user))</code>","text":"<p>Update an attribute type in user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.put(\"/users/{user_id}/graphs/{graph_id}/attribute-types/{name}\")\ndef update_attribute_type(\n    user_id: str, \n    graph_id: str, \n    name: str, \n    item: AttributeType,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Update an attribute type in user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot update attribute type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it exists in global schemas (read-only)\n    global_attributes = load_global_json(\"attribute_types.json\", [])\n    if any(a[\"name\"] == name for a in global_attributes):\n        raise HTTPException(status_code=403, detail=\"Cannot modify global schema elements. Create a user-specific version instead.\")\n\n    # Update in user schemas\n    user_attributes = load_user_json(user_id, \"attribute_types.json\", [])\n    for i, entry in enumerate(user_attributes):\n        if entry[\"name\"] == name:\n            user_attributes[i] = item.dict()\n            save_user_json(user_id, \"attribute_types.json\", user_attributes)\n            return {\"status\": \"updated in user schemas\"}\n\n    raise HTTPException(status_code=404, detail=\"Attribute type not found in user schemas\")\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.delete_attribute_type","title":"<code>delete_attribute_type(user_id: str, graph_id: str, name: str, user: User = Depends(current_active_user))</code>","text":"<p>Delete an attribute type from user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.delete(\"/users/{user_id}/graphs/{graph_id}/attribute-types/{name}\")\ndef delete_attribute_type(\n    user_id: str, \n    graph_id: str, \n    name: str,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Delete an attribute type from user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot delete attribute type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it exists in global schemas (cannot delete)\n    global_attributes = load_global_json(\"attribute_types.json\", [])\n    if any(a[\"name\"] == name for a in global_attributes):\n        raise HTTPException(status_code=403, detail=\"Cannot delete global schema elements\")\n\n    # Delete from user schemas\n    user_attributes = load_user_json(user_id, \"attribute_types.json\", [])\n    user_attributes = [a for a in user_attributes if a[\"name\"] != name]\n    save_user_json(user_id, \"attribute_types.json\", user_attributes)\n    return {\"status\": \"deleted from user schemas\"}\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.get_node_types","title":"<code>get_node_types(user_id: str, graph_id: str)</code>","text":"<p>Get all node types (global + user)</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/graphs/{graph_id}/node-types\")\ndef get_node_types(user_id: str, graph_id: str):\n    \"\"\"Get all node types (global + user)\"\"\"\n    global_nodes = load_global_json(\"node_types.json\", [])\n    user_nodes = load_user_json(user_id, \"node_types.json\", [])\n    return combine_schemas(global_nodes, user_nodes)\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.create_node_type","title":"<code>create_node_type(user_id: str, graph_id: str, item: NodeType, user: User = Depends(current_active_user))</code>","text":"<p>Create a new node type in user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.post(\"/users/{user_id}/graphs/{graph_id}/node-types\")\ndef create_node_type(\n    user_id: str, \n    graph_id: str, \n    item: NodeType,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Create a new node type in user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot create node type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it already exists in global schemas\n    global_nodes = load_global_json(\"node_types.json\", [])\n    if any(n[\"name\"] == item.name for n in global_nodes):\n        raise HTTPException(status_code=400, detail=\"Node type already exists in global schemas\")\n\n    # Check if it already exists in user schemas\n    user_nodes = load_user_json(user_id, \"node_types.json\", [])\n    if any(n[\"name\"] == item.name for n in user_nodes):\n        raise HTTPException(status_code=400, detail=\"Node type already exists in your schemas\")\n\n    # Add to user schemas\n    user_nodes.append(item.dict())\n    save_user_json(user_id, \"node_types.json\", user_nodes)\n    return {\"status\": \"node type added to user schemas\"}\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.update_node_type","title":"<code>update_node_type(user_id: str, graph_id: str, name: str, item: NodeType, user: User = Depends(current_active_user))</code>","text":"<p>Update a node type in user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.put(\"/users/{user_id}/graphs/{graph_id}/node-types/{name}\")\ndef update_node_type(\n    user_id: str, \n    graph_id: str, \n    name: str, \n    item: NodeType,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Update a node type in user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot update node type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it exists in global schemas (read-only)\n    global_nodes = load_global_json(\"node_types.json\", [])\n    if any(n[\"name\"] == name for n in global_nodes):\n        raise HTTPException(status_code=403, detail=\"Cannot modify global schema elements. Create a user-specific version instead.\")\n\n    # Update in user schemas\n    user_nodes = load_user_json(user_id, \"node_types.json\", [])\n    for i, entry in enumerate(user_nodes):\n        if entry[\"name\"] == name:\n            user_nodes[i] = item.dict()\n            save_user_json(user_id, \"node_types.json\", user_nodes)\n            return {\"status\": \"updated in user schemas\"}\n\n    raise HTTPException(status_code=404, detail=\"Node type not found in user schemas\")\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.delete_node_type","title":"<code>delete_node_type(user_id: str, graph_id: str, name: str, user: User = Depends(current_active_user))</code>","text":"<p>Delete a node type from user space</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.delete(\"/users/{user_id}/graphs/{graph_id}/node-types/{name}\")\ndef delete_node_type(\n    user_id: str, \n    graph_id: str, \n    name: str,\n    user: User = Depends(current_active_user)\n):\n    \"\"\"Delete a node type from user space\"\"\"\n    # Authorization check: user can only access their own data unless superuser\n    if not user.is_superuser and str(user.id) != user_id:\n        raise HTTPException(\n            status_code=403,\n            detail=\"Cannot delete node type: Access denied. You can only access your own data.\"\n        )\n\n    # Check if it exists in global schemas (cannot delete)\n    global_nodes = load_global_json(\"node_types.json\", [])\n    if any(n[\"name\"] == name for n in global_nodes):\n        raise HTTPException(status_code=403, detail=\"Cannot delete global schema elements\")\n\n    # Delete from user schemas\n    user_nodes = load_user_json(user_id, \"node_types.json\", [])\n    user_nodes = [n for n in user_nodes if n[\"name\"] != name]\n    save_user_json(user_id, \"node_types.json\", user_nodes)\n    return {\"status\": \"deleted from user schemas\"}\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.get_global_schemas","title":"<code>get_global_schemas(user_id: str)</code>","text":"<p>Get all global schemas (read-only reference)</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/schemas/global\")\ndef get_global_schemas(user_id: str):\n    \"\"\"Get all global schemas (read-only reference)\"\"\"\n    return {\n        \"relation_types\": load_global_json(\"relation_types.json\", []),\n        \"attribute_types\": load_global_json(\"attribute_types.json\", []),\n        \"node_types\": load_global_json(\"node_types.json\", []),\n        \"transition_types\": load_global_json(\"transition_types.json\", []),\n        \"function_types\": load_global_json(\"function_types.json\", [])\n    }\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.get_user_schemas","title":"<code>get_user_schemas(user_id: str)</code>","text":"<p>Get user's custom schemas</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/schemas/user\")\ndef get_user_schemas(user_id: str):\n    \"\"\"Get user's custom schemas\"\"\"\n    return {\n        \"relation_types\": load_user_json(user_id, \"relation_types.json\", []),\n        \"attribute_types\": load_user_json(user_id, \"attribute_types.json\", []),\n        \"node_types\": load_user_json(user_id, \"node_types.json\", [])\n    }\n</code></pre>"},{"location":"api/routes/schema_routes/#backend.routes.schema_routes.get_combined_schemas","title":"<code>get_combined_schemas(user_id: str)</code>","text":"<p>Get combined schemas (global + user)</p> Source code in <code>backend/routes/schema_routes.py</code> <pre><code>@router.get(\"/users/{user_id}/schemas/combined\")\ndef get_combined_schemas(user_id: str):\n    \"\"\"Get combined schemas (global + user)\"\"\"\n    return {\n        \"relation_types\": get_relation_types(user_id, \"dummy\"),\n        \"attribute_types\": get_attribute_types(user_id, \"dummy\"),\n        \"node_types\": get_node_types(user_id, \"dummy\")\n    }\n</code></pre>"},{"location":"api/routes/transitions/","title":"Graph Transitions Routes","text":"<p>This section documents the graph transitions API endpoints.</p>"},{"location":"api/routes/transitions/#backend.routes.transitions","title":"<code>backend.routes.transitions</code>","text":""},{"location":"api/routes/transitions/#backend.routes.transitions-classes","title":"Classes","text":""},{"location":"api/routes/users/","title":"User Management Routes","text":"<p>This section documents the user management API endpoints.</p>"},{"location":"api/routes/users/#backend.routes.users","title":"<code>backend.routes.users</code>","text":""},{"location":"api/routes/users/#backend.routes.users-classes","title":"Classes","text":""},{"location":"api/routes/users/#backend.routes.users.UserManager","title":"<code>UserManager</code>","text":"<p>               Bases: <code>UUIDIDMixin</code>, <code>BaseUserManager[User, UUID]</code></p> Source code in <code>backend/routes/users.py</code> <pre><code>class UserManager(UUIDIDMixin, BaseUserManager[User, UUID]):\n    user_db_model = User\n    reset_password_token_secret = SECRET\n    verification_token_secret = SECRET\n\n    async def on_after_register(self, user: User, request=None):\n        # Only print username and email, not hashed_password\n        print(f\"[DEBUG] User registered: username={getattr(user, 'username', None)}, email={getattr(user, 'email', None)}\")\n\n        # Check if this is the first user and make them superuser\n        await self._make_first_user_superuser(user)\n\n        # Initialize user directory structure\n        await self.initialize_user_directories(str(user.id))\n\n    async def _make_first_user_superuser(self, user: User):\n        \"\"\"Make the first user in the system a superuser\"\"\"\n        try:\n            with Session(engine) as session:\n                # Count total users\n                total_users = len(session.exec(select(User)).all())\n\n                if total_users == 1:  # This is the first user\n                    user.is_superuser = True\n                    session.add(user)\n                    session.commit()\n                    print(f\"[ADMIN] First user '{user.username}' automatically promoted to superuser\")\n\n                    # Log the admin promotion\n                    logger = get_logger()\n                    logger.security(\n                        f\"First user '{user.username}' automatically promoted to superuser\",\n                        event_type=\"first_user_superuser_promotion\",\n                        user_id=str(user.id),\n                        username=user.username\n                    )\n        except Exception as e:\n            print(f\"[ERROR] Failed to make first user superuser: {e}\")\n\n    async def initialize_user_directories(self, user_id: str):\n        \"\"\"Create the necessary directory structure and default files for a new user.\"\"\"\n\n        # Base user directory\n        user_dir = Path(\"graph_data\") / \"users\" / user_id\n        user_dir.mkdir(parents=True, exist_ok=True)\n\n        # Create subdirectories\n        subdirs = [\n            \"nodes\",\n            \"attributeNodes\", \n            \"relationNodes\",\n            \"transitions\",\n            \"functions\",\n            \"graphs\",\n            \"schemas\"  # For user-specific schema extensions\n        ]\n\n        for subdir in subdirs:\n            (user_dir / subdir).mkdir(exist_ok=True)\n\n        # Create default registry files\n        registries = [\n            \"node_registry.json\",\n            \"attribute_registry.json\", \n            \"relation_registry.json\",\n            \"transition_registry.json\",\n            \"function_registry.json\"\n        ]\n\n        for registry in registries:\n            registry_path = user_dir / registry\n            if not registry_path.exists():\n                with open(registry_path, 'w') as f:\n                    json.dump({}, f)\n\n        print(f\"[DEBUG] Initialized directories for user {user_id}\")\n\n    async def authenticate(self, credentials: UserLogin) -&gt; User | None:\n        \"\"\"Custom authenticate method that supports login with username or email\"\"\"\n        pwd_context = CryptContext(schemes=[\"argon2\", \"bcrypt\"], deprecated=\"auto\")\n\n        # Try to find user by username or email\n        with Session(engine) as session:\n            # Try username first\n            stmt = select(User).where(User.username == credentials.username_or_email)\n            user = session.exec(stmt).first()\n\n            if not user:\n                # Try email\n                stmt = select(User).where(User.email == credentials.username_or_email)\n                user = session.exec(stmt).first()\n\n            if not user:\n                return None\n\n            # Verify password\n            if not pwd_context.verify(credentials.password, user.hashed_password):\n                return None\n\n            return user\n\n    async def _create(self, user_create: UserCreate) -&gt; User:\n        \"\"\"Override _create to handle username field\"\"\"\n        pwd_context = CryptContext(schemes=[\"argon2\", \"bcrypt\"], deprecated=\"auto\")\n\n        user_data = {\n            \"email\": user_create.email,\n            \"username\": user_create.username,\n            \"hashed_password\": pwd_context.hash(user_create.password),\n            \"is_active\": user_create.is_active,\n            \"is_superuser\": user_create.is_superuser,\n            \"is_verified\": user_create.is_verified\n        }\n\n        user = User(**user_data)\n        return user\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.UserManager-functions","title":"Functions","text":""},{"location":"api/routes/users/#backend.routes.users.UserManager.initialize_user_directories","title":"<code>initialize_user_directories(user_id: str)</code>  <code>async</code>","text":"<p>Create the necessary directory structure and default files for a new user.</p> Source code in <code>backend/routes/users.py</code> <pre><code>async def initialize_user_directories(self, user_id: str):\n    \"\"\"Create the necessary directory structure and default files for a new user.\"\"\"\n\n    # Base user directory\n    user_dir = Path(\"graph_data\") / \"users\" / user_id\n    user_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create subdirectories\n    subdirs = [\n        \"nodes\",\n        \"attributeNodes\", \n        \"relationNodes\",\n        \"transitions\",\n        \"functions\",\n        \"graphs\",\n        \"schemas\"  # For user-specific schema extensions\n    ]\n\n    for subdir in subdirs:\n        (user_dir / subdir).mkdir(exist_ok=True)\n\n    # Create default registry files\n    registries = [\n        \"node_registry.json\",\n        \"attribute_registry.json\", \n        \"relation_registry.json\",\n        \"transition_registry.json\",\n        \"function_registry.json\"\n    ]\n\n    for registry in registries:\n        registry_path = user_dir / registry\n        if not registry_path.exists():\n            with open(registry_path, 'w') as f:\n                json.dump({}, f)\n\n    print(f\"[DEBUG] Initialized directories for user {user_id}\")\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.UserManager.authenticate","title":"<code>authenticate(credentials: UserLogin) -&gt; User | None</code>  <code>async</code>","text":"<p>Custom authenticate method that supports login with username or email</p> Source code in <code>backend/routes/users.py</code> <pre><code>async def authenticate(self, credentials: UserLogin) -&gt; User | None:\n    \"\"\"Custom authenticate method that supports login with username or email\"\"\"\n    pwd_context = CryptContext(schemes=[\"argon2\", \"bcrypt\"], deprecated=\"auto\")\n\n    # Try to find user by username or email\n    with Session(engine) as session:\n        # Try username first\n        stmt = select(User).where(User.username == credentials.username_or_email)\n        user = session.exec(stmt).first()\n\n        if not user:\n            # Try email\n            stmt = select(User).where(User.email == credentials.username_or_email)\n            user = session.exec(stmt).first()\n\n        if not user:\n            return None\n\n        # Verify password\n        if not pwd_context.verify(credentials.password, user.hashed_password):\n            return None\n\n        return user\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users-functions","title":"Functions","text":""},{"location":"api/routes/users/#backend.routes.users.login","title":"<code>login(credentials: UserLogin)</code>  <code>async</code>","text":"<p>Custom login endpoint that accepts username or email</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.post(\"/login\")\nasync def login(credentials: UserLogin):\n    \"\"\"Custom login endpoint that accepts username or email\"\"\"\n    user_manager = UserManager(get_user_db().__next__())\n    user = await user_manager.authenticate(credentials)\n\n    if not user:\n        raise HTTPException(status_code=401, detail=\"Invalid credentials\")\n\n    if not user.is_active:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n\n    # Generate JWT token\n    jwt_strategy = get_jwt_strategy()\n    token = await jwt_strategy.write_token(user)\n\n    return {\n        \"access_token\": token,\n        \"token_type\": \"bearer\",\n        \"user\": {\n            \"id\": str(user.id),\n            \"username\": user.username,\n            \"email\": user.email,\n            \"is_active\": user.is_active,\n            \"is_superuser\": user.is_superuser\n        }\n    }\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.get_auth_config","title":"<code>get_auth_config()</code>  <code>async</code>","text":"<p>Get authentication configuration including inactivity threshold</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.get(\"/config\")\nasync def get_auth_config():\n    \"\"\"Get authentication configuration including inactivity threshold\"\"\"\n    return {\n        \"inactivity_threshold_minutes\": 20,  # Should match the JWT strategy configuration\n        \"max_token_lifetime_hours\": 1,  # Should match the JWT strategy configuration\n        \"features\": {\n            \"inactivity_based_expiration\": True\n        }\n    }\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.admin_list_users","title":"<code>admin_list_users(user: User = Depends(current_active_user))</code>  <code>async</code>","text":"<p>List all users (admin only)</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.get(\"/admin/users\")\nasync def admin_list_users(user: User = Depends(current_active_user)):\n    \"\"\"List all users (admin only)\"\"\"\n    if not user.is_superuser:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n\n    try:\n        with Session(engine) as session:\n            users = session.exec(select(User)).all()\n            return {\n                \"users\": [\n                    {\n                        \"id\": str(u.id),\n                        \"username\": u.username,\n                        \"email\": u.email,\n                        \"is_active\": u.is_active,\n                        \"is_superuser\": u.is_superuser,\n                        \"is_verified\": u.is_verified,\n                        \"created_at\": getattr(u, 'created_at', None)\n                    }\n                    for u in users\n                ],\n                \"total\": len(users),\n                \"superusers\": sum(1 for u in users if u.is_superuser),\n                \"active_users\": sum(1 for u in users if u.is_active)\n            }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to list users: {str(e)}\")\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.admin_create_user","title":"<code>admin_create_user(user_data: AdminUserCreate, current_user: User = Depends(current_active_user))</code>  <code>async</code>","text":"<p>Create a new user (admin only)</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.post(\"/admin/users\")\nasync def admin_create_user(\n    user_data: AdminUserCreate,\n    current_user: User = Depends(current_active_user)\n):\n    \"\"\"Create a new user (admin only)\"\"\"\n    if not current_user.is_superuser:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n\n    try:\n        user_manager = UserManager(get_user_db().__next__())\n\n        # Create user using FastAPI Users schema\n        user_create = UserCreate(\n            email=user_data.email,\n            username=user_data.username,\n            password=user_data.password,\n            is_active=user_data.is_active,\n            is_superuser=user_data.is_superuser,\n            is_verified=True\n        )\n\n        user = await user_manager.create(user_create)\n\n        # Log the admin action\n        logger = get_logger()\n        logger.security(\n            f\"Admin '{current_user.username}' created user '{user.username}'\",\n            event_type=\"admin_user_creation\",\n            admin_user_id=str(current_user.id),\n            created_user_id=str(user.id),\n            created_username=user.username,\n            is_superuser=user_data.is_superuser\n        )\n\n        return {\n            \"message\": \"User created successfully\",\n            \"user\": {\n                \"id\": str(user.id),\n                \"username\": user.username,\n                \"email\": user.email,\n                \"is_active\": user.is_active,\n                \"is_superuser\": user.is_superuser\n            }\n        }\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Failed to create user: {str(e)}\")\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.admin_update_user","title":"<code>admin_update_user(user_id: str, user_data: UserUpdateRequest, current_user: User = Depends(current_active_user))</code>  <code>async</code>","text":"<p>Update user details (admin only)</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.put(\"/admin/users/{user_id}\")\nasync def admin_update_user(\n    user_id: str,\n    user_data: UserUpdateRequest,\n    current_user: User = Depends(current_active_user)\n):\n    \"\"\"Update user details (admin only)\"\"\"\n    if not current_user.is_superuser:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n\n    try:\n        # Convert user_id to proper UUID format if needed\n        if len(user_id) == 32:\n            # Add hyphens to make it a proper UUID\n            user_id = f\"{user_id[:8]}-{user_id[8:12]}-{user_id[12:16]}-{user_id[16:20]}-{user_id[20:]}\"\n\n        with Session(engine) as session:\n            # Find the user to update\n            user = session.exec(select(User).where(User.id == UUID(user_id))).first()\n            if not user:\n                raise HTTPException(status_code=404, detail=\"User not found\")\n\n            # Prevent admin from demoting themselves\n            if str(user.id) == str(current_user.id) and user_data.is_superuser == False:\n                raise HTTPException(status_code=400, detail=\"Cannot demote yourself\")\n\n            # Update fields\n            if user_data.username is not None:\n                user.username = user_data.username\n            if user_data.email is not None:\n                user.email = user_data.email\n            if user_data.is_active is not None:\n                user.is_active = user_data.is_active\n            if user_data.is_superuser is not None:\n                user.is_superuser = user_data.is_superuser\n\n            session.add(user)\n            session.commit()\n\n            # Log the admin action\n            logger = get_logger()\n            logger.security(\n                f\"Admin '{current_user.username}' updated user '{user.username}'\",\n                event_type=\"admin_user_update\",\n                admin_user_id=str(current_user.id),\n                updated_user_id=str(user.id),\n                updated_username=user.username,\n                changes=user_data.dict(exclude_unset=True)\n            )\n\n            return {\n                \"message\": \"User updated successfully\",\n                \"user\": {\n                    \"id\": str(user.id),\n                    \"username\": user.username,\n                    \"email\": user.email,\n                    \"is_active\": user.is_active,\n                    \"is_superuser\": user.is_superuser\n                }\n            }\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid user ID format: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to update user: {str(e)}\")\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.admin_promote_user","title":"<code>admin_promote_user(user_id: str, request: UserPromoteRequest, current_user: User = Depends(current_active_user))</code>  <code>async</code>","text":"<p>Promote a user to superuser (admin only)</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.post(\"/admin/users/{user_id}/promote\")\nasync def admin_promote_user(\n    user_id: str,\n    request: UserPromoteRequest,\n    current_user: User = Depends(current_active_user)\n):\n    \"\"\"Promote a user to superuser (admin only)\"\"\"\n    if not current_user.is_superuser:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n\n    try:\n        # Convert user_id to proper UUID format if needed\n        if len(user_id) == 32:\n            # Add hyphens to make it a proper UUID\n            user_id = f\"{user_id[:8]}-{user_id[8:12]}-{user_id[12:16]}-{user_id[16:20]}-{user_id[20:]}\"\n\n        with Session(engine) as session:\n            user = session.exec(select(User).where(User.id == UUID(user_id))).first()\n            if not user:\n                raise HTTPException(status_code=404, detail=\"User not found\")\n\n            if user.is_superuser:\n                raise HTTPException(status_code=400, detail=\"User is already a superuser\")\n\n            user.is_superuser = True\n            session.add(user)\n            session.commit()\n\n            # Log the admin action\n            logger = get_logger()\n            logger.security(\n                f\"Admin '{current_user.username}' promoted user '{user.username}' to superuser\",\n                event_type=\"admin_user_promotion\",\n                admin_user_id=str(current_user.id),\n                promoted_user_id=str(user.id),\n                promoted_username=user.username,\n                reason=request.reason\n            )\n\n            return {\n                \"message\": f\"User '{user.username}' promoted to superuser successfully\",\n                \"user\": {\n                    \"id\": str(user.id),\n                    \"username\": user.username,\n                    \"email\": user.email,\n                    \"is_active\": user.is_active,\n                    \"is_superuser\": user.is_superuser\n                }\n            }\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid user ID format: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to promote user: {str(e)}\")\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.admin_demote_user","title":"<code>admin_demote_user(user_id: str, request: UserDemoteRequest, current_user: User = Depends(current_active_user))</code>  <code>async</code>","text":"<p>Demote a superuser to regular user (admin only)</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.post(\"/admin/users/{user_id}/demote\")\nasync def admin_demote_user(\n    user_id: str,\n    request: UserDemoteRequest,\n    current_user: User = Depends(current_active_user)\n):\n    \"\"\"Demote a superuser to regular user (admin only)\"\"\"\n    if not current_user.is_superuser:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n\n    try:\n        # Convert user_id to proper UUID format if needed\n        if len(user_id) == 32:\n            # Add hyphens to make it a proper UUID\n            user_id = f\"{user_id[:8]}-{user_id[8:12]}-{user_id[12:16]}-{user_id[16:20]}-{user_id[20:]}\"\n\n        with Session(engine) as session:\n            user = session.exec(select(User).where(User.id == UUID(user_id))).first()\n            if not user:\n                raise HTTPException(status_code=404, detail=\"User not found\")\n\n            if not user.is_superuser:\n                raise HTTPException(status_code=400, detail=\"User is not a superuser\")\n\n            # Prevent admin from demoting themselves\n            if str(user.id) == str(current_user.id):\n                raise HTTPException(status_code=400, detail=\"Cannot demote yourself\")\n\n            user.is_superuser = False\n            session.add(user)\n            session.commit()\n\n            # Log the admin action\n            logger = get_logger()\n            logger.security(\n                f\"Admin '{current_user.username}' demoted user '{user.username}' from superuser\",\n                event_type=\"admin_user_demotion\",\n                admin_user_id=str(current_user.id),\n                demoted_user_id=str(user.id),\n                demoted_username=user.username,\n                reason=request.reason\n            )\n\n            return {\n                \"message\": f\"User '{user.username}' demoted from superuser successfully\",\n                \"user\": {\n                    \"id\": str(user.id),\n                    \"username\": user.username,\n                    \"email\": user.email,\n                    \"is_active\": user.is_active,\n                    \"is_superuser\": user.is_superuser\n                }\n            }\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid user ID format: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to demote user: {str(e)}\")\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.admin_delete_user","title":"<code>admin_delete_user(user_id: str, current_user: User = Depends(current_active_user))</code>  <code>async</code>","text":"<p>Delete a user (admin only)</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.delete(\"/admin/users/{user_id}\")\nasync def admin_delete_user(\n    user_id: str,\n    current_user: User = Depends(current_active_user)\n):\n    \"\"\"Delete a user (admin only)\"\"\"\n    if not current_user.is_superuser:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n\n    try:\n        # Convert user_id to proper UUID format if needed\n        if len(user_id) == 32:\n            # Add hyphens to make it a proper UUID\n            user_id = f\"{user_id[:8]}-{user_id[8:12]}-{user_id[12:16]}-{user_id[16:20]}-{user_id[20:]}\"\n\n        with Session(engine) as session:\n            user = session.exec(select(User).where(User.id == UUID(user_id))).first()\n            if not user:\n                raise HTTPException(status_code=404, detail=\"User not found\")\n\n            # Prevent admin from deleting themselves\n            if str(user.id) == str(current_user.id):\n                raise HTTPException(status_code=400, detail=\"Cannot delete yourself\")\n\n            username = user.username\n            user_email = user.email\n\n            # Delete the user\n            session.delete(user)\n            session.commit()\n\n            # Log the admin action\n            logger = get_logger()\n            logger.security(\n                f\"Admin '{current_user.username}' deleted user '{username}'\",\n                event_type=\"admin_user_deletion\",\n                admin_user_id=str(current_user.id),\n                deleted_username=username,\n                deleted_email=user_email\n            )\n\n            return {\n                \"message\": f\"User '{username}' deleted successfully\"\n            }\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid user ID format: {str(e)}\")\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to delete user: {str(e)}\")\n</code></pre>"},{"location":"api/routes/users/#backend.routes.users.admin_get_stats","title":"<code>admin_get_stats(current_user: User = Depends(current_active_user))</code>  <code>async</code>","text":"<p>Get system statistics (admin only)</p> Source code in <code>backend/routes/users.py</code> <pre><code>@users_router.get(\"/admin/stats\")\nasync def admin_get_stats(current_user: User = Depends(current_active_user)):\n    \"\"\"Get system statistics (admin only)\"\"\"\n    if not current_user.is_superuser:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n\n    try:\n        with Session(engine) as session:\n            total_users = session.exec(select(User)).all()\n            active_users = session.exec(select(User).where(User.is_active == True)).all()\n            superusers = session.exec(select(User).where(User.is_superuser == True)).all()\n\n            return {\n                \"total_users\": len(total_users),\n                \"active_users\": len(active_users),\n                \"inactive_users\": len(total_users) - len(active_users),\n                \"superusers\": len(superusers),\n                \"regular_users\": len(total_users) - len(superusers),\n                \"system_info\": {\n                    \"first_user_created\": True if len(total_users) &gt; 0 else False,\n                    \"has_superusers\": len(superusers) &gt; 0\n                }\n            }\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to get stats: {str(e)}\")\n</code></pre>"},{"location":"development/installation/","title":"Installation Guide","text":"<p>This guide will help you set up the NDF Studio backend for development.</p>"},{"location":"development/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+: The backend requires Python 3.8 or higher</li> <li>Git: For cloning the repository</li> <li>Virtual Environment: Recommended for Python dependency management</li> </ul>"},{"location":"development/installation/#step-by-step-installation","title":"Step-by-Step Installation","text":""},{"location":"development/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/gnowledge/nodeBook.git\ncd nodeBook\n</code></pre>"},{"location":"development/installation/#2-set-up-python-virtual-environment","title":"2. Set Up Python Virtual Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\n# On Linux/macOS:\nsource venv/bin/activate\n# On Windows:\nvenv\\Scripts\\activate\n</code></pre>"},{"location":"development/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install Python dependencies\npip install -r backend/requirements.txt\n\n# Install spaCy model (required for CNL parsing)\npython -m spacy download en_core_web_sm\n</code></pre>"},{"location":"development/installation/#4-set-up-environment-variables","title":"4. Set Up Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># Backend configuration\nBACKEND_HOST=localhost\nBACKEND_PORT=8000\nDEBUG=True\n\n# Database configuration\nDATABASE_URL=sqlite:///./ndf_studio.db\n\n# JWT configuration\nJWT_SECRET_KEY=your-secret-key-here\nJWT_ALGORITHM=HS256\nJWT_ACCESS_TOKEN_EXPIRE_MINUTES=30\n\n# spaCy model\nSPACY_MODEL=en_core_web_sm\n</code></pre>"},{"location":"development/installation/#5-initialize-the-database","title":"5. Initialize the Database","text":"<pre><code># Run database migrations (if using Alembic)\n# alembic upgrade head\n\n# Or create initial database\npython -c \"from backend.core.models import *; from backend.config import get_data_root; print('Database initialized')\"\n</code></pre>"},{"location":"development/installation/#6-create-admin-user","title":"6. Create Admin User","text":"<pre><code># Run the post-install script to create admin user\npython scripts/post_install.py\n</code></pre>"},{"location":"development/installation/#running-the-application","title":"Running the Application","text":""},{"location":"development/installation/#development-server","title":"Development Server","text":"<pre><code># Start the backend server\nbash scripts/start_backend.sh\n\n# Or manually\ncd backend\nuvicorn main:app --reload --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"development/installation/#production-server","title":"Production Server","text":"<pre><code># For production deployment\ncd backend\nuvicorn main:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"development/installation/#verification","title":"Verification","text":"<p>Once the server is running, you can verify the installation:</p> <ol> <li>Health Check: Visit <code>http://localhost:8000/api/health</code></li> <li>API Documentation: Visit <code>http://localhost:8000/docs</code></li> <li>Alternative Docs: Visit <code>http://localhost:8000/redoc</code></li> </ol>"},{"location":"development/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/installation/#common-issues","title":"Common Issues","text":""},{"location":"development/installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you're in the correct directory and the virtual environment is activated:</p> <pre><code># Make sure you're in the project root\npwd  # Should show /path/to/nodeBook\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Check Python path\npython -c \"import sys; print('\\n'.join(sys.path))\"\n</code></pre>"},{"location":"development/installation/#spacy-model-issues","title":"spaCy Model Issues","text":"<p>If spaCy model is not found:</p> <pre><code># Reinstall spaCy model\npython -m spacy download en_core_web_sm\n\n# Verify installation\npython -c \"import spacy; nlp = spacy.load('en_core_web_sm'); print('spaCy model loaded successfully')\"\n</code></pre>"},{"location":"development/installation/#port-already-in-use","title":"Port Already in Use","text":"<p>If port 8000 is already in use:</p> <pre><code># Find process using port 8000\nlsof -i :8000\n\n# Kill the process\nkill -9 &lt;PID&gt;\n\n# Or use a different port\nuvicorn main:app --reload --host 0.0.0.0 --port 8001\n</code></pre>"},{"location":"development/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Check the API Reference for available endpoints</li> <li>Run tests with <code>bash run_tests.sh</code></li> <li>Explore the codebase in the <code>backend/</code> directory </li> </ol>"}]}